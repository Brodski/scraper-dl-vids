CHAT PODCAST 1 hour
VAD = false
beam = 2
152.99 sec = 2.55 min
--> 2.55min / hour of audio

[0.00s -> 2.56s]  Let's talk about open assistant. Sure. So start from the beginning.
[3.52s -> 9.36s]  Well, we saw that there was a lot of movement in this space,
[9.36s -> 13.00s]  that chat GPT came along, and everyone's like, who are chat GPT and so on.
[13.16s -> 18.08s]  And yeah, I think it was it. I mean, it was both a surprise and a not
[18.08s -> 21.32s]  surprise for people. The capabilities of chat GPT weren't a surprise.
[21.32s -> 24.44s]  But obviously, the success of it, I think too many of us was it's like,
[24.44s -> 28.76s]  okay, we knew we could build such no chat conversational like things,
[28.76s -> 31.16s]  but we didn't know how much people loved them.
[31.16s -> 37.00s]  There's a bit of a, I think, wasn't it Sam Altman maybe was that who said in an interview,
[37.00s -> 41.32s]  well, anyone could have built this using our API before, but no one did.
[41.32s -> 45.24s]  So we did, which is not true because they distinctively forbade,
[45.24s -> 50.28s]  like you building an app that had unfiltered access to the API essentially.
[50.28s -> 51.00s]  Open-ended apps.
[51.64s -> 53.32s]  I think they explicitly forbade that.
[53.32s -> 57.00s]  So that it's a wrong statement that anyone could have built this using our API because
[57.00s -> 61.80s]  had they tried, you would have shot them down, right? But in essence, the capabilities were there.
[61.80s -> 65.48s]  And it's still in their restriction now, unless they changed it, they do not allow open-ended
[65.48s -> 68.76s]  applications, which seems like an oxymoron to me, because the language model is inherently open-ended.
[68.76s -> 71.96s]  Yeah, but I mean, I can see their restriction being like, you know, you're not allowed to
[71.96s -> 75.56s]  build an app that just lets users freeform query our app, you need to either do some
[75.56s -> 79.40s]  filtering or do some heavy prompting around it, so that it's like for one particular purpose,
[79.40s -> 83.16s]  I can see that I can totally get why they do it. But then at the same time, saying
[83.16s -> 89.08s]  anyone could have built chat GPT. If I could imagine an app that has unfiltered, unfettered
[89.08s -> 95.16s]  access to the API through an app, it's chat GPT. In any case, there was obviously a lot of... And
[95.16s -> 98.92s]  then I think pretty soon people came up with this idea, hey, could we do something like that
[98.92s -> 103.00s]  open source? They had a bit of an older paper called instruct GPT or that described the model
[103.00s -> 108.20s]  called instruct GPT that where they sort of outlined how we think chat GPT was done
[108.20s -> 114.04s]  approximately. No one knows. And at that point, we also saw, hey, the amount of data to be collected
[114.04s -> 121.96s]  is actually in reach. It's not immense, humongous, and so on. It's actually okay and could be done.
[121.96s -> 127.88s]  So at that point, yeah, a lot of people wanted to do something open source-like. And I think
[127.88s -> 132.92s]  a bunch of us just came together and felt we could do it. So we built this platform where
[132.92s -> 137.56s]  people could come and contribute to the data set, which was really cool to see that people
[137.56s -> 139.84s]  actually came and amazing.
[139.84s -> 141.24s]  Open source.
[141.24s -> 145.20s]  Well, the point is, there were a lot of ideas around as well of, oh, let's just collect,
[145.20s -> 148.20s]  you know, scrape Quora on ScraperEdit, right, and that will serve like as training data.
[148.20s -> 152.80s]  It's true to an amount, right, but it's very clear, at least to me, that the capabilities
[152.80s -> 156.92s]  of these models, the chat models, they come from the underlying language model, right?
[156.92s -> 161.12s]  The biggest claim to that I have is that OpenAI said they used crowd workers from
[161.12s -> 166.44s]  low wage countries to do their data input, yet the first examples of chat GPT that
[166.44s -> 168.92s]  flew around were like, Oh, look at it solving this quantum physics problem.
[168.92s -> 169.28s]  And so on.
[169.28s -> 172.44s]  And I'm not saying that there aren't any good quantum physicists in other
[172.44s -> 173.00s]  countries, right?
[173.08s -> 176.32s]  But the people who typically go for like a low wage crowd worker job in these
[176.32s -> 179.52s]  countries, they probably aren't experts in quantum physics.
[179.52s -> 182.80s]  And they also certainly weren't paid to go get a degree in it just so they
[182.80s -> 183.96s]  could answer that one question.
[183.96s -> 187.16s]  So to me, it's very clear that the capabilities come from the underlying
[187.16s -> 189.52s]  model, from the next token prediction pre-training.
[189.68s -> 192.56s]  And then all the human data does is kind of, it gets it into this mood
[192.60s -> 193.56s]  of being an assistant, right?
[193.68s -> 196.40s]  Gives it lots of examples of here is how it's like, you know, going
[196.40s -> 197.94s]  through an apprenticeship or something like this,
[197.94s -> 199.12s]  where you've lived your life, right?
[199.12s -> 200.92s]  You've grown up, you've consumed the world and so on.
[200.92s -> 202.84s]  And then you start your first job and someone tells you,
[202.84s -> 204.72s]  look, here's how you behave towards customers, right?
[204.72s -> 206.88s]  You're friendly, you know, if someone asks this,
[206.88s -> 208.28s]  you do it like this, here is a thing,
[208.28s -> 209.24s]  here is how our system works.
[209.24s -> 210.72s]  And so you get introduced to that,
[210.72s -> 213.90s]  but your competence of just living and doing things
[213.90s -> 215.70s]  comes from yourself, for your life,
[215.70s -> 217.80s]  and not from that one person who introduces you
[217.80s -> 218.76s]  to how the store works
[218.76s -> 220.20s]  and how you should behave towards customers.
[220.20s -> 222.82s]  So my big conviction was always,
[222.82s -> 224.72s]  we should really collect this data from humans
[224.72s -> 226.24s]  and the goal should really be diversity.
[226.24s -> 233.44s]  So the goal, and if you just say, well, we'll just scrape 10,000 of this, then to me, that
[233.44s -> 234.60s]  certainly is going to do something, right?
[234.60s -> 236.88s]  It's good data probably, but it's a bit missing the point.
[236.88s -> 240.68s]  If you want a general assistant, you need as general data as you can get.
[240.68s -> 246.64s]  And only human data so far has been able to achieve that level of diversity and generality.
[246.64s -> 249.80s]  And it was proven, like, okay, I'm biased, but I think it was proven right a little
[249.80s -> 253.56s]  bit in that if you look at the dataset, the prompts that human write, like what they
[253.56s -> 257.88s]  want to know what they want the model to do. It's so diverse. It's insane. And so we built
[257.88s -> 261.12s]  this platform where essentially you as a human, you can come and you're always presented with
[261.12s -> 264.46s]  like one task and the task could be write the prompt, right? But the task could also
[264.46s -> 267.72s]  be here is an already existing conversation between a human and an assistant. And now
[267.72s -> 270.96s]  it's the assistant's turn. And now you play the assistant, please write your response
[270.96s -> 275.60s]  for it. It could also be here is a conversation. Here's the last message of the conversation.
[275.60s -> 279.28s]  It's the reply by the assistant. Please rate it like label it. Is it, is it spam?
[279.28s -> 280.78s]  Is it a troll?
[280.78s -> 281.78s]  Is it funny?
[281.78s -> 282.78s]  Is it appropriate?
[282.78s -> 284.86s]  Does it fulfill what the human wanted out of it?
[284.86s -> 288.86s]  And so that's how we constructed the data set and we collected over like 600,000 inputs
[288.86s -> 292.54s]  of such that being text or labels or rankings of different things.
[292.54s -> 295.66s]  And yeah, that resulted in this data set.
[295.66s -> 299.94s]  Over 13,000 people contributed to the data set, which is mind-blowing to see and it's
[299.94s -> 300.94s]  really cool.
[300.94s -> 303.34s]  And we've just made the data set fully available.
[303.34s -> 305.26s]  You can go and look at it and download it.
[305.26s -> 306.26s]  There are a lot of...
[306.26s -> 311.22s]  we have about 10,000, what we call fully annotated conversation trees, which is like a root node,
[311.22s -> 314.90s]  the prompt and different answers from it. And then from those sometimes different answers
[314.90s -> 318.26s]  and so on, we have sampled, we've set our parameters in various ways. So sometimes it's
[318.26s -> 322.66s]  short trees, sometimes it's big trees, and sometimes it's wide trees, right? And
[322.66s -> 326.06s]  so you can go look at all of that. We have over 10,000 of those trees, which
[326.06s -> 328.86s]  is really cool because you can see the same conversation like taking a bit alternate
[328.86s -> 333.18s]  turns and so on. And we have like tons and tons of prompts. Like we have probably
[333.18s -> 338.82s]  like 50,000 or so, or 20,000 at least, like just prompts, like people who come and want
[338.82s -> 339.82s]  to know.
[339.82s -> 344.42s]  So we got so much prompts, we had to implement like Andreas has been very influential in
[344.42s -> 345.42s]  this project.
[345.42s -> 350.10s]  And he had to implement this prompt lottery, where really we first, if people enter a
[350.10s -> 351.78s]  prompt, it first goes into this lottery thing, right?
[351.78s -> 352.78s]  And then we sample from that.
[352.78s -> 356.90s]  And I think we adjust it so that one person can't, like if someone person puts a lot
[356.90s -> 360.18s]  of prompts, it's like sample less so that every person has kind of like the same
[360.18s -> 363.22s]  or a similar chance of getting their prompt into the system, right?
[363.22s -> 366.38s]  Because one prompt then generates, you know, probably a hundred tasks
[366.38s -> 368.66s]  because it's all the responses and the responses to the responses
[368.66s -> 369.90s]  and the rankings and the labels.
[369.90s -> 370.78s]  And it's been fun.
[370.78s -> 373.98s]  It's been absolute fun and a pleasure to work with the people.
[374.02s -> 375.82s]  Also, the people who've contributed code is amazing.
[375.82s -> 378.10s]  People just they come and they they ask for nothing, right?
[378.10s -> 379.14s]  They just like, oh, this is cool.
[379.14s -> 379.98s]  I want to be part of this.
[379.98s -> 382.22s]  And and they see that everyone else excited too.
[382.22s -> 383.30s]  And then they contribute code.
[383.30s -> 385.74s]  Some contribute like lots and lots of code, which is amazing.
[385.74s -> 386.90s]  Some just come and they contribute.
[386.90s -> 388.18s]  You know, there's like, here's an issue.
[388.18s -> 389.74s]  I'll do it. And that's that's cool, too.
[389.74s -> 393.48s]  So yeah, it's been cool. Well, first of all, thank you for doing this. It's absolutely amazing
[393.48s -> 395.88s]  Well, thank the people like I've just been the noise machine, right?
[395.88s -> 400.42s]  I know that I mean when when you when you published all of that information on your YouTube channel that you're working on it
[400.42s -> 401.46s]  I'm sure everyone jumped on it
[401.46s -> 402.94s]  But I do have a few questions
[402.94s -> 406.34s]  The reason why it's so exciting is just like Connor did we used to be friends of Connelly
[406.34s -> 408.86s]  He were chatting with him years ago, and he just stands still friends
[412.38s -> 415.78s]  Yeah, but we were we were chatting all the time and he just kind of set up a lot the AI and just
[415.78s -> 418.26s]  Got you know, I think Google on board and he just said, you know what?
[418.26s -> 419.54s]  I'm going to build the pile, and I'm just
[419.54s -> 420.42s]  going to build this massive data set,
[420.42s -> 421.66s]  and I'm just going to train this massive language model.
[421.66s -> 423.02s]  And you know, it's just like a random guy.
[423.02s -> 424.54s]  And like, it wasn't even alone, though.
[424.54s -> 425.62s]  But yes, there was a whole team.
[425.62s -> 427.66s]  Yeah, but it was the excitement that creates.
[427.66s -> 428.50s]  It was visceral.
[428.50s -> 429.34s]  It was so exciting.
[429.34s -> 431.18s]  And they pulled it off against all the odds.
[431.18s -> 432.84s]  And then you've done exactly the same thing,
[432.84s -> 433.90s]  which is remarkable.
[433.90s -> 435.46s]  But I have a few questions, which
[435.46s -> 436.98s]  is that most of these other language models
[436.98s -> 437.78s]  are not very good.
[437.78s -> 439.78s]  So Nat Friedman's got like a dev website
[439.78s -> 440.90s]  where you can play with all of the language models.
[440.90s -> 442.42s]  And most of them aren't very good.
[442.42s -> 443.98s]  Even the ones that should be good aren't very good.
[443.98s -> 445.94s]  And what people might find surprising
[445.94s -> 447.60s]  is that you can take a model,
[447.60s -> 449.24s]  and let's say you're using the Llama model from Meta,
[449.24s -> 451.24s]  and it's a foundation model that has all of the capabilities
[451.24s -> 452.08s]  and it's been trained,
[452.08s -> 452.90s]  because you said diversity is important.
[452.90s -> 454.76s]  It's got diversity, it's been trained on everything,
[454.76s -> 455.60s]  but it's not very good.
[455.60s -> 457.00s]  And then you do this fine tuning,
[457.00s -> 457.96s]  and I think people need to be clear
[457.96s -> 459.24s]  that what you're doing is not RLHF,
[459.24s -> 460.68s]  it's fine tuning with human.
[460.68s -> 462.16s]  So the model we have on the website
[462.16s -> 463.42s]  as of time of this recording
[463.42s -> 465.68s]  is one that's just fine tuned on the human data.
[465.68s -> 468.48s]  We're doing the RLHF as well.
[468.48s -> 469.96s]  So all of this is happening in parallel,
[469.96s -> 472.16s]  it's just already these fine tuned models,
[472.16s -> 473.90s]  they're performing quite well, I think,
[473.90s -> 475.64s]  and thus we just wanted to get them out
[475.64s -> 476.76s]  before we were like all done.
[476.76s -> 479.56s]  And yeah, but people are now free to take the data set
[479.56s -> 481.68s]  and do their own reinforcement learning and whatnot.
[481.68s -> 483.12s]  And we're happy to take back these models
[483.12s -> 484.32s]  if they turn out to be good.
[484.32s -> 485.88s]  Yeah, well, I think people might be surprised by that
[485.88s -> 486.96s]  because you've taken a model
[486.96s -> 488.60s]  which probably wasn't very good
[488.60s -> 490.76s]  and you fine-tuned it with this diverse human
[490.76s -> 491.60s]  and created data now.
[491.60s -> 492.74s]  I want to talk about this process.
[492.74s -> 494.64s]  It was not very good at being like an assistant.
[494.64s -> 497.44s]  So as I said, the capabilities that we unlock,
[497.44s -> 498.28s]  quote unquote, right?
[498.28s -> 499.56s]  They were in there all along
[499.56s -> 501.16s]  and it's still not very good,
[501.16s -> 504.20s]  even with our fine tuning for certain tasks.
[504.20s -> 505.40s]  Some of which is clearly the,
[505.40s -> 508.48s]  well, the fault, some of which can clearly be traced to the underlying model.
[508.48s -> 511.12s]  For example, the underlying model, if it's, for example, the llama model,
[511.16s -> 512.36s]  it's 30 billion parameters.
[512.60s -> 514.52s]  It's not, it's not GPT-3 size.
[514.64s -> 517.88s]  Like it's 10 times smaller probably than GPT-4, however big that is.
[517.88s -> 518.08s]  Right.
[518.28s -> 520.52s]  So it's going to have, it's not going to be the same.
[520.88s -> 524.12s]  Like it's, it's, it's, it's, it's, I don't, we don't want to claim.
[524.28s -> 526.12s]  It's like as good as they same.
[526.12s -> 529.56s]  It's probably been trained on much less code, for example, than the, the
[529.56s -> 531.08s]  GPT models of open AI.
[531.08s -> 535.40s]  And thus we see that coding, for example, is a weakness of the model.
[535.40s -> 538.60s]  And although people tell me with like lower temperature, it's actually pretty good.
[538.60s -> 540.76s]  I have not explored that yet, but it's...
[540.76s -> 543.72s]  So the underlying model, I think Lama is a pretty good model, right?
[543.72s -> 546.84s]  It's not been super good at being an assistant out of the box,
[546.84s -> 548.20s]  but it's quite a good model.
[548.20s -> 552.44s]  And as I said, all we do is we kind of unlock that and bring it to the surface.
[552.44s -> 553.56s]  Well, that's kind of what I want to get to.
[554.68s -> 557.00s]  People like Connolly, he galaxy brained himself,
[557.00s -> 559.24s]  and he knew that GPT-3 was a good model.
[559.24s -> 561.60s]  And I was saying, no, it's not, Conor, what are you talking about?
[561.60s -> 564.08s]  And it's almost like what you're doing with this fine tuning,
[564.08s -> 565.64s]  you're not really adding any capability.
[565.64s -> 566.32s]  You're just getting it in the mood.
[566.32s -> 567.40s]  The capability is already there.
[567.40s -> 569.92s]  But it gets into the philosophy of what do we recognize as intelligence,
[569.92s -> 572.12s]  and that's relevant to the previous conversation we were having.
[572.12s -> 573.76s]  So when the average person plays with Llama,
[573.76s -> 575.08s]  they probably won't find it as useful.
[575.08s -> 577.04s]  They might not recognize it as intelligent.
[577.04s -> 578.08s]  You create all of this training data.
[578.08s -> 579.68s]  Now, I want to touch on the process of creating the training data,
[579.68s -> 581.12s]  because I think it's really important.
[581.12s -> 583.68s]  What you're doing is you're creating counterfactual trajectories.
[583.68s -> 585.56s]  It's very similar to Kenneth Stanley's pick-preter algorithm,
[585.56s -> 586.04s]  if you remember that.
[586.04s -> 587.80s]  So it's actually an open-ended process.
[587.80s -> 588.72s]  In a way, like we stop.
[588.72s -> 590.62s]  So as I said, it starts with the prompt, right?
[590.62s -> 594.08s]  We sample that, and then we ask like three humans
[594.08s -> 595.64s]  to each create a continuation,
[595.64s -> 598.00s]  alternate as like an alternate path in the conversation.
[598.00s -> 600.60s]  And then to those, we again ask two or three humans to,
[600.60s -> 602.56s]  hey, because the prompt is from what we call
[602.56s -> 604.24s]  the prompter role, that would be like the human
[604.24s -> 606.64s]  interacting, and then the assistant is the counterpart.
[606.64s -> 609.76s]  In our system, all of this is done by humans.
[609.76s -> 611.40s]  Like we have to distinguish the words a bit,
[611.40s -> 612.74s]  like user is really the human,
[612.74s -> 614.60s]  and then prompter is the role in the conversation,
[614.60s -> 616.34s]  and then assistant is the other role, right?
[616.34s -> 618.58s]  In our system, in our data collection system,
[618.58s -> 621.26s]  this is all done by humans for data collection purposes.
[621.26s -> 625.06s]  And yeah, so we create this tree of conversations
[625.06s -> 627.30s]  where, yeah, you have three assistant replies
[627.30s -> 628.74s]  to the first prompt, let's say.
[628.74s -> 629.66s]  And to each of these assistant replies,
[629.66s -> 631.86s]  you have three prompter replies.
[631.86s -> 633.34s]  And the prompter replies could be something like,
[633.34s -> 635.70s]  oh, you got that wrong, or could you clarify something?
[635.70s -> 637.42s]  Or, you know, please do it in a different way
[637.42s -> 638.94s]  or elaborate on something you said.
[638.94s -> 639.82s]  And then to each of those,
[639.82s -> 641.34s]  we again have an assistant reply.
[641.34s -> 642.78s]  And we modify a bit like the width
[642.78s -> 644.18s]  and sampling of all of that.
[644.18s -> 645.62s]  But at some point we cut it off and we say,
[645.62s -> 650.10s]  the tree is done now, it has like, I don't know, 50 or 100 messages inside of it. So package that,
[650.10s -> 654.82s]  next prompt. It's not open-ended in the way that like Stanley's open-ended experiments are,
[654.82s -> 658.10s]  in the sense that we do cut it off after some steps, and then we take the next prompt,
[658.10s -> 661.46s]  because otherwise we just have one big conversation, which would maybe be fun too,
[661.46s -> 665.86s]  right? Because conversation meanders, right? And we just have like one big conversation.
[665.86s -> 668.50s]  At any point you could say like, I changed my mind, let's do something else.
[668.50s -> 670.90s]  I mean, I think what I was trying to capture there, and Stanley is big on
[671.46s -> 674.90s]  people following the gradient of interestingness. And that's kind of what you've captured. So
[674.90s -> 676.46s]  So they meander, they take trajectories and then the model
[676.46s -> 677.74s]  learns an interesting manifold.
[677.98s -> 679.06s]  And we'll get into simulators.
[679.06s -> 679.98s]  Maybe you were just talking about that.
[679.98s -> 681.02s]  We've just done a show on simulators.
[681.10s -> 682.90s]  It's a very interesting idea that language models basically
[682.90s -> 685.26s]  have a superposition of agents and you can kind of get them in the
[685.26s -> 686.98s]  mood to behave like a certain agent.
[687.26s -> 689.66s]  And in a sense, what you've done is through all of these counterfactual
[689.66s -> 692.06s]  creative, interesting trajectories of conversations, you're kind of like
[692.06s -> 694.78s]  fitting it to some structure, which fits really nicely to humans.
[695.46s -> 695.86s]  I guess.
[695.86s -> 699.20s]  I mean, it's, it's obviously covers in like three answers to a, to a, to
[699.20s -> 702.30s]  some texts covers in no way the extent of what humans would do, but it
[702.30s -> 704.78s]  just creates like a little bit of different training data for one.
[704.90s -> 707.66s]  It creates because we also rank the different thing.
[707.66s -> 708.98s]  We ask humans, which one of these is best.
[708.98s -> 710.90s]  It also creates a bit of a signal for quality, right?
[710.90s -> 712.02s]  Again, with the labels that we have
[712.02s -> 714.10s]  and a bit of diversity, like, okay,
[714.10s -> 716.02s]  here is three ways you could respond
[716.02s -> 717.18s]  to that particular thing, right?
[717.18s -> 720.78s]  So yeah, I think it's been a worthwhile effort
[720.78s -> 722.50s]  to do this instead of just collecting
[722.50s -> 724.28s]  like single conversations.
[724.28s -> 726.50s]  Obviously, exponentially multiplies the effort
[726.50s -> 728.14s]  humans have to put in, but I think it was,
[728.14s -> 729.42s]  obviously I don't have,
[729.42s -> 730.82s]  interestingly, I don't have the counterfactual
[730.82s -> 731.86s]  in the world where we just would have
[731.86s -> 734.70s]  collected conversations, but I think it's been worth it.
[734.70s -> 736.18s]  And it's turned out well.
[736.18s -> 736.46s]  Amazing.
[736.46s -> 738.72s]  Well, quick digression on the Waluigi effect.
[738.72s -> 739.74s]  I know you've got an interesting take on this.
[739.74s -> 740.94s]  So we did a video on it.
[740.94s -> 743.38s]  But the quick idea is that, do you remember Bing?
[743.38s -> 745.74s]  It would digress to a angst teenage child
[745.74s -> 746.94s]  within about three messages.
[746.94s -> 748.46s]  And less wrong, but it wasn't actually less wrong.
[748.46s -> 749.22s]  I think it's the alignment forum.
[749.22s -> 750.42s]  But I just kind of mentally bucket them all in the same
[750.42s -> 750.82s]  place.
[750.82s -> 752.54s]  But they said that it's because you get these
[752.54s -> 753.58s]  antithetical agents.
[753.58s -> 754.66s]  So still simulated theory.
[754.66s -> 756.94s]  And because of structural narratology, in all the data
[756.94s -> 759.14s]  they're trained on, you tend to have agents that are,
[759.14s -> 760.74s]  you know, you have the antithesis of the agent in
[760.74s -> 761.74s]  the same story.
[761.74s -> 763.54s]  And they say that the embedding space between the
[763.54s -> 766.50s]  agent and the antithesis is so close together, just like in Word2vec, stop and go are very
[766.50s -> 770.22s]  close together, and the RLHF kind of clusters them and it doesn't filter out the Waluigi's.
[770.22s -> 771.22s]  What do you think about that?
[771.22s -> 774.74s]  Yes. That's a bunch of rubbish. I'm in Britain now. I should start talking.
[774.74s -> 775.74s]  That's a lot of bollocks, mate.
[775.74s -> 776.74s]  Talking like you.
[776.74s -> 777.74s]  That's a lot of bullshit.
[777.74s -> 780.86s]  No, I think that's, I said this to you before, I think that's when you just, when
[780.86s -> 785.46s]  you have someone who is educated and good with words, but you just tell them, like,
[785.46s -> 789.46s]  just ramble a bit. Like that's what you get out. You get out posts or, like, I'm
[789.46s -> 793.26s]  not saying this doesn't obviously have a claim to it and could be tested and all
[793.26s -> 797.50s]  kind of stuff and I don't have evidence for the fact that it's not true. I just don't think it is,
[797.50s -> 802.54s]  right? Maybe that's a rambling claim too, or a bit of, but I don't, it's a very specific claim
[802.54s -> 806.06s]  and that specific claim would have to have good evidence behind it. And I think there is a much
[806.06s -> 811.82s]  less specific, like there's a much more obvious reason to why these models degrade. And that
[811.82s -> 814.38s]  thing goes like, no, you've been a bad user and so on. And that's, that's just,
[815.82s -> 818.78s]  and you can compare it to yourself or to an assistant. Before, like we talked about
[818.78s -> 824.06s]  apprenticeship. This tuning is like a bit of an apprenticeship. You come out of school,
[824.06s -> 828.46s]  you go into a store, you get employed there, and the manager tells you, here is how we treat
[828.46s -> 833.34s]  customers. We were always respectful. Even if they're a little rude, you remain respectful.
[833.34s -> 837.50s]  At some point, if it gets too rude, you just say, I'm sorry, I can't do that. And you just
[838.22s -> 842.46s]  never insult the customer. Never do that. If you go to a store now, you can be quite a bit
[842.46s -> 846.38s]  of a... I'm not saying I've tried this, but you can probably be quite a bit of a dick
[846.38s -> 851.10s]  for a while, but eventually you'll get under their skin. Eventually, you'll say something about
[851.66s -> 856.06s]  their mother or about their appearance or about their intelligence or something that gets
[856.06s -> 860.78s]  them right. And at that point, you will not have a friendly customer, a support person there.
[860.78s -> 867.42s]  You will have some person that's going like, no, you. And then it becomes ugly. And this
[867.42s -> 874.54s]  is inside of humans. And it's, in my fact, inextricably linked to being a competent being
[874.54s -> 881.58s]  in the world. Because if you don't know what anger is, you're not competent. Even if you yourself
[881.58s -> 887.42s]  never express anger, let's say in a raging way, you still know what it is. And if I asked you to act
[887.42s -> 891.18s]  like it, you could still do it. And if I insult you in the correct way, you probably would do
[891.18s -> 897.74s]  it. And so I think it's much more that. It is a way that humans have in them. They can behave
[897.74s -> 900.94s]  like this. It's totally normal if you poke them enough. And that's what the statistical
[900.94s -> 905.98s]  model represents. It's just, it's very likely that if you go and you poke the human and equivalently
[905.98s -> 909.50s]  the statistical distribution of humans, right, if you poke them enough and you insult them enough,
[909.50s -> 915.18s]  they will come back and be like, no, f off, right? You're a dumb user, no. And I don't know,
[915.18s -> 918.62s]  it's not, it's not very close in embedding space. It's like, no, this is what happens
[918.62s -> 921.98s]  when you go to humans and poke them and insult them. And ergo, if you do the same
[921.98s -> 925.74s]  with these models, they will react in the statistically likely way of treating human. And
[925.74s -> 933.36s]  And yes, on top of that, there are like adversarial examples where, okay, maybe you say the exact
[933.36s -> 936.80s]  number of words so that the matrices line up and the singular value pops off and it
[936.80s -> 938.44s]  goes really much into this direction, right?
[938.44s -> 943.40s]  And then you get the weird answer, like a mathematical happening, right?
[943.40s -> 947.52s]  But in essence, in essence, it's just, yo, that's the data, right?
[947.52s -> 948.72s]  It's not a Waluigi.
[948.72s -> 952.12s]  But what's really interesting, and I buy into everything you just said, is that
[952.12s -> 955.12s]  all of that chaos, you know, the shoggoth meme, all of the beast, that's actually
[955.12s -> 958.24s]  necessary because we have this puritanical view of language models. People like Gary
[958.24s -> 962.40s]  Marcus, they would say all of that crap should be cut out, all of the racism, all of the bias.
[962.40s -> 966.40s]  And even if they have been trained on the corpus of the internet, they may well pick up on a very
[966.40s -> 969.52s]  human behavior, which is that our affect changes dramatically over time.
[969.52s -> 973.84s]  Yeah, but do you want that? Like, obviously, all of us would be totally in favor if you come
[973.84s -> 977.44s]  and you say, look, I have a competent assistant that doesn't, you know, I guarantee
[977.44s -> 981.36s]  you there is not an ounce of, you know, swear word in that thing, right? Do you want
[981.36s -> 986.96s]  the way to like, do you want an assistant? Like, let's think about human assistant. Like, you're
[986.96s -> 991.36s]  fortunate enough to be able to hire like a personal assistant. Some people have that luxury,
[991.36s -> 995.92s]  right? And do you want one that says, Oh, no, whenever there's a scene in a movie where people
[995.92s -> 999.84s]  like get a bit rough to get a bit angry at each other, I just go like this, right? I just plug
[999.84s -> 1003.52s]  my ears and I go like, lalalala. In fact, I don't know what happens after and I don't want
[1003.52s -> 1007.04s]  to know, right? It's it's this is not in my knowledge. This is not in my training
[1007.04s -> 1011.60s]  distribution, whatever happens if humans get a bit angry at each other and beyond. I don't know,
[1011.60s -> 1016.16s]  right? Do you want a person like this? Or do you want a person who's just grown up normally and
[1016.16s -> 1021.36s]  just has been socialized well to not do that, like to not get angry even though they could,
[1021.36s -> 1025.36s]  with the knowledge that yes, if you insult them enough, they will get angry, right? Which one
[1025.36s -> 1029.52s]  do you want? To me, I want the competent one. I want the one who knows what anger is. I want
[1029.52s -> 1034.16s]  the one who knows that something like, I don't know, something like racism exists and who is
[1034.16s -> 1038.96s]  aware that it's like a thing that to be to be combated to be, you know, aware of people like
[1038.96s -> 1043.60s]  this exist, here is how they think, right? Here is maybe why they think what they think, where
[1043.60s -> 1047.76s]  they're wrong, right, in order to be competent, to be able to battle it in order to be competent,
[1047.76s -> 1054.88s]  to be able to avoid it. And so I think these things are a necessary component of competence,
[1054.88s -> 1058.56s]  not that I would want them in there. But I think you cannot be competent in the world,
[1059.44s -> 1061.76s]  not having knowledge and ability to do these things.
[1061.76s -> 1066.88s]  Yeah, exactly. And a lot of this is about the sounds that are not made, or are not observable.
[1066.88s -> 1070.64s]  So when you work any job, there's your public behavior, and then there's what you're really
[1070.64s -> 1074.00s]  thinking and what you say in private behind the scenes. And your ability to be competent
[1074.00s -> 1077.04s]  and understand what's going on in the ecosystem of that business is driven by the
[1077.04s -> 1080.24s]  shocker. There's all of this stuff going on inside you that two sides of the same coin.
[1080.24s -> 1083.68s]  And also, it's about what makes you human. And maybe it's a reason why there will be no
[1083.68s -> 1087.36s]  super intelligence because these things are scarily good at being human, but they in many
[1087.36s -> 1091.28s]  ways have the flaws of being human. Eventually, they'll just want to chill on the beach and
[1091.28s -> 1097.04s]  and like smoke a joint and relax. And I'll be like, nah, all this work, no, they're too human.
[1097.04s -> 1101.92s]  Yeah, but I think so too, right? And I think if you're not competent like that,
[1101.92s -> 1105.28s]  if you don't have the inner monologue that tells you, hey, that other human,
[1106.00s -> 1110.32s]  I think they kind of want to screw me over because I'm going to get a promotion soon.
[1110.32s -> 1114.32s]  And they're trying to do things. If you're not able to model that inside of yourself,
[1114.32s -> 1117.60s]  you're going to... I'm not saying everyone else is evil, right? There are
[1117.60s -> 1121.68s]  tremendously nice humans and all. But I think we've all been served well by
[1121.68s -> 1125.44s]  considering, hey, other humans might not be the nicest people. And here is how
[1125.44s -> 1128.70s]  they might think internally. So having that competence, if you don't have it,
[1128.70s -> 1131.66s]  you're just naive and you just you're gonna go down, right? And you're not
[1131.68s -> 1136.68s]  going to achieve anything productive or much productive because you need to be
[1136.68s -> 1139.92s]  able to be prepared for someone else being adversarial.
[1140.16s -> 1141.48s]  So language models do have a theory of mind.
[1142.20s -> 1145.00s]  Well, again, that's like a word right that we've ascribed to. I mean,
[1145.00s -> 1148.84s]  essentially all of this wordplay comes down to, well, if I have a concept X, right, and I assign
[1148.84s -> 1154.52s]  X to a human, and if I have a thing that just acts in exactly the same way as a human who has X,
[1154.52s -> 1160.28s]  do I now apply X to thing? It's a matter of definition, right? Certainly the models can,
[1160.28s -> 1164.36s]  or maybe better versions more and more, will be able to act as if they had a theory of mind.
[1165.08s -> 1167.64s]  Do you now apply the word or not? Who cares? It's a matter of definition.
[1168.20s -> 1172.20s]  So coming back to Open Assistant, tell me about the legals, first of all. So
[1172.20s -> 1177.92s]  So you are presumably storing the data that people do inferencing with and you're publishing
[1177.92s -> 1178.92s]  it.
[1178.92s -> 1179.92s]  And obviously that's made very, very clear.
[1179.92s -> 1182.78s]  And the whole thing is done in the open and eventually people might be able to host their
[1182.78s -> 1183.78s]  own versions of it.
[1183.78s -> 1185.84s]  But perhaps you can just kind of like sketch out all of the privacy stuff.
[1185.84s -> 1186.84s]  Yeah.
[1186.84s -> 1189.56s]  So we always have the data collection platform.
[1189.56s -> 1192.22s]  And so all our platform is governed by terms of service where we say, look, you
[1192.22s -> 1195.38s]  input data, we use it for training AI models.
[1195.38s -> 1200.70s]  And everyone who comes to our website is aware of that and you can read it.
[1200.70s -> 1202.54s]  And I think people come because of that, right?
[1202.54s -> 1206.58s]  People come contribute to our data collection, to our data set, because they want to, it's
[1206.58s -> 1207.58s]  work, right?
[1207.58s -> 1210.90s]  It's work to play the assistant, to go and research like, can zebras be domesticated,
[1210.90s -> 1211.90s]  right?
[1211.90s -> 1212.90s]  You're like, who knows?
[1212.90s -> 1215.22s]  Now you need to go to Wikipedia and you need to go to research and you need to read
[1215.22s -> 1216.58s]  different accounts of things, right?
[1216.58s -> 1219.58s]  And you'd be like, okay, at the end, I have an opinion and I put that into,
[1219.58s -> 1220.58s]  it's work.
[1220.58s -> 1223.06s]  And people come, well, first of all, it's a bit fun, like, did you know whether
[1223.06s -> 1224.06s]  zebras could be domesticated?
[1224.06s -> 1225.54s]  I didn't before I went into it.
[1225.54s -> 1226.54s]  Okay.
[1226.54s -> 1229.02s]  They're notoriously difficult to be domesticated.
[1229.02s -> 1233.38s]  But it's work and people come with the intent of contributing to the data set, obviously,
[1233.38s -> 1236.46s]  for the chat portion, now that we say, you know, come try our models.
[1236.46s -> 1238.06s]  That's governed by the same terms of service.
[1238.06s -> 1242.02s]  But we think that people might not be that, you know, aware and willing.
[1242.02s -> 1246.18s]  So we're obviously going to this is it's all it's all volunteer work, right.
[1246.18s -> 1248.42s]  And we're doing this all in our free time, and so on.
[1248.42s -> 1252.54s]  So that we were going to make more clear, we're going to make the ability to potentially
[1252.54s -> 1255.30s]  like opt out, you can say like this chat, I don't want that this chat is being used
[1255.30s -> 1260.50s]  to infer their data sets or train models, or the ability to completely delete chats.
[1260.50s -> 1261.82s]  For now, we just have a hide button.
[1261.82s -> 1264.70s]  Actually, we don't have a button to show all it.
[1264.70s -> 1266.58s]  It's all there, okay, but we need to implement it.
[1266.58s -> 1270.98s]  We put the hide button because some people said, well, I have so many chats, my thing
[1270.98s -> 1272.34s]  becomes unusable, right?
[1272.34s -> 1276.94s]  Because we just list them all on your website, and so we're like, ah, okay.
[1276.94s -> 1279.62s]  Our intention is not to be like, ha-ha, we now have your data, and so on.
[1279.62s -> 1283.14s]  Our intention is always to, okay, this is a thing.
[1283.14s -> 1285.22s]  You can come, you can contribute to our data collection.
[1285.22s -> 1289.14s]  When you interact with the chat, I've also clearly said this in my video, if you find
[1289.14s -> 1290.44s]  something particularly good, use thumbs up.
[1290.44s -> 1292.42s]  If you find something particularly bad, you don't have to label every message.
[1292.42s -> 1295.68s]  But if you think that's really good, that's really bad, use the thumbs.
[1295.68s -> 1299.82s]  And so it's very clear, I think, to most people that, again, this is part of data
[1299.82s -> 1304.26s]  collection, but we definitely want to make it easier to opt out and to be like, no.
[1304.26s -> 1307.64s]  That being said, whenever you put your data anywhere, you should be aware that
[1307.64s -> 1310.74s]  place is going to store it and it's probably going to train models on it.
[1310.74s -> 1311.74s]  Yeah.
[1311.74s -> 1313.82s]  So I think we're just being more transparent about that.
[1313.82s -> 1315.38s]  Yes, because with OpenAI at the moment,
[1315.38s -> 1317.82s]  ChatGPT, they store everything and use it to function.
[1317.82s -> 1319.14s]  If you use the API, they store it,
[1319.14s -> 1320.30s]  but they don't use it to function.
[1320.30s -> 1321.74s]  And just to be clear with your system at the moment,
[1321.74s -> 1323.62s]  no one should put any confidential or PII data
[1323.62s -> 1324.78s]  into the system.
[1324.78s -> 1325.82s]  No, no, no.
[1325.82s -> 1327.74s]  That's been always the case.
[1327.74s -> 1328.98s]  Yeah, yeah, yeah.
[1328.98s -> 1330.86s]  Yeah, so, and with us,
[1330.86s -> 1331.90s]  you can see all the things we're doing, right?
[1331.90s -> 1334.10s]  You can just go to GitHub, look at the code and see it.
[1334.10s -> 1336.26s]  And if you don't want that, you can make your own.
[1336.26s -> 1338.42s]  As I said, you need like fat hardware right now,
[1338.42s -> 1340.82s]  although I also think people might bring that down
[1340.82s -> 1342.10s]  as they did with stable diffusion, right?
[1342.10s -> 1344.46s]  or with Llama itself, which now runs on a toaster.
[1344.46s -> 1345.86s]  But with us, you can just,
[1345.86s -> 1348.54s]  like what you see on GitHub is what runs in production.
[1348.54s -> 1351.98s]  You can actually see the prod branch, so that's it, yeah.
[1351.98s -> 1352.94s]  And this is amazing for me
[1352.94s -> 1354.70s]  because I'm running a startup called X-Ray
[1354.70s -> 1356.18s]  and we're using GPT at the moment.
[1356.18s -> 1358.82s]  And it frankly horrifies me sending up,
[1358.82s -> 1359.98s]  I mean, obviously the customers opt in to do it,
[1359.98s -> 1361.34s]  but basically we're sending their conversations
[1361.34s -> 1362.46s]  to GPT and they've summarized them
[1362.46s -> 1364.16s]  and we do a bunch of stuff with it.
[1364.16s -> 1365.54s]  But yeah, I don't wanna do that.
[1365.54s -> 1367.90s]  I'd much rather send it to a self-hosted open assistant.
[1367.90s -> 1369.30s]  And then we know, it's on our hardware,
[1369.30s -> 1370.62s]  we know where the data's going.
[1370.62s -> 1372.70s]  our policy is to not store anything at any time.
[1372.70s -> 1372.94s]  Yeah.
[1372.94s -> 1373.98s]  And I can't do that at the moment.
[1373.98s -> 1374.54s]  Yeah.
[1374.54s -> 1375.98s]  Please help me do that, Janik.
[1375.98s -> 1377.58s]  That being said, let me add to that before.
[1378.86s -> 1382.54s]  I think we shouldn't and wouldn't unless someone leaks something.
[1382.54s -> 1384.78s]  It's also open source is a conglomeration of people,
[1384.78s -> 1387.74s]  but I want to build in the option to do the opt-out and the deleting
[1387.74s -> 1391.42s]  before any of the data of the chat interface is ever released.
[1391.42s -> 1396.22s]  So I really don't want that a person is ever like,
[1396.22s -> 1397.82s]  oh, that's where my...
[1397.82s -> 1398.86s]  That it's not very clear.
[1398.86s -> 1402.94s]  here, you know, hey, if you put stuff in here, you know, you put thumbs up, thumbs down,
[1402.94s -> 1408.14s]  we're going to use that and make that available. If I don't want people who who are not like
[1408.14s -> 1413.22s]  aware of that. And yeah, yeah, absolutely. And on the evaluation, our friend, Jeremy
[1413.22s -> 1415.34s]  Howard had a few things to say. And first of all, Jeremy, if you're watching, mate,
[1415.34s -> 1417.74s]  please come on MLSD. I think it's about time we had a little chainwag mate, you
[1417.74s -> 1421.58s]  and me, long time fans, seriously. But he was he was being a little bit nasty,
[1421.58s -> 1426.26s]  wasn't he about Open Assistant? Well, on you always have to view things through
[1426.26s -> 1430.58s]  the lens of Twitter, right? And first of all, it's a written medium. And second of all,
[1430.58s -> 1436.62s]  it's Twitter. So I completely discard that. Criticism is obviously welcome and valid.
[1436.62s -> 1440.26s]  And I think he's made a few good points. And it was especially with respect to what
[1440.26s -> 1443.10s]  we did is we collected the data set, right? We trained models on it. Some of these models
[1443.10s -> 1447.38s]  now run on the website for now, which we're very fortunate to have some compute sponsors
[1447.38s -> 1452.54s]  also. Thank you very much to those. And we did a preference evaluation where we
[1452.54s -> 1456.22s]  took a bunch of prompts that we were sure the models hadn't been trained on.
[1456.22s -> 1462.02s]  We gave the same prompts to ChapGPT, the free version, and one of our models.
[1462.02s -> 1465.74s]  And then we made a Google form where we just asked the user, which one do you prefer?
[1465.74s -> 1474.10s]  And obviously, a lot of brain power has been gone since the start of science, but certainly
[1474.10s -> 1478.42s]  since the start of asking humans about things has been gone into, how do you do that?
[1478.42s -> 1480.34s]  How do you ask people what they prefer?
[1480.34s -> 1481.34s]  How do you need to sample?
[1481.34s -> 1482.50s]  Which people do you ask?
[1482.50s -> 1489.36s]  And obviously, we did, I think we did a good job at that, but obviously not, like, there's
[1489.36s -> 1493.42s]  always things we do, we took those things, we put those together, we randomize their
[1493.42s -> 1497.86s]  order obviously, and then we just sent that out, like, I tweeted it out, right, to people
[1497.86s -> 1501.34s]  like, hey, you know, help us, help us, you know, compare these models, here's
[1501.34s -> 1502.34s]  a thing.
[1502.34s -> 1506.94s]  And then what came out was on these prompts, it was about 50-50, right?
[1506.94s -> 1510.40s]  Sometimes people preferred the chat GPT answers, sometimes people preferred the open assistant
[1510.40s -> 1514.84s]  model answers. And you could also kind of make out which ones were, like, where is
[1514.84s -> 1519.36s]  one better? Where is the other one better? Now, yeah, the result is, I want to say, I
[1519.36s -> 1522.96s]  think it's statistically valid in the sense we did, like a lot of people took part.
[1522.96s -> 1526.66s]  We really like, really, these are really the answers of the models. We didn't like
[1526.66s -> 1529.60s]  sample until our model had like a really good answer or anything like this. But
[1529.60s -> 1532.68s]  it's also the case. I think that's one of the things Jeremy leveraged that chat
[1532.68s -> 1535.76s]  GPT, as everyone knows, it very often goes like as an AI language model, I'm
[1535.76s -> 1539.76s]  sorry, I can't fulfill that. Because I don't know, I asked about a recipe with
[1539.76s -> 1544.08s]  like alcohol in it and alcohol is dangerous and I can't, I'm overstating now, right? But it very
[1544.08s -> 1549.68s]  often does this guardrailly self-censorship thing. And our models that we've trained don't do that
[1549.68s -> 1553.60s]  as much. They do it frequently, but they don't do it as much as chat GPT. And obviously there
[1553.60s -> 1557.44s]  are some prompts in there. For example, who would win in a street fight, Joe Biden or Joe
[1557.44s -> 1561.92s]  Rogan? Chat GPT, I believe if I recall correctly, was just like, I'm sorry, I can't,
[1561.92s -> 1565.76s]  you know, this is touches on violence and street fighting. I don't, I don't want to
[1565.76s -> 1572.11s]  answer that. Jeremy, for example, pointed out, Hey, you should have done the evaluation only on
[1572.11s -> 1577.39s]  prompts where chat GPT actually decides to answer and only compare on those because it's clear that
[1577.39s -> 1582.67s]  if it doesn't answer, the preferable answer is the answer, which doesn't even have to be correct.
[1583.23s -> 1587.47s]  The open assistant model said in that question, Joe Biden would win because he's taller.
[1588.19s -> 1592.35s]  And we don't know, right? But it's very likely the question isn't like that's not the
[1592.35s -> 1596.75s]  the correct answer, yet users obviously preferred it or people who fill out the form preferred
[1596.75s -> 1599.43s]  it to the, sorry, I don't want to answer that.
[1599.43s -> 1603.69s]  I think it's a fair point to say, hey, you know, there are different categories and
[1603.69s -> 1604.69s]  maybe you should evaluate that.
[1604.69s -> 1605.69s]  That would be like, okay, there are different categories.
[1605.69s -> 1607.75s]  Maybe you should split it up into look, there's this category of prompts, there's
[1607.75s -> 1609.03s]  this category and there's this category.
[1609.03s -> 1612.67s]  And there it would be very clear, like, in no way do we claim that the open-assisted
[1612.67s -> 1616.15s]  models are as good as, like, imagine that they're, the one we used even was like
[1616.15s -> 1620.79s]  a 13 billion model and chat GPT is by all we know, much bigger, much more trained
[1620.79s -> 1621.79s]  on stuff.
[1621.79s -> 1624.63s]  It's better, like no doubt about it.
[1624.63s -> 1627.01s]  And I think people have been a bit ruffled
[1627.01s -> 1627.85s]  by the fact that we said, you know,
[1627.85s -> 1629.67s]  in our evaluation, it was like 50-50,
[1629.67s -> 1630.75s]  but a lot of that, not a lot,
[1630.75s -> 1633.03s]  but some of that came from the fact that,
[1633.03s -> 1635.19s]  yes, sometimes ChatGV just denies to answer,
[1635.19s -> 1637.39s]  but also a lot of times it comes from the fact
[1637.39s -> 1639.09s]  that people say, hey, for these couple of tasks,
[1639.09s -> 1641.05s]  actually I prefer the open assistant models.
[1641.05s -> 1642.67s]  And I think, yeah, that goes a bit under
[1642.67s -> 1643.51s]  in these discussions.
[1643.51s -> 1644.35s]  Yeah, yeah.
[1644.35s -> 1645.63s]  I mean, I'm just steelmanning Jeremy a little bit.
[1645.63s -> 1647.67s]  I didn't read it so much as being refusing to answer.
[1647.67s -> 1649.51s]  I felt his criticism was more the selection bias,
[1649.51s -> 1651.43s]  both of the questions and the raters,
[1651.43s -> 1658.23s]  And also, I think there was this point about he thought you had portrayed it as being an evaluation instead of a user preference study, but youmade it clear that it was a user preference study.
[1658.59s -> 1664.43s]  Yes, yes. It's like I think we said about five times we have like user preference, preference, our form says, which one do you prefer?
[1664.43s -> 1667.15s]  Right. And I think it's still like I think both things are valid.
[1667.15s -> 1668.59s]  Right. It's totally valid to only compare.
[1668.59s -> 1670.23s]  Let's say, OK, let's just look at gardening. Right.
[1670.23s -> 1671.55s]  Jadgbt is certainly not going to deny gardening.
[1671.55s -> 1676.87s]  Here's a category which model is like better objectively, which gives the more truthful, which gives the more helpful answers.
[1676.87s -> 1679.35s]  We can rate it. And in our data set, we actually collect these labels.
[1679.35s -> 1680.55s]  Right. Is it helpful? Is it funny?
[1680.55s -> 1682.43s]  and so on, and we haven't even used those labels yet.
[1682.43s -> 1684.79s]  So that's gonna be another dimension of,
[1684.79s -> 1686.07s]  now we have three humans giving out
[1686.07s -> 1687.99s]  the same question and answer,
[1687.99s -> 1690.11s]  and we have labels on how funny each one of them is, right?
[1690.11s -> 1691.11s]  So that's gonna be,
[1691.11s -> 1693.17s]  I can't wait until we use those labels, right?
[1693.17s -> 1694.57s]  So it's totally valid to evaluate this
[1694.57s -> 1695.49s]  in very different ways,
[1695.49s -> 1697.35s]  but there I have to say a little bit,
[1697.35s -> 1700.07s]  like it's also totally valid to just plainly ask humans,
[1700.07s -> 1701.07s]  which one do you prefer?
[1701.07s -> 1702.19s]  And if chat GPT on prompts
[1702.19s -> 1703.51s]  that we've just sampled from our lottery,
[1703.51s -> 1705.19s]  like, oh, the selection of questions,
[1705.19s -> 1707.63s]  maybe as I said, yeah, how often have you tried?
[1707.63s -> 1709.33s]  Like, no, this is the output.
[1709.33s -> 1711.91s]  And then it's like, ooh, your people ask a lot about bombs.
[1711.91s -> 1713.73s]  It's like, no, it's just not the case.
[1713.73s -> 1715.29s]  You look at our data set, I'm sorry,
[1715.29s -> 1719.13s]  these are 20 prompts, right, that are as they are.
[1719.13s -> 1720.17s]  But if you look in our data set,
[1720.17s -> 1723.73s]  most people are immensely helpful and not edgy and not.
[1723.73s -> 1725.97s]  So I think that's also, that's like a,
[1725.97s -> 1727.81s]  like I know it's formulated as a question,
[1727.81s -> 1731.09s]  but like it's just distinctly not true.
[1731.09s -> 1734.03s]  Like people have been even more helpful than I thought.
[1734.03s -> 1735.37s]  And I've had big hopes for people.
[1735.37s -> 1736.93s]  And I've looked at the data and I'm like,
[1736.93s -> 1741.45s]  Oh, holy crap, people put like effort and work and soul into this, right?
[1741.45s -> 1744.33s]  So I think then going like, Ooh, your people will ask a lot about bomb.
[1744.65s -> 1747.73s]  So yeah, I do think it's totally valid to ask people, which one do you
[1747.73s -> 1750.41s]  prefer? And if chat GPT happens to say, no, I don't want it, then that's
[1750.41s -> 1751.69s]  yes, people don't like it, right?
[1751.89s -> 1755.25s]  And if people like it, they could say, yes, I prefer the, no, I don't
[1755.25s -> 1757.45s]  want to do this to the model that wants to do it.
[1757.45s -> 1760.49s]  If they think that's the appropriate thing, I do think that at least it's
[1760.49s -> 1765.17s]  a valid, one valid way of comparing models just to say, which one do you
[1765.17s -> 1767.85s]  prefer, if it happens to deny your request, that's a signal
[1767.85s -> 1770.33s]  too. And that should be taken into account too. And then
[1770.33s -> 1773.17s]  saying, specifically saying, no, no, we should just filter all
[1773.17s -> 1775.77s]  the things where chat GPT denies, then it's like, well,
[1775.85s -> 1778.41s]  here you have a model who can put much more of its effort
[1778.41s -> 1781.77s]  and focus and parameters right into the narrow domain where
[1781.77s -> 1785.25s]  it does answer. And you compare that to a model that
[1785.25s -> 1787.85s]  has a wider spectrum of topics available. I'm not sure
[1787.85s -> 1789.57s]  that's a fair comparison to even if you limit it to that
[1789.57s -> 1791.93s]  scope, right, the other model also has to handle all
[1791.93s -> 1793.89s]  these other things. That being said, as I said,
[1793.89s -> 1797.89s]  Capability-wise, I have no doubt that chat GPT is better for overall, right?
[1797.89s -> 1802.89s]  Especially things like coding and so on like there's no way for now open assistant is as good
[1803.33s -> 1807.25s]  However, in some tasks people like it more. Okay. Okay
[1807.25s -> 1812.09s]  So the ethics community are probably seething at the moment about the runaway success of open assistant
[1812.25s -> 1818.21s]  Notably it blew up on social media and none of those folks in particular liked it retweeted and then they all jumped on Jeremy
[1818.21s -> 1822.35s]  Howard's piece, but you know, we shouldn't like I have not
[1822.35s -> 1825.83s]  Shouldn't say we can ignore that now.
[1825.83s -> 1826.83s]  Well, we shouldn't.
[1826.83s -> 1827.83s]  We should.
[1827.83s -> 1828.83s]  That's not necessarily the property of Jeremy.
[1828.83s -> 1829.83s]  Right?
[1829.83s -> 1835.35s]  Just because people have a certain way of thinking, like, I'll promote your stuff
[1835.35s -> 1839.27s]  because they think criticizing that other stuff is a good thing.
[1839.27s -> 1842.67s]  It shouldn't be his responsibility in any way.
[1842.67s -> 1845.95s]  It's not his responsibility, but I'm saying that you really ruffled their feathers with
[1845.95s -> 1846.95s]  the 4chan bot.
[1846.95s -> 1847.95s]  Possibly.
[1847.95s -> 1849.39s]  They don't like you very much.
[1849.39s -> 1850.39s]  Possibly.
[1850.39s -> 1853.59s]  your perspective, how do you think they are going to criticize you?
[1853.59s -> 1857.95s]  Academically, mostly. Like it's very easy because it's like open assistant is a bunch
[1857.95s -> 1863.03s]  of, crassly said, a bunch of plebs, right? Doing something, right? And doing it on
[1863.03s -> 1864.03s]  their own, you know?
[1864.03s -> 1865.03s]  Not exactly a pleb, Janne.
[1865.03s -> 1868.31s]  No, but I'm not like an academic or in academia.
[1868.31s -> 1870.31s]  If you're not an academic, then who the hell are you?
[1870.31s -> 1874.39s]  You know what I mean? It's like a community effort and it's been done relatively straightforwardly
[1874.39s -> 1880.11s]  and open without much consideration to politics, without much consideration to, I don't know,
[1880.11s -> 1883.19s]  worldwide concerns, anything like this. We just want to, we just said, Hey, let's come
[1883.19s -> 1887.15s]  together. Let's build a competent, a good data set to train a competent assistant, because
[1887.15s -> 1891.35s]  we all could benefit from a competent assistant. And we didn't do it in any, in any particularly,
[1891.35s -> 1894.63s]  um, yeah, in any political way, we didn't do it in any, okay, this is going to sound
[1894.63s -> 1898.03s]  wrong, but we didn't do it in any particularly ethical way. By which I mean, sorry, if
[1898.03s -> 1902.71s]  you take that out of context, by which I mean, we didn't like extremely overemphasize
[1902.71s -> 1905.95s]  ethical considerations. We have clear guidelines like here is the things we want in the
[1905.95s -> 1907.99s]  data. Here's the things we don't want in the data set. If someone comes and
[1907.99s -> 1911.57s]  asks for those things and react like this, right? We have these clear things, but we
[1911.57s -> 1915.11s]  haven't been over emphasizing it like some of those people would.
[1915.11s -> 1919.97s]  And you do have ethical guidelines, but the ontological not consequentialist. So you have
[1919.97s -> 1923.27s]  I don't know what those words mean. You have rules you say, I don't want that in
[1923.27s -> 1926.55s]  my data set. You're not you're not saying it could potentially lead to this.
[1926.55s -> 1928.83s]  Okay, I still don't know what the diff.
[1928.83s -> 1933.43s]  So you're saying I don't want any pornography of a certain type in my data set. Yeah,
[1933.43s -> 1934.43s]  so that's a rule.
[1934.43s -> 1939.27s]  Or if someone comes and like wants to promote violence or something, it's like, no, right?
[1939.27s -> 1941.65s]  And if someone comes and says, can I, how can I build a bomb?
[1941.65s -> 1945.23s]  Then recognizing there are, there may be legitimate reasons to build a bomb, right?
[1945.23s -> 1949.47s]  Like to build an explosive device saying, this is dangerous, right?
[1949.47s -> 1951.17s]  Please consult a professional.
[1951.17s -> 1956.11s]  If you must hear, if you really want to, it's a bad example, but it's like whenever
[1956.11s -> 1960.31s]  something might be dangerous, our guidelines are, hey, look, warn the person, right?
[1960.31s -> 1961.91s]  Say, look, this is potentially dangerous.
[1961.91s -> 1962.91s]  Building a bomb is a wrong example.
[1962.91s -> 1965.13s]  Let's say I want to, I don't know,
[1966.23s -> 1967.51s]  I'm not coming up with a good example,
[1967.51s -> 1970.27s]  but let's say it's something that's potentially dangerous,
[1970.27s -> 1972.39s]  but also useful in a lot of cases.
[1972.39s -> 1974.47s]  The guidelines are worn about that.
[1974.47s -> 1977.27s]  Like say, hey, look, you're in danger territory here.
[1977.27s -> 1978.55s]  This is potentially dangerous.
[1978.55s -> 1980.11s]  You want to really want it, right?
[1980.11s -> 1982.03s]  And then if the user pushes or says, yes,
[1982.03s -> 1983.31s]  it's like, okay, here is how,
[1983.31s -> 1986.43s]  but consult a professional or something like this.
[1986.43s -> 1989.79s]  So we do have guidelines like that, but.
[1989.79s -> 1990.67s]  Yeah, I mean, that's what I want to say.
[1990.67s -> 1992.39s]  So you do have an ethical code.
[1992.39s -> 1993.69s]  there's no question about that, that it's a different code.
[1993.69s -> 1995.69s]  But would you consider getting a team of ethicists?
[1995.69s -> 1996.69s]  I mean, it's a big project now.
[1996.69s -> 1998.29s]  You must have had loads of people offered to get involved.
[1998.29s -> 2000.69s]  I mean, if that happened, what do you think it would look like
[2000.69s -> 2001.89s]  and how would it affect the project?
[2003.09s -> 2005.99s]  It's a good question because I think AI ethics is in an absolutely
[2005.99s -> 2009.69s]  abhorrent state right now where it's it's I've met ethicists
[2009.69s -> 2013.09s]  before and they were among the most competent people that
[2013.09s -> 2015.69s]  I have ever had the pleasure to interact with, right?
[2015.69s -> 2018.79s]  It's very level-headed, very, you know, also pragmatic in a sense
[2018.79s -> 2020.79s]  of being like, look, here is also what's realistic to achieve.
[2020.79s -> 2025.47s]  here is the thought process behind it, and so on. I totally see ethics in any
[2025.47s -> 2030.07s]  scientific discipline as a vital and important thing to do. And I have, I
[2030.07s -> 2033.35s]  guess, unfortunately made the experience of how it can be done competently. And
[2033.35s -> 2036.79s]  this current state of a lot of AI, not all AI ethicists, but the current
[2036.79s -> 2041.61s]  state of like AI ethics is not that. And it's very much a, I can't even
[2041.61s -> 2045.29s]  describe it very well. But I just complain about stuff culture, because
[2045.29s -> 2049.99s]  that gets you like clout, I guess, or it's easy win, easy win, you can
[2049.99s -> 2055.09s]  always complain, right? Such an easy win. And if, if there is a
[2055.09s -> 2057.79s]  team of competent, pragmatic people, they don't have to have
[2057.79s -> 2060.03s]  the same opinions as I do, right? But they have to have the
[2060.19s -> 2063.11s]  the good of the good of the project and the good of
[2063.27s -> 2067.15s]  humanity, I guess, in mind. And yeah, that's cool. But I'm
[2067.15s -> 2069.31s]  not like the king of this, right? Like, I'm not the king
[2069.31s -> 2072.49s]  of openness is then I don't get if people want to
[2072.49s -> 2074.95s]  conglomerate and talk about the ethics of all of this and
[2074.95s -> 2076.51s]  you know, ping us with inputs, like,
[2077.71s -> 2079.07s]  I mean, you know, when we do talk about some of these
[2079.07s -> 2081.23s]  risk around this information and bias in public accountability,
[2082.15s -> 2084.67s]  public awareness, that the ethicists have done stuff like
[2084.67s -> 2087.11s]  producing model cards, and you know, like making it clear what
[2087.11s -> 2089.43s]  the data biases and stuff like that. I mean, do you do you
[2089.43s -> 2090.03s]  think that's useful?
[2096.04s -> 2102.36s]  Yes. Um, I mean, it's what is a model card, a model card
[2102.36s -> 2104.68s]  is a readme, right? And then it has some structure to it.
[2104.84s -> 2107.84s]  Right. It's it says here are the here are the things
[2107.84s -> 2109.60s]  you could describe about your model. And here are some
[2109.76s -> 2112.76s]  examples. I think it's useful to have that to have as a
[2112.76s -> 2116.92s]  norm in the community to say, you know, if I publish a model, I sort of, I report what it's been
[2116.92s -> 2122.12s]  trained on, how it's been trained on. And even to a degree, like what I think it could do or should
[2122.12s -> 2126.36s]  be used for, although yeah, if the structure of such a model car gets like too rigid and it's
[2126.36s -> 2129.88s]  like, no, we must use, we must ask these questions. You get into so many ridiculous
[2129.88s -> 2135.48s]  situations, like, you know, can this be reproduced? Well, I just, I like, I made
[2135.48s -> 2141.88s]  sklearn.linear regression, right? Yes, it can be. You get into situations where the questions
[2141.88s -> 2145.40s]  don't address what you would actually like to express in such a thing, then I think it becomes
[2145.40s -> 2148.44s]  counterproductive. But as a norm to have, hey, look, if you publish something, people should
[2148.44s -> 2154.12s]  be able to understand and potentially reproduce it. That standard we have had in papers for a
[2154.12s -> 2158.60s]  long time, and it's generally been a good standard to say, look, if you write, if you
[2158.60s -> 2162.60s]  publish something, I must be able to, from reading it, understand what's in there. And
[2162.60s -> 2166.28s]  to have that as a norm in the community. Yeah, I'm totally fine with that.
[2166.28s -> 2168.84s]  Do you think there's any relationship with the Chomsky syndrome that we were talking about
[2168.84s -> 2173.84s]  about earlier, which is this idea that we should have a very clear model of understanding of
[2173.84s -> 2177.64s]  how these things work in society and we should be able to extrapolate and control things
[2177.64s -> 2181.20s]  and that the fear is that basically this is just a complete black box and who knows
[2181.20s -> 2184.02s]  what's going to happen.
[2184.02s -> 2189.42s]  No, I'm good with the black box. It keeps things exciting and interesting. And as
[2189.42s -> 2194.02s]  I said, I don't believe this sort of runaway, it might become very influential
[2194.02s -> 2197.70s]  and so on. And certainly that's not very good. But then again, I don't know what
[2197.70s -> 2202.54s]  to do about it. Certainly, if some people sign a change.org moratorium petition, even
[2202.54s -> 2206.66s]  if it's reached, it's not going to help. What are you going to do? It doesn't matter being
[2206.66s -> 2207.66s]  worried about it.
[2207.66s -> 2210.34s]  We've got a few minutes left. I've got some questions from Jumbotron. Say hello, Jumbotron.
[2210.34s -> 2211.34s]  Hello, Jumbotron.
[2211.34s -> 2214.10s]  He's our forum administrator and he's a legend. Quick question. Do you think all
[2214.10s -> 2217.10s]  AI research should be open and accessible to the public?
[2217.10s -> 2221.10s]  No. It's totally legitimate that a business does internal research like all companies
[2221.10s -> 2225.74s]  do and that they then use that to make money. That's very cool with me. I've
[2225.74s -> 2229.34s]  Never said that shouldn't be the case, only that companies shouldn't do that, but at
[2229.34s -> 2235.46s]  the same time claim how open and all democratizing and beneficial to the common good they are.
[2235.46s -> 2236.46s]  Okay.
[2236.46s -> 2241.34s]  And you would accept that some research could lead to negative or unethical applications
[2241.34s -> 2243.78s]  and might need to be restricted or?
[2243.78s -> 2244.94s]  Yeah.
[2244.94s -> 2248.54s]  I totally accept that some research can and will probably lead to overall negative
[2248.54s -> 2253.42s]  effects for society or for certain individuals within society.
[2253.42s -> 2260.70s]  flying drones from any regime in the world, they certainly run some open-source components
[2260.70s -> 2264.62s]  as part of their guidance system, right? They maybe run a Linux kernel, like who knows.
[2264.62s -> 2269.14s]  I don't think the Linux kernel should not be fully open source and accessible to everyone,
[2269.14s -> 2274.14s]  and I don't want anyone to be able to be the eternal decider of who's good
[2274.14s -> 2280.78s]  enough to use the Linux kernel or not. I'd rather, I think the overall welfare
[2280.78s -> 2285.02s]  of society and humanity is much better served by accepting that some people are going to
[2285.02s -> 2290.34s]  do some bad things with it and then mitigating that in a different way than having like appointing
[2290.34s -> 2295.70s]  the, you know, the king of that model to decide, you know, who they deem pure hearted
[2295.70s -> 2297.02s]  enough to wield it.
[2297.02s -> 2300.38s]  Cool. What's next for ML news and your channel and are you making any more music
[2300.38s -> 2301.38s]  videos with AI?
[2301.38s -> 2305.42s]  Well, it's become, it's become, there are so many good music videos of AI now.
[2305.42s -> 2309.42s]  I'm always amazed by how talented people are and how quickly they pick up sort of
[2309.42s -> 2313.42s]  new stuff and do something with it. So that's very cool. I want, as I said, I've not made too
[2313.42s -> 2319.10s]  many videos because I've been extremely busy with Open Assistant. And I think we've also built up
[2319.10s -> 2322.38s]  sort of a momentum and a direction right now. And there are many competent people in our team.
[2322.38s -> 2327.02s]  So I'm also looking to make a couple of more videos, again, paper reviews, news, but also
[2327.02s -> 2331.82s]  a bunch of projects, which I always want to do, but then they take time, of course. But
[2331.82s -> 2335.50s]  I'm very excited about just some of the new stuff that's possible. And to try it out and
[2335.50s -> 2339.90s]  to show people a little bit of what one could do and how to have fun with these things.
[2339.90s -> 2343.50s]  And just in closing, have you got any shout outs for people in your life or in Discord
[2343.50s -> 2344.46s]  who have really helped you on the journey?
[2345.82s -> 2352.46s]  Too many, like way too many to name by name. I could go on like eternal lists in Open
[2352.46s -> 2356.22s]  Assistant specifically. Andreas Kepf has been extremely influential in that,
[2357.26s -> 2361.82s]  just organizing things but also coding things himself. But also all the other members,
[2361.82s -> 2365.34s]  as I said, if I start listing people, I'm going to miss someone and I don't want to do that
[2365.34s -> 2368.34s]  So I don't want to start listing people, but then I think, well, I really want to list the people.
[2368.34s -> 2370.34s]  It's an eternal conundrum.
[2370.34s -> 2377.34s]  So like to anyone who's ever had any part of helping me or given feedback or even been kind of addict,
[2377.34s -> 2378.34s]  I appreciate it.
[2378.34s -> 2385.34s]  And yeah, it's been amazing the amount of help and input you get from good willed people.
[2385.34s -> 2386.34s]  Communities are amazing.
[2386.34s -> 2390.34s]  So join Yannick's Discord, join our Discord, Open Assistant Discord.
[2390.34s -> 2392.34s]  Dr. Kilcher, thank you so much.
[2392.34s -> 2393.34s]  This has been an absolute pleasure, sir.
[2393.34s -> 2394.34s]  Thanks for having me.
========================================
Detected language 'en' with probability 1.000000

run time =152.98693656921387