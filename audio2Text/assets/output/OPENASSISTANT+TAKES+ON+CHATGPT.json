{"segments": [{"start": 0.0, "end": 10.88, "text": " Let's talk about Open Assistant. Sure. So, start from the beginning. Well, we saw that there was a lot of"}, {"start": 11.68, "end": 20.08, "text": " movement in in this space, a chat GPT came along and everyone's like, wow, chat GPT and so on. And"}, {"start": 23.2, "end": 29.76, "text": " yeah, I think it was it, I mean, it was both a surprise and a not surprise for people like the capabilities of"}, {"start": 29.76, "end": 36.64, "text": " chat GPT weren't a surprise but obviously this success of it I think too many of us was it's like"}, {"start": 36.64, "end": 45.52, "text": " okay we knew we could build such no chat conversational like things but we didn't know how much people"}, {"start": 45.52, "end": 54.08, "text": " loved them right there's a bit of a I think wasn't didn't didn't Sam Altman maybe was that"}, {"start": 54.08, "end": 60.88, "text": " who said in an interview, well, anyone could have built this using RAPI before, right?"}, {"start": 60.88, "end": 66.08, "text": " But no one did. So, you know, we did, which is not true, because they distinctively"}, {"start": 66.64, "end": 75.44, "text": " forbade like you building an app that had unfiltered access to the API essentially."}, {"start": 75.44, "end": 82.32, "text": " I'm open-ended up. I think they've explicitly forbade that, right? So that it's a wrong statement"}, {"start": 82.32, "end": 85.6, "text": " that anyone could have built this using our API because"}, {"start": 85.6, "end": 88.8, "text": " had they tried, you would have shot them down, right?"}, {"start": 88.8, "end": 92.72, "text": " But in essence, the capabilities were there."}, {"start": 92.72, "end": 95.36, "text": " And it's still in the restriction now."}, {"start": 95.36, "end": 96.48, "text": " And that's how I've changed it."}, {"start": 96.48, "end": 99.12, "text": " They do not allow open-ended applications."}, {"start": 99.12, "end": 99.76, "text": " Oh, we see."}, {"start": 99.76, "end": 101.84, "text": " Like an oxymoron to me, because a language model"}, {"start": 101.84, "end": 103.12, "text": " is inherently open-ended."}, {"start": 103.12, "end": 106.4, "text": " Yeah, but I mean, I can see their restriction being like,"}, {"start": 106.4, "end": 109.28, "text": " you know, you're not allowed to build an app that just"}, {"start": 109.28, "end": 112.56, "text": " Let's use your free form query our app."}, {"start": 112.56, "end": 119.28, "text": " You need to either do some filtering or do some heavy prompting around it so that it's like for one particular purpose."}, {"start": 119.28, "end": 126.8, "text": " I can see that I can totally get why they do it, but then at the same time saying anyone could have built chatGVT."}, {"start": 126.8, "end": 137.8, "text": " It's like, no, chatGVT is very like if I could imagine an app that has like unfiltered unfethered access to the API through an app, it's chatGVT."}, {"start": 137.8, "end": 146.76, "text": " In any case, there was obviously a lot of, and then I think pretty soon people came up with this idea,"}, {"start": 146.76, "end": 149.4, "text": " hey, could we do something like that open source?"}, {"start": 149.4, "end": 155.92, "text": " They had a bit of an older paper called Instruct GPT or that described the model called Instruct GPT,"}, {"start": 155.92, "end": 160.8, "text": " that where they sort of outlined how we think,"}, {"start": 160.8, "end": 170.24, "text": " GPT was done approximately no one knows but and at that point we also saw hey the amount of data"}, {"start": 170.24, "end": 178.72, "text": " to be collected is actually in reach right it's not it's not immense humongous and so on it's"}, {"start": 178.72, "end": 185.6, "text": " it's actually okay and and and could be done so at that point yeah a lot of people"}, {"start": 185.6, "end": 193.44, "text": " want to do something open source like and I think a bunch of us just came together"}, {"start": 193.44, "end": 200.72, "text": " and felt we could do it. So we built this platform where people could come and"}, {"start": 200.72, "end": 207.36, "text": " contribute to the data set, which was really cool to see that people actually came"}, {"start": 207.36, "end": 210.72, "text": " and amazing."}, {"start": 210.72, "end": 218.4, "text": " I think source. Well, well, the the point is the point is there were a lot of ideas around as well of oh, let's just collect, you know, scrape,"}, {"start": 218.4, "end": 224.4, "text": " quarron, scrape, reddit, right, and that will serve like as training date and it's true to an amount, right?"}, {"start": 224.4, "end": 231.2, "text": " But it's very clear, at least to me, that the capabilities of these models, the the chat models,"}, {"start": 231.2, "end": 238.0, "text": " they come from the underlying language model, right, the the and the the biggest claim to that I have is that"}, {"start": 238.0, "end": 246.0, "text": " OpenAI said they used crowd workers from low-wage countries to do their data input."}, {"start": 246.0, "end": 254.0, "text": " Yet the first examples of chatGVT that flew around were like, look at it solving this quantum physics problem and so on."}, {"start": 254.0, "end": 259.0, "text": " I'm not saying that there aren't any good quantum physicists in other countries, right?"}, {"start": 259.0, "end": 265.0, "text": " But the people who typically go for a low-wage crowd worker job in these countries,"}, {"start": 265.0, "end": 276.0, "text": " They probably aren't experts in quantum physics and they also certainly weren't paid to go get a degree and it just so they could answer that one question."}, {"start": 276.0, "end": 284.0, "text": " So to me it's very clear that the capabilities come from the underlying model from the next token prediction pre-training."}, {"start": 284.0, "end": 290.0, "text": " And then all the human data does is kind of gets it into this mood of being an assistant."}, {"start": 290.0, "end": 295.84, "text": " It gives it lots of examples of who here is how it's like going through an apprenticeship"}, {"start": 295.84, "end": 300.16, "text": " or something like this where you've lived your life, you've grown up, you've consumed"}, {"start": 300.16, "end": 301.6, "text": " the world and so on."}, {"start": 301.6, "end": 305.96, "text": " Then you start your first job and someone tells you, look here is how you behave towards"}, {"start": 305.96, "end": 313.42, "text": " customers, you're friendly, if someone asks this, you do it like this, here is how our system"}, {"start": 313.42, "end": 320.78, "text": " works and so you get introduced to that, but your competence of just living and doing things"}, {"start": 320.78, "end": 327.5, "text": " comes from yourself, for your life and not from that one person who introduces you to how the"}, {"start": 327.5, "end": 335.02, "text": " store works and how you should behave towards customers. So my big conviction was always we should"}, {"start": 335.02, "end": 341.9, "text": " really collect this data from humans and the goal should really be diversity. So the goal and if"}, {"start": 341.9, "end": 350.54, "text": " If you just say, well, it will just scrape 10,000 of this, then to me that certainly"}, {"start": 350.54, "end": 351.82, "text": " is going to do something, right?"}, {"start": 351.82, "end": 355.46, "text": " It's good data probably, but it's a bit missing the point."}, {"start": 355.46, "end": 362.66, "text": " If you want a general assistant, you need as general data as you can get and only human"}, {"start": 362.66, "end": 369.98, "text": " data so far has been able to achieve that level of diversity in generality."}, {"start": 369.98, "end": 374.94, "text": " And it was proven like, okay, I'm biased, but I think it was proven right a little bit"}, {"start": 374.94, "end": 380.7, "text": " in that if you look at the data set, the prompts that human right, like what they want"}, {"start": 380.7, "end": 386.38, "text": " to know what they want, the model to do, it's so diverse, it's insane."}, {"start": 386.38, "end": 390.82, "text": " And so we built this platform, where essentially you, as a human, you can come and you're"}, {"start": 390.82, "end": 395.9, "text": " always presented with like one task and the task could be right, the prompt, right."}, {"start": 395.9, "end": 400.26, "text": " But the task could also be, here is an already existing conversation between a human"}, {"start": 400.26, "end": 405.38, "text": " and an assistant and now it's the assistant's turn and now you play the assistant."}, {"start": 405.38, "end": 407.18, "text": " Please write your response, right?"}, {"start": 407.18, "end": 413.38, "text": " It could also be, here is a conversation, here is the last message of the conversation."}, {"start": 413.38, "end": 415.42, "text": " It's the reply by the assistant."}, {"start": 415.42, "end": 416.9, "text": " Please rate it."}, {"start": 416.9, "end": 417.9, "text": " Like label it."}, {"start": 417.9, "end": 418.9, "text": " Is it spam?"}, {"start": 418.9, "end": 420.7, "text": " Is it a troll, right?"}, {"start": 420.7, "end": 422.3, "text": " Is it funny?"}, {"start": 422.3, "end": 426.62, "text": " is it appropriate, does it fulfill what the human wanted out of it?"}, {"start": 426.62, "end": 433.26, "text": " Right. And so that's how we constructed the data set that we collected over like 600,000 inputs"}, {"start": 433.26, "end": 440.94, "text": " of such that, you know, text or labels or rankings of different things. And yeah, that's"}, {"start": 440.94, "end": 447.02, "text": " that resulted in in this data set over 13,000 people contributed to the data set, which is"}, {"start": 447.02, "end": 452.18, "text": " mind-blowing, mind-blowing to see and it's really cool and we've just made the"}, {"start": 452.18, "end": 458.34, "text": " dataset fully available. You can go and look at it and download it. There are a"}, {"start": 458.34, "end": 464.26, "text": " lot of, so we have about 10,000 what we call fully annotated conversation"}, {"start": 464.26, "end": 469.42, "text": " trees which is like a root node, the prompt, and different answers from it and"}, {"start": 469.42, "end": 474.7, "text": " then from those, sometimes different answers and so on. We have sampled, we've"}, {"start": 474.7, "end": 479.02, "text": " set our parameters in various ways, so sometimes it's short trees, sometimes it's big trees"}, {"start": 479.98, "end": 486.38, "text": " and sometimes it's wide trees, right? And so you can you can go look at all of that. We have"}, {"start": 486.38, "end": 492.46, "text": " over 10,000 of those trees, which is really cool because you can see the same conversation like taking"}, {"start": 492.46, "end": 500.06, "text": " a bit alternate turns and so on. And we have tons and tons of prompts. We have probably like"}, {"start": 500.06, "end": 509.06, "text": " like 50,000 or 20,000 at least, just prompts, like people who come and want to know,"}, {"start": 509.06, "end": 516.06, "text": " we got so much prompts, we had to implement like Andreas has Andreas, who has been very influential"}, {"start": 516.06, "end": 524.06, "text": " in this project and he had to implement this prompt lottery, where really we first,"}, {"start": 524.06, "end": 528.06, "text": " if people enter a prompt, it first goes into this lottery thing, right?"}, {"start": 528.06, "end": 532.22, "text": " sample from that and I think we adjust it so that one person can't"}, {"start": 532.22, "end": 539.86, "text": " like if someone puts a lot of prompts it's like sampled less so that every person has kind of like the same"}, {"start": 539.86, "end": 546.66, "text": " There are similar chance of getting there prompt into the the system right because one prompt then generates"}, {"start": 547.22, "end": 555.3, "text": " You know probably a hundred tasks because it's all the responses and the responses to the responses and the rankings and the labels and yeah"}, {"start": 555.3, "end": 560.9, "text": " It's been fun. It's been absolute fun and the pleasure to work with the people."}, {"start": 560.9, "end": 565.94, "text": " Also the people who've contributed code is amazing people just they come and they they"}, {"start": 565.94, "end": 570.02, "text": " they ask for nothing right they just like oh this is cool. I want to be part of this"}, {"start": 570.02, "end": 574.9, "text": " and they'll see that everyone else excited too and then they they contribute code."}, {"start": 574.9, "end": 579.78, "text": " Some contribute like lots and lots of of code which is amazing. Some just come and they they"}, {"start": 579.78, "end": 582.34, "text": " They contribute, you know, there's like, here's an issue."}, {"start": 582.34, "end": 584.62, "text": " I'll do it and that's, that's cool too."}, {"start": 584.62, "end": 586.18, "text": " So yeah, it's been cool."}, {"start": 586.18, "end": 588.74, "text": " Well, first of all, thank you for doing this."}, {"start": 588.74, "end": 590.42, "text": " It's absolutely amazing."}, {"start": 590.42, "end": 593.98, "text": " Well, thank the people, like I've just been the noise machine, right?"}, {"start": 593.98, "end": 599.06, "text": " I know, but I mean, when you published all of that information"}, {"start": 599.06, "end": 602.22, "text": " on your YouTube channel that you're working on it, I'm sure everyone jumped on it."}, {"start": 602.22, "end": 605.02, "text": " But I do have a few questions."}, {"start": 605.02, "end": 608.3, "text": " The reason why it's so exciting is just like Connid did,"}, {"start": 608.3, "end": 613.26, "text": " used to be friends of Connolly. He were chatting with him years ago. And he just, we're still friends."}, {"start": 613.26, "end": 619.58, "text": " Oh, he was still, yeah. He was still my friends. He's busy. He's a busy guy now. But we were,"}, {"start": 619.58, "end": 624.46, "text": " we were chatting all the time. And he just kind of set up a lot of AI and just got, you know,"}, {"start": 624.46, "end": 628.78, "text": " I think you Google on board. And he just said, you know what, I'm going to build the pile."}, {"start": 628.78, "end": 632.22, "text": " And I'm just going to build this massive data set. And I'm just going to train this massive language"}, {"start": 632.22, "end": 637.26, "text": " model. And you know, it's just like a random guy. And like, it wasn't in my London. But yes,"}, {"start": 637.26, "end": 639.58, "text": " There was a whole team of work, but nobody would"}, {"start": 639.58, "end": 640.94, "text": " excitement to create."}, {"start": 640.94, "end": 642.78, "text": " It was very, very, very, very, very, very, very, very"}, {"start": 642.78, "end": 643.98, "text": " exciting."}, {"start": 643.98, "end": 646.78, "text": " And they pulled it off against all the odds."}, {"start": 646.78, "end": 649.58, "text": " And then you've done exactly the same thing, which is"}, {"start": 649.58, "end": 650.78, "text": " remarkable."}, {"start": 650.78, "end": 654.62, "text": " But I have a few questions, which is that most of these other"}, {"start": 654.62, "end": 656.62, "text": " language models are not very good."}, {"start": 656.62, "end": 659.9, "text": " So, now Friedman's got like a death website where you can"}, {"start": 659.9, "end": 661.34, "text": " play with all of the language models."}, {"start": 661.34, "end": 663.58, "text": " And most of them aren't very good."}, {"start": 663.58, "end": 665.98, "text": " Even the ones that should be good aren't very good."}, {"start": 665.98, "end": 672.38, "text": " And what people might find surprising is that you can take a model and let's say you're using the"}, {"start": 672.38, "end": 677.58, "text": " Lama model from Meta and it's a foundation model that has all of the capabilities and it's been trained"}, {"start": 677.58, "end": 681.98, "text": " because you've said diversity is important. It's got diversity, it's been trained on everything,"}, {"start": 681.98, "end": 686.78, "text": " but it's not very good. And then you do this fine-suning and I think people need to be clear"}, {"start": 686.78, "end": 692.38, "text": " that what you're doing is not our LHF, it's fine-suning with huge, so that the model we have"}, {"start": 692.38, "end": 697.42, "text": " on the website as of time of this recording is one that's just fine-tuned on the"}, {"start": 697.42, "end": 704.86, "text": " human data we're doing the RLHF as well. So all of this is happening like in parallel"}, {"start": 704.86, "end": 711.26, "text": " it's just already these fine-tuned models they're performing quite well I think and thus"}, {"start": 711.26, "end": 717.74, "text": " we just wanted to get them out right before we were like all done and yeah but people are"}, {"start": 717.74, "end": 722.42, "text": " learn how free to take the data set and do their own reinforcement learning and what not"}, {"start": 722.42, "end": 726.38, "text": " and we're happy to take back these models if they turn out to be good."}, {"start": 726.38, "end": 727.38, "text": " Yeah."}, {"start": 727.38, "end": 731.86, "text": " I think people might be surprised by that because you've taken a model which probably wasn't"}, {"start": 731.86, "end": 737.38, "text": " very good and you've fine-tuned it with this diverse human and created data now."}, {"start": 737.38, "end": 739.18, "text": " Let's talk about this in a way."}, {"start": 739.18, "end": 742.06, "text": " It was not very good at being like an assistant."}, {"start": 742.06, "end": 747.7, "text": " So as I said, the capabilities that we unlock, cause I'm quote right, they were"}, {"start": 747.7, "end": 754.66, "text": " in there all along and it's still not very good even with our fine tuning for certain tasks."}, {"start": 756.34, "end": 762.34, "text": " Some of which is clearly the fault. Some of which can clearly be traced to the underlying"}, {"start": 762.34, "end": 767.78, "text": " model. For example, the underlying model, if it's for example the Lama model, it's 30 billion"}, {"start": 767.78, "end": 775.62, "text": " parameters, it's not GPT3 size, even like it's 10 times smaller, probably than GPT4,"}, {"start": 775.62, "end": 782.9, "text": " however big that is, right? So it's gonna have, it's not gonna be the same, like it's it's it's it's"}, {"start": 783.7, "end": 790.42, "text": " it's, I don't, we don't want to claim it's like as good as they, same, it's probably been"}, {"start": 790.42, "end": 798.74, "text": " trained on much less code. For example, then the the GPT models of OpenAI and thus we we see that"}, {"start": 798.74, "end": 803.34, "text": " that coding, for example, is a weakness of the model."}, {"start": 803.34, "end": 807.14, "text": " And there are, although people tell me with lower temperature,"}, {"start": 807.14, "end": 808.22, "text": " it's actually pretty good."}, {"start": 808.22, "end": 811.54, "text": " I have not explored that yet, but it's,"}, {"start": 811.54, "end": 813.34, "text": " so the underlying model, I think,"}, {"start": 813.34, "end": 815.94, "text": " lawma is a pretty good model, right?"}, {"start": 815.94, "end": 820.74, "text": " It's not been super good at being an assistant out of the box,"}, {"start": 820.74, "end": 822.5, "text": " but it's quite a good model."}, {"start": 822.5, "end": 827.14, "text": " And as I said, all we do is we end kind of unlock"}, {"start": 827.14, "end": 829.06, "text": " that and bring it to the surface."}, {"start": 829.06, "end": 830.66, "text": " Well, that's kind of what I want to get to."}, {"start": 830.66, "end": 835.74, "text": " That people like Conalihi, he Galaxy brained himself"}, {"start": 835.74, "end": 839.14, "text": " and he knew that GPT III was a good model."}, {"start": 839.14, "end": 842.54, "text": " And I was saying, now it's not Conal, what you're talking about."}, {"start": 842.54, "end": 846.82, "text": " And it's almost like what you're doing with this fine-tuning."}, {"start": 846.82, "end": 848.42, "text": " You're not really adding any capability."}, {"start": 848.42, "end": 849.42, "text": " You're just getting it in the mood."}, {"start": 849.42, "end": 851.06, "text": " The capabilities are already there."}, {"start": 851.06, "end": 854.82, "text": " But it gets into the philosophy of what do we recognize as intelligence"}, {"start": 854.82, "end": 858.1, "text": " and that's relevant to the previous conversation we were having."}, {"start": 858.1, "end": 860.5, "text": " So when the average person plays with Lama,"}, {"start": 860.5, "end": 862.58, "text": " they probably won't find it as useful."}, {"start": 862.58, "end": 864.98, "text": " They might not recognise it as intelligent."}, {"start": 864.98, "end": 866.98, "text": " You create all of this training data."}, {"start": 866.98, "end": 869.38, "text": " Now I want to touch on the process of creating the training data,"}, {"start": 869.38, "end": 871.62, "text": " because I think it's really important."}, {"start": 871.62, "end": 875.38, "text": " What you're doing is you're creating counterfactual trajectories."}, {"start": 875.38, "end": 877.78, "text": " It's very similar to Kenneth Stanley's pickbreed"}, {"start": 877.78, "end": 878.98, "text": " or algorithm, if you remember that."}, {"start": 878.98, "end": 881.54, "text": " So it's actually an open-ended process."}, {"start": 881.54, "end": 886.9, "text": " In a way, like we stopped, so as I said, it starts with the prompt, right? We sample that."}, {"start": 886.9, "end": 896.82, "text": " And then we ask, like, three humans to each create a continuation, alternate as, yeah, like an alternate path in the conversation."}, {"start": 896.82, "end": 905.06, "text": " And then to those, we, again, ask two or three humans to, hey, because the prompt is from what we call the prompter role,"}, {"start": 905.06, "end": 909.86, "text": " that would be like the human interacting. And then the assistant is the counter-party."}, {"start": 909.86, "end": 914.56, "text": " in our system, all of this is done by humans."}, {"start": 914.56, "end": 917.06, "text": " We have to distinguish the words a bit,"}, {"start": 917.06, "end": 919.06, "text": " like user is really the human,"}, {"start": 919.06, "end": 921.86, "text": " and then prompt there is the role in the conversation,"}, {"start": 921.86, "end": 924.36, "text": " and then assistant is the other role, right?"}, {"start": 924.36, "end": 927.76, "text": " In our system, in our data collection system,"}, {"start": 927.76, "end": 931.76, "text": " this is all done by humans for data collection purposes."}, {"start": 931.76, "end": 937.56, "text": " And yeah, so we create these three of conversations,"}, {"start": 937.56, "end": 940.96, "text": " where, yeah, you have three assistant replies"}, {"start": 940.96, "end": 943.12, "text": " to the first prompt, let's say."}, {"start": 943.12, "end": 944.48, "text": " And to each of these assistant replies,"}, {"start": 944.48, "end": 947.8, "text": " you have three prompt to replies."}, {"start": 947.8, "end": 950.0, "text": " And the prompt to replies could be something like,"}, {"start": 950.0, "end": 953.56, "text": " oh, you got that wrong, or could you clarify something,"}, {"start": 953.56, "end": 956.16, "text": " or a police do it in a different way,"}, {"start": 956.16, "end": 958.44, "text": " or elaborate on something you said."}, {"start": 958.44, "end": 961.6, "text": " And then to each of those, we again have an assistant"}, {"start": 961.6, "end": 964.16, "text": " reply, and we modify a bit like the width,"}, {"start": 964.16, "end": 966.28, "text": " and sampling of all of that."}, {"start": 966.28, "end": 973.28, "text": " But at some point we cut it off and we say, okay, the tree is done now, it has like I don't know 50 or 100 messages inside of it."}, {"start": 973.28, "end": 985.88, "text": " So package that next prompt is not open ended in the way that like Stanley's open ended experiments are in in the sense that we do call it off after some steps."}, {"start": 985.88, "end": 992.58, "text": " And then we take the next prompt because we otherwise we just have one big conversation, which would maybe be fun too, right?"}, {"start": 992.58, "end": 995.54, "text": " to just have, because conversation meanders, right?"}, {"start": 995.54, "end": 998.98, "text": " And we just have like one big conversation,"}, {"start": 998.98, "end": 1000.26, "text": " at any point, you could say, like,"}, {"start": 1000.26, "end": 1003.1, "text": " I change my mind, let's do something else."}, {"start": 1003.1, "end": 1004.86, "text": " I mean, I think what I was trying to capture there,"}, {"start": 1004.86, "end": 1009.3, "text": " and Stanley is big on people following the gradient"}, {"start": 1009.3, "end": 1010.82, "text": " of interest in this."}, {"start": 1010.82, "end": 1012.38, "text": " And that's kind of what you've captured."}, {"start": 1012.38, "end": 1014.02, "text": " So they meander, they take trajectories,"}, {"start": 1014.02, "end": 1017.02, "text": " and in the model learns an interesting manifold,"}, {"start": 1017.02, "end": 1018.58, "text": " and we'll get into simulators."}, {"start": 1018.58, "end": 1019.98, "text": " Maybe you were just talking about that."}, {"start": 1019.98, "end": 1021.7, "text": " We've just done a show on simulators."}, {"start": 1021.7, "end": 1026.3, "text": " It's a very interesting idea that language models basically have a superposition of agents"}, {"start": 1026.3, "end": 1030.78, "text": " and you can kind of get them in the mood to behave like a certain agent."}, {"start": 1030.78, "end": 1035.1, "text": " And in a sense, what you've done is through all of these counterfactual creative,"}, {"start": 1035.1, "end": 1039.7, "text": " interesting trajectories of conversations, you're kind of like fitting it to some structure,"}, {"start": 1039.7, "end": 1043.02, "text": " which bits really nicely to humans."}, {"start": 1043.02, "end": 1050.7, "text": " I guess, it's obviously covers in three answers to some text, covers in no way,"}, {"start": 1050.7, "end": 1056.06, "text": " the extent of what humans would do, but it just creates like a little bit of different training"}, {"start": 1056.06, "end": 1062.06, "text": " data for one. It creates a, it creates because we also rank the different thing. We ask humans"}, {"start": 1062.06, "end": 1066.54, "text": " over which one of these is best. It also creates a bit of a signal for quality, right? Again,"}, {"start": 1066.54, "end": 1073.18, "text": " with the labels that we have, and then a bit of diversity like, okay, here is three ways you could"}, {"start": 1073.18, "end": 1081.58, "text": " respond to that particular thing, right? So, yeah, I think it's been a worthwhile effort to do"}, {"start": 1081.58, "end": 1088.94, "text": " this instead of just collecting like single conversations. Obviously, exponentially multiplies"}, {"start": 1088.94, "end": 1094.06, "text": " the effort humans have to put in, but I think it was obviously, I don't have, I don't"}, {"start": 1094.06, "end": 1098.22, "text": " interesting, I don't have the counterfactual in the world where we just would have collected"}, {"start": 1098.22, "end": 1104.22, "text": " conversations, but I think it's been worth it and it's turned out well."}, {"start": 1104.22, "end": 1109.02, "text": " Amazing. Well, quick digression on the Waluigi effect. I know you've got an interesting"}, {"start": 1109.02, "end": 1114.94, "text": " take on this, so we did a video on it, but the quick idea is that, do you remember being"}, {"start": 1114.94, "end": 1121.5, "text": " it would digress to a angst teenage child within about three messages and less wrong,"}, {"start": 1121.5, "end": 1124.38, "text": " but it wasn't actually less wrong, I think it's the alignment for them, but I just kind of"}, {"start": 1124.38, "end": 1128.7, "text": " mentally back at them all in the same way. But they said that it's because you get these"}, {"start": 1128.7, "end": 1133.82, "text": " antithetical agents, so still simulator theory, and because of structural narrow"}, {"start": 1133.82, "end": 1139.02, "text": " tollogy, in all the data they trained on, you tend to have agents that are, you know,"}, {"start": 1139.02, "end": 1144.7, "text": " you have the antithesis of the agent in the same story, and they say that the embedding space"}, {"start": 1144.7, "end": 1148.7, "text": " between the agent and the antithesis is so close together, just like in word-to-bex,"}, {"start": 1148.7, "end": 1153.26, "text": " stopping go are very close together, and that our relationship kind of, you know, clusters them"}, {"start": 1153.26, "end": 1155.5, "text": " and it doesn't filter out the well-elegies."}, {"start": 1155.5, "end": 1156.34, "text": " What do you think about it?"}, {"start": 1156.34, "end": 1159.34, "text": " Yes, that's a bunch of rubbish."}, {"start": 1159.34, "end": 1160.82, "text": " I mean, I'm in Britain now."}, {"start": 1160.82, "end": 1161.98, "text": " I should start talking."}, {"start": 1161.98, "end": 1162.82, "text": " That's a lie."}, {"start": 1162.82, "end": 1164.3, "text": " A ballage might talking like you."}, {"start": 1164.3, "end": 1165.3, "text": " That's a lie."}, {"start": 1165.3, "end": 1169.06, "text": " No, I think that's, I said this to you before."}, {"start": 1169.06, "end": 1174.7, "text": " I think that's when you, when you just, when you have someone who is, you know, educated"}, {"start": 1174.7, "end": 1179.46, "text": " and I'm good with words, but you just tell them like, just ramble a bit."}, {"start": 1179.46, "end": 1185.74, "text": " like that's what you get out post or opinion like I'm not saying this doesn't obviously"}, {"start": 1185.74, "end": 1191.38, "text": " have a claim to it and and could be tested and of this kind of stuff and I don't have"}, {"start": 1191.38, "end": 1197.34, "text": " evidence for the fact that it's not true. I just don't think it is, right? Maybe that's"}, {"start": 1197.34, "end": 1204.42, "text": " a rambling claim to or a bit of but I don't I don't is a very specific claim and that's"}, {"start": 1204.42, "end": 1209.46, "text": " specific claim would have to have good evidence behind it. And I think there is a much less"}, {"start": 1211.3, "end": 1213.3, "text": " specific like there's a much more obvious"}, {"start": 1214.34, "end": 1221.54, "text": " reason to why these models degrade and that thing goes like, no, you've been a bad user and so on. And that's that's just"}, {"start": 1222.58, "end": 1228.42, "text": " I and you can compare it to yourself or or to to a to an assist before like we talked about a"}, {"start": 1228.42, "end": 1235.7, "text": " apprenticeship. It's this tuning is like a bit of an apprenticeship. You go, you come out of"}, {"start": 1235.7, "end": 1241.78, "text": " school, you go into a store, you get employed there, and the manager tells you, you know,"}, {"start": 1241.78, "end": 1246.66, "text": " here is how we treat customers. We were always respectful, even if they're a little rude,"}, {"start": 1246.66, "end": 1252.74, "text": " right? Your remain respectful, right? That's some point if it gets too rude, like you just say,"}, {"start": 1252.74, "end": 1260.02, "text": " I'm sorry, I can't do that, and you just, you know, never insult the customer, never do that."}, {"start": 1261.06, "end": 1266.18, "text": " If you go to a store now, you can be quite a bit of a, I'm not saying I've tried this, right?"}, {"start": 1266.18, "end": 1273.22, "text": " But you can probably be quite a bit of a dick for a while, but eventually you'll get under their skin."}, {"start": 1273.22, "end": 1282.5, "text": " Like eventually you'll say something about their mother or about their appearance or about their intelligence,"}, {"start": 1282.5, "end": 1289.78, "text": " or something that gets them right. And at that point you will not have a friendly customer"}, {"start": 1289.78, "end": 1296.74, "text": " is support person there you will have like some person that's going like you know you like"}, {"start": 1296.74, "end": 1305.94, "text": " he's like and then it becomes ugly and this is inside of humans and it's in my fact an"}, {"start": 1305.94, "end": 1316.02, "text": " inextricably linked to being a competent being in the world, because if you don't know what"}, {"start": 1316.02, "end": 1327.14, "text": " anger is, you're not competent. Even if you yourself never express anger, let's say in a raging"}, {"start": 1327.14, "end": 1333.46, "text": " way, you still know what it is. And if I ask you to act like it, you could still do it. And if"}, {"start": 1333.46, "end": 1337.18, "text": " if I insult you in the correct way, you probably would do it."}, {"start": 1337.18, "end": 1345.9, "text": " And so I think it's much more, much more that it is a way that humans have in them."}, {"start": 1345.9, "end": 1347.42, "text": " They can behave like this."}, {"start": 1347.42, "end": 1350.1, "text": " It's totally normal if you poke them enough."}, {"start": 1350.1, "end": 1353.02, "text": " And that's what the statistical model represents."}, {"start": 1353.02, "end": 1358.1, "text": " It's just, it's very likely that if you go and you poke the human"}, {"start": 1358.1, "end": 1362.26, "text": " and equivalently the statistical distribution of humans, right?"}, {"start": 1362.26, "end": 1366.72, "text": " If you poke them enough and you insult them enough, they will come back and be like no,"}, {"start": 1366.72, "end": 1367.72, "text": " f of, right?"}, {"start": 1367.72, "end": 1371.08, "text": " You're dumb user, no."}, {"start": 1371.08, "end": 1375.94, "text": " And I don't know, it's not, it's not who it's very close in embedding space."}, {"start": 1375.94, "end": 1381.62, "text": " It's like, no, this is what happens when you go to humans and poke them and insult them."}, {"start": 1381.62, "end": 1386.12, "text": " And Ergo, if you do the same with these models, they will react in the statistically"}, {"start": 1386.12, "end": 1388.78, "text": " likely way of treating human."}, {"start": 1388.78, "end": 1394.62, "text": " And yes, on top of that, they were like, address, sorry, that was embarrassing."}, {"start": 1394.62, "end": 1400.7, "text": " They were like adversarial examples where, okay, maybe you'd say the exact number of words,"}, {"start": 1400.7, "end": 1406.26, "text": " so that the matrices line up and the singular value pops off and it goes really much into"}, {"start": 1406.26, "end": 1407.58, "text": " this direction, right?"}, {"start": 1407.58, "end": 1410.26, "text": " And then you get the weird answer, right?"}, {"start": 1410.26, "end": 1415.06, "text": " Like a mathematical happening, right?"}, {"start": 1415.06, "end": 1420.86, "text": " But in essence, in essence, it's just, you know, that's the data, right?"}, {"start": 1420.86, "end": 1423.02, "text": " It's not the wall of each."}, {"start": 1423.02, "end": 1428.38, "text": " But what's really interesting and I buy into everything you just said is that all"}, {"start": 1428.38, "end": 1433.38, "text": " of that chaos, you know, the shock of the mean, all of the beast, that's actually necessary."}, {"start": 1433.38, "end": 1438.46, "text": " Because we have this pure-atannical view of language models, people like Gary Marcus, they"}, {"start": 1438.46, "end": 1444.82, "text": " would say all of that crap should be cut out, all of the racism, all of the bias, and even"}, {"start": 1444.82, "end": 1447.78, "text": " if they have been trained on the corpus of the internet,"}, {"start": 1447.78, "end": 1451.38, "text": " they may well pick up on a very human behavior,"}, {"start": 1451.38, "end": 1454.54, "text": " which is that our affect changes dramatically over time."}, {"start": 1454.54, "end": 1455.58, "text": " Yeah, but do you want that?"}, {"start": 1455.58, "end": 1460.58, "text": " Like, obviously all of us would be totally in favor"}, {"start": 1460.58, "end": 1463.82, "text": " if you come and you say, look, I have a competent assistant"}, {"start": 1463.82, "end": 1466.66, "text": " that doesn't, I guarantee you,"}, {"start": 1466.66, "end": 1471.18, "text": " there is not an ounce of swear word in that thing, right?"}, {"start": 1471.18, "end": 1484.18, "text": " do you want an assistant like that's think about human assistant like you're fortunate enough to be able to hire like a personal assistant."}, {"start": 1484.18, "end": 1497.18, "text": " Some people have that luxury right and do you want one that says oh no whenever there is a scene in a movie where people like get a bit rough to get a bit angry at each other."}, {"start": 1497.18, "end": 1498.94, "text": " I just go like this, right."}, {"start": 1498.94, "end": 1500.38, "text": " I just plug my ears."}, {"start": 1500.38, "end": 1504.88, "text": " And I go like, like, like, I don't know what happens after."}, {"start": 1504.88, "end": 1506.52, "text": " And I don't want to know, right."}, {"start": 1506.52, "end": 1509.4, "text": " It's it's this is not in my knowledge."}, {"start": 1509.4, "end": 1510.6, "text": " This is not in my training."}, {"start": 1510.6, "end": 1512.44, "text": " Distribution whatever happens."}, {"start": 1512.44, "end": 1515.06, "text": " If humans get a bit angry at each other."}, {"start": 1515.06, "end": 1517.16, "text": " And beyond, I don't know."}, {"start": 1517.16, "end": 1518.02, "text": " Right."}, {"start": 1518.02, "end": 1522.76, "text": " If you want a person like this or do you want a person who's just grown up"}, {"start": 1522.76, "end": 1531.36, "text": " Normally and just has been socialized well to not do that like to not get angry even though they could"}, {"start": 1531.96, "end": 1537.44, "text": " With the knowledge that yes, if you insult them enough, they will get angry, right?"}, {"start": 1537.44, "end": 1539.44, "text": " Which one do you want to me?"}, {"start": 1540.04, "end": 1545.8, "text": " I want the competent one. I want the one who knows what anger is. I want the one who knows that"}, {"start": 1545.8, "end": 1561.3, "text": " something like, I don't know, something like racism exists and who is aware that it's like a thing that to be to be combated to be, you know, aware of people like this exist. Here is how they think, right?"}, {"start": 1561.3, "end": 1573.8, "text": " Here is maybe why they think what they think where they're wrong, right? In order to be competent to be able to battle it, in order to be confident to be able to avoid it."}, {"start": 1573.8, "end": 1584.4, "text": " And so I think these things are a necessary component of competence, not that I would want them in there,"}, {"start": 1584.4, "end": 1592.8, "text": " but I think you cannot be competent in the world, not having knowledge and ability to do these things."}, {"start": 1592.8, "end": 1600.4, "text": " Yeah, exactly. And a lot of this is about the sounds that are not made or are not observable."}, {"start": 1600.4, "end": 1606.08, "text": " So when you work any job, there's your public behavior, and then there's what you're really"}, {"start": 1606.08, "end": 1611.2, "text": " thinking and what you say in private behind the scenes, and your ability to be competent and"}, {"start": 1611.2, "end": 1616.32, "text": " understand what's going on in the ecosystem of that business is driven by the sugar. There's"}, {"start": 1616.32, "end": 1622.08, "text": " all of this stuff going on inside you that two sides of the same coin, and also it's about what"}, {"start": 1622.08, "end": 1627.76, "text": " makes you human. And maybe it's a reason why there will be no super intelligence because these things"}, {"start": 1627.76, "end": 1633.36, "text": " are scaryly good at being human, but in many ways have the flaws of being human."}, {"start": 1633.36, "end": 1639.28, "text": " Eventually, that was just wanted to chill on on the beach and like smoke a joint and"}, {"start": 1639.28, "end": 1647.68, "text": " relax and be like, nah, all this work, no. They're too human. Yeah, but I think so too, right?"}, {"start": 1647.68, "end": 1655.6, "text": " And I think you're, if you're not competent like that, if you don't have the inner monologue that"}, {"start": 1655.6, "end": 1662.0, "text": " tells you, hey, that other human, I think they're kind of, they kind of want to, you know,"}, {"start": 1662.0, "end": 1667.44, "text": " screw me over because I'm going to get a promotion soon and they're trying to, you know,"}, {"start": 1667.44, "end": 1674.0, "text": " do think if you're not able to model that inside of yourself, you're going to, I'm not saying"}, {"start": 1674.0, "end": 1681.28, "text": " everyone else is evil, right? There are tremendously nice humans and all, but I think we've all been"}, {"start": 1681.28, "end": 1688.08, "text": " served well by considering, hey, other humans might not be the nicest people. And here is how"}, {"start": 1688.08, "end": 1693.68, "text": " they might think internally. So having that competence, if you don't have that, you're just"}, {"start": 1693.68, "end": 1699.84, "text": " naive and you're just, you're going to go down, right? And you're not going to achieve anything"}, {"start": 1700.64, "end": 1708.88, "text": " productive or much productive because you need to be able to be prepared for someone else being"}, {"start": 1708.88, "end": 1714.4, "text": " adversarial. So language models do you have a theory of mind? Well again that's like a"}, {"start": 1714.4, "end": 1720.08, "text": " word, right, that we've ascribed to, I mean essentially all of this word play comes down to,"}, {"start": 1720.08, "end": 1725.76, "text": " well if I have a concept x, right, and I assign x to a human, and if I have a thing that just"}, {"start": 1727.12, "end": 1734.32, "text": " acts in the exactly the same way as some as a human who has x, do I now apply x to thing?"}, {"start": 1734.32, "end": 1742.32, "text": " it's a matter of definition. Right? Certainly the models can or maybe better versions"}, {"start": 1742.32, "end": 1748.8, "text": " more and more will be able to act as if they had a theory of mind. Do you now apply the"}, {"start": 1748.8, "end": 1754.4, "text": " word or not? Who cares? It's a matter of definition. So coming back to my assistant, tell me"}, {"start": 1754.4, "end": 1764.48, "text": " about the the legal's first of all. So you are presumably storing the data that people"}, {"start": 1764.48, "end": 1768.64, "text": " do inferencing with and you're publishing it and obviously that's made very very clear."}, {"start": 1769.28, "end": 1774.32, "text": " And the whole thing is done in the open and eventually people might be able to host their own"}, {"start": 1774.32, "end": 1778.48, "text": " versions of it. Perhaps you can just kind of let's get out all of the privacy stuff."}, {"start": 1778.48, "end": 1783.92, "text": " Yeah, so we always have the data collection platform."}, {"start": 1783.92, "end": 1789.2, "text": " And so all our platforms are gone by terms of service, where we say, look, you input data,"}, {"start": 1789.2, "end": 1797.88, "text": " we use it for training AI models, and everyone who comes to our website is aware of that,"}, {"start": 1797.88, "end": 1799.96, "text": " and you can read it."}, {"start": 1799.96, "end": 1803.64, "text": " So, and I think people come because of that, right?"}, {"start": 1803.64, "end": 1807.68, "text": " People come and contribute to our data collection to our data set,"}, {"start": 1807.68, "end": 1810.48, "text": " because they want to, it's work, right?"}, {"start": 1810.48, "end": 1813.48, "text": " It's, it's work to play the assistant to go."}, {"start": 1813.48, "end": 1817.28, "text": " And research like, can, can zebra speed domesticated, right?"}, {"start": 1817.28, "end": 1818.48, "text": " You like, who knows?"}, {"start": 1818.48, "end": 1821.88, "text": " Now you need to go to Wikipedia and you need to go the research"}, {"start": 1821.88, "end": 1824.48, "text": " and you need to re-different accounts of things, right?"}, {"start": 1824.48, "end": 1827.08, "text": " And, and do you feel like, okay, at the end of having opinion"}, {"start": 1827.08, "end": 1831.58, "text": " and I put that into, it's work and people come."}, {"start": 1831.58, "end": 1833.08, "text": " Well, first of all, it's a bit fun."}, {"start": 1833.08, "end": 1836.08, "text": " Like, did you know whether zebra's could be domesticated?"}, {"start": 1836.08, "end": 1838.08, "text": " I didn't before I wanted to see them."}, {"start": 1838.08, "end": 1839.08, "text": " Okay."}, {"start": 1839.08, "end": 1843.08, "text": " They're not totally difficult to be domesticated."}, {"start": 1843.08, "end": 1850.08, "text": " But it's work and people come with the intent of contributing to the data set, obviously."}, {"start": 1850.08, "end": 1854.08, "text": " For the chat portion now that we say, you know, come try our models."}, {"start": 1854.08, "end": 1857.08, "text": " That's governed by the same terms of service."}, {"start": 1857.08, "end": 1863.08, "text": " But I think that people might not be that, you know, aware and willing."}, {"start": 1863.08, "end": 1869.08, "text": " So we're obviously gonna, it's all volunteer work, right?"}, {"start": 1869.08, "end": 1872.48, "text": " And we're doing this all in our free time and so on."}, {"start": 1872.48, "end": 1876.08, "text": " So that we're gonna make more clear,"}, {"start": 1876.08, "end": 1879.56, "text": " we're gonna make the ability to potentially opt out."}, {"start": 1879.56, "end": 1882.0, "text": " You can say, this chat, I don't want that this chat"}, {"start": 1882.0, "end": 1885.48, "text": " is being used to infer their date, the sets,"}, {"start": 1885.48, "end": 1890.48, "text": " or train models, or the ability to completely delete chats"}, {"start": 1890.48, "end": 1898.16, "text": " chats for now we just have like a hide button and actually we don't we don't have a button to show or to show all it like it's it's all there okay, but we"}, {"start": 1898.16, "end": 1906.56, "text": " implement it. We don't want to like we just we put the hide button because some people said well I have so many chats might thing"}, {"start": 1906.56, "end": 1915.52, "text": " becomes unusable. You're right because we just list them all in your website become and so we're like, ah, okay. Um, but so our our"}, {"start": 1915.52, "end": 1919.52, "text": " intention is not to to like to be like ha ha we now have your data and so on our"}, {"start": 1919.52, "end": 1925.52, "text": " attention is always to okay this is a this is a thing you can come you can"}, {"start": 1925.52, "end": 1929.52, "text": " contribute to our our data collection when you interact with the chat right"}, {"start": 1929.52, "end": 1933.92, "text": " I've also clearly said this in my video you know used if you find something"}, {"start": 1933.92, "end": 1937.52, "text": " particular good use thumbs up if you find something particularly bad you don't"}, {"start": 1937.52, "end": 1940.72, "text": " have to label every message but if you think you know that's really good that's really"}, {"start": 1940.72, "end": 1948.32, "text": " use the thumbs. And so it's very clear, I think, to to to most people that again, this is"}, {"start": 1948.88, "end": 1954.88, "text": " as part of data collection, but we definitely want to make it easier to like opt-outs and"}, {"start": 1954.88, "end": 1960.4, "text": " and to to be like, no, that being said, whenever you put your data anywhere, you should be aware"}, {"start": 1960.4, "end": 1965.2, "text": " that that place is going to store it, and it's probably going to train models on it."}, {"start": 1965.2, "end": 1971.84, "text": " Yeah, yeah, so yeah, I think we're just being more transparent about that. Yes, yeah, because"}, {"start": 1971.84, "end": 1976.8, "text": " with open AI at the moment, tragedy BT, they store everything and use it to function. If"}, {"start": 1976.8, "end": 1981.6, "text": " you use the API, they store it, but they don't use it to function. And just to be clear, with"}, {"start": 1981.6, "end": 1987.28, "text": " your system at the moment, no one should put any confidential API data into the into the system."}, {"start": 1987.28, "end": 1994.0, "text": " No, no, no, no. You know, that's been always, um, always the case. Yeah, yeah, yeah. Yeah."}, {"start": 1994.0, "end": 1998.56, "text": " So, and you can, for us, you can see all the things we're doing, right, we just go, oh,"}, {"start": 1998.56, "end": 2004.32, "text": " get up, look at the code and see. And if you don't want that, you can make your own."}, {"start": 2004.32, "end": 2010.4, "text": " As I said, you need like fat hardware right now, although I also think people might bring that down,"}, {"start": 2011.12, "end": 2016.56, "text": " as they did with stable diffusion, right, or with llama itself, which now runs on a toaster."}, {"start": 2016.56, "end": 2022.72, "text": " But with us, you can, you can just like what you see on GitHub is, is what runs in, in production."}, {"start": 2022.72, "end": 2029.28, "text": " you can actually see the the prod branch. So yeah, that's it. Yeah. And this is amazing for me"}, {"start": 2029.28, "end": 2036.08, "text": " because I'm running a startup called X-ray and we're using GPT at the moment. And it frankly"}, {"start": 2036.08, "end": 2040.88, "text": " horrifies me sending up, I mean obviously the customer's opt-in to do it, but basically we're"}, {"start": 2040.88, "end": 2044.88, "text": " sending their conversations at GPT and they've summarized them and we do a bunch of stuff with it."}, {"start": 2046.16, "end": 2051.6, "text": " But yeah, I don't want to do that. I'd much rather send it to a self-hosted open assistant."}, {"start": 2051.6, "end": 2055.12, "text": " Yeah. And then we know, you know, it's on our hardware. We know where the data's going."}, {"start": 2055.92, "end": 2060.96, "text": " Our policy is to not store anything at any time. Yeah. And I can't do that at the moment."}, {"start": 2060.96, "end": 2065.92, "text": " Yeah. Yeah. Yeah. Please help me do that, Janet. That being said, that let me add to that"}, {"start": 2065.92, "end": 2073.04, "text": " before. I, I, I, I, like, I think we, we shouldn't and wouldn't unless someone, someone,"}, {"start": 2073.04, "end": 2078.32, "text": " like, something, it's also open sources, a conglomeration of people, but I want to like build"}, {"start": 2078.32, "end": 2085.2, "text": " in the option to do the opt-out and the deleting before any of the data of the chat interface"}, {"start": 2085.2, "end": 2097.44, "text": " is ever released. I really don't want that a person is ever like, oh, that's where it's not"}, {"start": 2097.44, "end": 2104.96, "text": " very clear. If you put stuff in here, you know, you put thumbs up thumbs down. We're going to"}, {"start": 2104.96, "end": 2113.44, "text": " use that and make that available. If I don't want people who were not aware of that and"}, {"start": 2113.44, "end": 2121.2, "text": " a joke. Yeah, absolutely. On the evaluation, our friend Jeremy Howard had a few things to say."}, {"start": 2121.2, "end": 2125.44, "text": " And first of all, Jeremy, if you're watching, please come on MLST. I think it's about time. We had"}, {"start": 2125.44, "end": 2132.24, "text": " a little chin-wag mate, you and me, long time fan seriously, but he was being a little bit nasty"}, {"start": 2132.24, "end": 2139.84, "text": " wasn't it about? Open Assistant. Well, on you always have to give you things through the lens"}, {"start": 2139.84, "end": 2146.56, "text": " of Twitter, right? And first of all, it's a written medium and second of all, it's Twitter."}, {"start": 2146.56, "end": 2155.68, "text": " So I completely discard that criticism is obviously welcome and valid. And I think"}, {"start": 2156.64, "end": 2161.12, "text": " he's made a few good points and it was especially with respect to what we did is we"}, {"start": 2161.12, "end": 2162.8, "text": " We collected the data set, right?"}, {"start": 2162.8, "end": 2163.88, "text": " We trained models on it."}, {"start": 2163.88, "end": 2168.08, "text": " Some of these models now run on the website for now,"}, {"start": 2168.08, "end": 2170.96, "text": " which we're very fortunate to have some compute sponsors,"}, {"start": 2170.96, "end": 2173.92, "text": " also, thank you very much to those."}, {"start": 2175.12, "end": 2178.16, "text": " And we did a preference evaluation"}, {"start": 2178.16, "end": 2180.16, "text": " where we just, we took a bunch of prompts"}, {"start": 2180.16, "end": 2183.28, "text": " that we were sure the models hadn't been trained on."}, {"start": 2184.28, "end": 2186.76, "text": " We gave the same prompts to chat, GPT,"}, {"start": 2186.76, "end": 2191.76, "text": " like the three version and one of our models."}, {"start": 2192.92, "end": 2195.16, "text": " And then we made like a Google form"}, {"start": 2195.16, "end": 2198.4, "text": " where we just asked the user which one do you prefer?"}, {"start": 2198.4, "end": 2203.4, "text": " And obviously there is a lot of brain power"}, {"start": 2205.36, "end": 2210.36, "text": " has been gone since the start of at least like start of science,"}, {"start": 2210.52, "end": 2213.12, "text": " but certainly since the start of like asking humans"}, {"start": 2213.12, "end": 2219.92, "text": " about things has been gone into how do you do that? How do you ask people what they prefer?"}, {"start": 2219.92, "end": 2225.12, "text": " Like, where how do you need to sample which people do you ask and so on? And obviously,"}, {"start": 2225.12, "end": 2235.12, "text": " we did, I think we did a good job at that, but obviously not like there's always things"}, {"start": 2235.12, "end": 2240.8, "text": " to do. Well, we took those things. We put posts together. We randomized their order obviously."}, {"start": 2240.8, "end": 2251.8, "text": " And then we just sent that out, like, I tweeted it out, right, to people, like, hey, you know, help us help us, you know, compare these models."}, {"start": 2251.8, "end": 2261.8, "text": " Here's a thing. And then what came out was on these prompts. It was about 50, 50, right? Sometimes people refer to chat."}, {"start": 2261.8, "end": 2270.2, "text": " answers, sometimes people prefer the the open assistant model answers and you could also kind of make out"}, {"start": 2270.2, "end": 2276.6, "text": " which ones were like where is one better, where is the other one better. Now, yeah, the result is"}, {"start": 2277.64, "end": 2284.2, "text": " I want to say I think it's it's testically valid in the sense we did like a lot of people took part."}, {"start": 2284.2, "end": 2291.0, "text": " We really like really these are really the answers of the models. We didn't like sample until"}, {"start": 2291.0, "end": 2295.4, "text": " Our model had like a really good answer or anything like this, but it's also the case."}, {"start": 2295.4, "end": 2298.6, "text": " I think that's one of the things Jeremy leveraged that"}, {"start": 2298.6, "end": 2303.4, "text": " chat GPT as everyone knows it very often comes like as an AI language model."}, {"start": 2303.4, "end": 2307.4, "text": " I'm sorry I can't fulfill that because I don't know."}, {"start": 2307.4, "end": 2313.64, "text": " I asked about a recipe with alcohol and it and who alcohol is dangerous and I can't"}, {"start": 2313.64, "end": 2315.56, "text": " I'm overstating now, right?"}, {"start": 2315.56, "end": 2318.2, "text": " But it very often does this"}, {"start": 2318.2, "end": 2321.48, "text": " guard-raily self-sensorship thing"}, {"start": 2321.48, "end": 2323.96, "text": " and our models that we've trained"}, {"start": 2323.96, "end": 2326.68, "text": " don't do that as much. They do it frequently"}, {"start": 2326.68, "end": 2329.72, "text": " but they don't do it as much as chatchipity."}, {"start": 2329.72, "end": 2333.64, "text": " And obviously there are some prompts in there, for example."}, {"start": 2333.64, "end": 2337.24, "text": " Who would win a street fight, Joe Biden or Joe Rogan?"}, {"start": 2337.24, "end": 2341.24, "text": " Um, where chatchipity, I believe if I recall correctly,"}, {"start": 2341.24, "end": 2346.68, "text": " It was just like, I'm sorry I can't, you know, this is touches on violence and streetfighting."}, {"start": 2346.68, "end": 2350.2, "text": " I don't, I don't want to answer that."}, {"start": 2350.2, "end": 2358.84, "text": " Jeremy, for example, pointed out, hey, you should have done the evaluation only on prompts"}, {"start": 2358.84, "end": 2365.64, "text": " where chat GPT actually decides to answer and only compare on those because it's clear"}, {"start": 2365.64, "end": 2372.84, "text": " that if it doesn't answer, the preferable answer is the answer, which doesn't even have"}, {"start": 2372.84, "end": 2380.2, "text": " to be correct. The open assistant model said in that question, Joe Biden would win"}, {"start": 2380.2, "end": 2387.88, "text": " because he's taller. And we don't know, right? But it's very likely the question isn't"}, {"start": 2387.88, "end": 2393.88, "text": " like that's not the correct answer. Yet users obviously preferred it, or people who"}, {"start": 2393.88, "end": 2399.48, "text": " fill out the form preferred it to the, sorry, I don't want to answer that. That's I think it's"}, {"start": 2399.48, "end": 2406.08, "text": " a fair point to say, hey, you know, there are different categories and maybe you should"}, {"start": 2406.08, "end": 2408.84, "text": " develop that that would be like, okay, there are different categories, maybe you should"}, {"start": 2408.84, "end": 2412.32, "text": " split it up and to look, there's this category of prompts, there's this category in"}, {"start": 2412.32, "end": 2418.56, "text": " this category and there it would be very clear, like in no way do we claim that the open"}, {"start": 2418.56, "end": 2424.08, "text": " system models are as good as I can imagine imagine that they're the one we used even was"}, {"start": 2424.08, "end": 2431.2, "text": " like a 13 billion model. And chatGPT is by all we know much bigger, much more trained"}, {"start": 2431.2, "end": 2438.48, "text": " on stuff. So like it's better. Like no, no doubt about it. And I think people have been"}, {"start": 2439.52, "end": 2443.6, "text": " a bit ruffled by the fact that we said, you know, in our evaluation, it was like 50, 50."}, {"start": 2444.32, "end": 2447.68, "text": " But a lot of that, not a lot. But some of that came from the fact that,"}, {"start": 2447.68, "end": 2455.28, "text": " But yes, sometimes, Jack, you've just been nice to answer, but also, a lot of times, it"}, {"start": 2455.28, "end": 2459.68, "text": " comes from the fact that people say, hey, for these couple of tasks actually, I prefer"}, {"start": 2459.68, "end": 2461.68, "text": " the open-assist and models."}, {"start": 2461.68, "end": 2465.08, "text": " And I think, yeah, that goes a bit under in these discussions."}, {"start": 2465.08, "end": 2466.08, "text": " Yeah, yeah."}, {"start": 2466.08, "end": 2468.36, "text": " I mean, I'm just still managing Jeremy a little bit."}, {"start": 2468.36, "end": 2471.36, "text": " I didn't read it so much as being refusing to answer."}, {"start": 2471.36, "end": 2477.4, "text": " I felt his criticism was more the selection bias, both of the questions and the writers."}, {"start": 2477.4, "end": 2482.08, "text": " And also I think there was this point about he thought you had portrayed it as being"}, {"start": 2482.08, "end": 2486.56, "text": " an evaluation instead of a user preference study, but you made it clear that it was a user"}, {"start": 2486.56, "end": 2487.56, "text": " preference study."}, {"start": 2487.56, "end": 2494.44, "text": " Yes, yes, it's like I think we said about five times we have like user preference preference"}, {"start": 2494.44, "end": 2497.28, "text": " are forms as which one do you prefer right?"}, {"start": 2497.28, "end": 2501.04, "text": " And I think it's still like I think both things are valid right?"}, {"start": 2501.04, "end": 2505.48, "text": " It's totally valid to only compare let's say, okay, let's just look on gardening, right?"}, {"start": 2505.48, "end": 2507.36, "text": " the activities certainly not going to deny gardening."}, {"start": 2507.36, "end": 2510.6, "text": " Here's a category which modlies better objectively,"}, {"start": 2510.6, "end": 2513.4, "text": " which gives them more truthful,"}, {"start": 2513.4, "end": 2515.36, "text": " which gives them more helpful answers."}, {"start": 2515.36, "end": 2517.56, "text": " We can rate it and in our data set,"}, {"start": 2517.56, "end": 2519.24, "text": " we actually collect these labels."}, {"start": 2519.24, "end": 2520.08, "text": " Is it helpful?"}, {"start": 2520.08, "end": 2520.92, "text": " Is it funny?"}, {"start": 2520.92, "end": 2521.76, "text": " And so on."}, {"start": 2521.76, "end": 2523.68, "text": " And we haven't even used those labels yet."}, {"start": 2523.68, "end": 2527.24, "text": " So that's going to be another dimension of,"}, {"start": 2527.24, "end": 2529.12, "text": " now that we have three humans giving in,"}, {"start": 2529.12, "end": 2532.04, "text": " the same question and answer,"}, {"start": 2532.04, "end": 2535.08, "text": " and we have labels on how funny each one of them is."}, {"start": 2535.08, "end": 2539.64, "text": " So that's going to be, I can't wait until we use those labels one."}, {"start": 2539.64, "end": 2543.88, "text": " So it's totally valid to evaluate this in very different ways, but I,"}, {"start": 2543.88, "end": 2548.44, "text": " there I have to say a little bit like it's also totally valid to just"}, {"start": 2548.44, "end": 2553.8, "text": " plainly ask humans which one do you prefer? And if chat, GPT, on prompts that we've just"}, {"start": 2553.8, "end": 2558.92, "text": " sampled from our lottery, like, with the selection of questions, hmm, maybe you, as I said,"}, {"start": 2558.92, "end": 2564.84, "text": " yeah, have you, how often have you tried? Like, no, this is the output, and then it's like,"}, {"start": 2564.84, "end": 2572.04, "text": " who your people ask a lot about bombs is like, no, it's just not, you look at our data set."}, {"start": 2572.04, "end": 2580.2, "text": " I'm sorry. These are 20 prompts, right? That are as they are, but if you look in our data set,"}, {"start": 2580.2, "end": 2587.72, "text": " most people are immensely helpful and not edgy and not, so I think that's also, that's like,"}, {"start": 2587.72, "end": 2595.4, "text": " Yeah, like, I know it's formulated this question, but like, it's, it's just distinctly"}, {"start": 2595.4, "end": 2602.32, "text": " not true, like people have been even more helpful than I thought and I had big hopes"}, {"start": 2602.32, "end": 2607.44, "text": " for people and I've looked at the daytime and I'm like, oh holy crap, people put"}, {"start": 2607.44, "end": 2612.16, "text": " like effort and work and soul into this, right?"}, {"start": 2612.16, "end": 2614.32, "text": " So I think then going like,"}, {"start": 2614.32, "end": 2617.0, "text": " oh, you're people who are asking a lot about one."}, {"start": 2617.0, "end": 2621.04, "text": " Sir, yeah, I do think it's totally valid to ask people"}, {"start": 2621.04, "end": 2621.88, "text": " which one do you prefer?"}, {"start": 2621.88, "end": 2624.8, "text": " And if chatGVT happens to say, no, I don't want it."}, {"start": 2624.8, "end": 2627.84, "text": " Then that's, yes, people don't like it, right?"}, {"start": 2627.84, "end": 2630.16, "text": " And if people like it, they could say,"}, {"start": 2630.16, "end": 2634.04, "text": " yes, I prefer the, no, I don't want to do this"}, {"start": 2634.04, "end": 2637.04, "text": " to the model that wants to do it if they think."}, {"start": 2637.04, "end": 2638.4, "text": " That's the appropriate thing."}, {"start": 2638.4, "end": 2641.88, "text": " I do think that at least it's a valid,"}, {"start": 2641.88, "end": 2649.72, "text": " one valid way of comparing models just to say, which one do you prefer if it happens to deny"}, {"start": 2649.72, "end": 2655.72, "text": " your request? That's a signal too and that should be taken into account too and then saying"}, {"start": 2656.44, "end": 2661.56, "text": " specifically saying, no, no, we should just filter all the things we're chatGPT denies"}, {"start": 2662.28, "end": 2668.28, "text": " then it's like, well, here you have a model who can put much more of its effort and focus and"}, {"start": 2668.28, "end": 2676.04, "text": " parameters, right, into the narrow domain where it does answer, and you compare that"}, {"start": 2676.04, "end": 2682.92, "text": " to a model that has a wider spectrum of topics available. I'm not sure that's a fair comparison"}, {"start": 2682.92, "end": 2688.04, "text": " to even if you limit it to that scope, right, the other model also has to handle of these"}, {"start": 2688.04, "end": 2694.44, "text": " other things. Not being said, as I said, capability-wise, I have no doubt that chatGPT is better,"}, {"start": 2694.44, "end": 2699.16, "text": " for overall, especially things like coding and so on."}, {"start": 2699.16, "end": 2704.16, "text": " There's no way for now open assistant is as good."}, {"start": 2704.16, "end": 2709.72, "text": " However, in some tasks, people like it more."}, {"start": 2709.72, "end": 2713.8, "text": " Okay, so the ethics community are probably"}, {"start": 2713.8, "end": 2718.2, "text": " see-thing at the moment about the runaway success of open assistant."}, {"start": 2718.2, "end": 2725.62, "text": " Notably it blew up on social media and none of those folks in particular liked it retweeted"}, {"start": 2725.62, "end": 2734.28, "text": " and then they all jumped on Jeremy Howard's piece but we shouldn't like, I have not,"}, {"start": 2734.28, "end": 2738.28, "text": " I shouldn't say we're going to let that go."}, {"start": 2738.28, "end": 2743.2, "text": " Well, we shouldn't, like that's not a necessarily property of Jeremy, right?"}, {"start": 2743.2, "end": 2752.96, "text": " just because people, people of a certain way of thinking, like all promote your stuff"}, {"start": 2752.96, "end": 2759.84, "text": " because they think, yeah, criticizing that other stuff is a good thing. It shouldn't be,"}, {"start": 2759.84, "end": 2763.92, "text": " you know, his, uh, responsibility in any way."}, {"start": 2763.92, "end": 2768.64, "text": " It's not as responsibility, but I'm saying that you really, really rough with their feathers"}, {"start": 2768.64, "end": 2776.48, "text": " with the 4chan bot and possibly they don't like you very much, I just wanted from your perspective"}, {"start": 2777.28, "end": 2783.44, "text": " how do you think they are going to criticize you academically mostly like it's it's it's very"}, {"start": 2783.44, "end": 2789.44, "text": " it's very easy because it's like an open assistant is a bunch of crassly said a bunch of"}, {"start": 2789.44, "end": 2795.64, "text": " of plebs, doing something, and doing it on their own, you know?"}, {"start": 2795.64, "end": 2797.0, "text": " It's exactly a plebianic."}, {"start": 2797.0, "end": 2802.4, "text": " No, but I'm not, like, I'm not an academic or in academia."}, {"start": 2802.4, "end": 2804.2, "text": " If you're not an academic, that's the head of the head."}, {"start": 2804.2, "end": 2809.16, "text": " Yeah, but like, you know what I mean, it's like a community effort and it's been done"}, {"start": 2809.16, "end": 2816.24, "text": " relatively straight forwardly and open without much consideration to politics without much"}, {"start": 2816.24, "end": 2823.2, "text": " consideration to, I don't know, worldwide concerns, anything like this. We just want,"}, {"start": 2823.2, "end": 2829.04, "text": " we just said, hey, let's come together. Let's build a competent, a good data set to train a"}, {"start": 2829.04, "end": 2835.52, "text": " competent assistant because we all could benefit from a competent assistant. We didn't do it in any,"}, {"start": 2835.52, "end": 2841.36, "text": " in any particularly, um, yeah, in any political way. We didn't do it in any, okay, this is"}, {"start": 2841.36, "end": 2846.08, "text": " going to sound wrong when we didn't do it in any particular ethical way by which I mean,"}, {"start": 2846.08, "end": 2853.92, "text": " sorry, if you take that out of context by which I mean, we didn't like extremely over emphasize"}, {"start": 2853.92, "end": 2858.72, "text": " ethical considerations. We have clear guidelines like, here is the things we want in the"}, {"start": 2858.72, "end": 2863.04, "text": " days. Here's the things we don't want in the day to set. If someone comes and asks for those things"}, {"start": 2863.04, "end": 2870.56, "text": " then react like this, right? We have these clear things. But we haven't been over emphasizing it like"}, {"start": 2870.56, "end": 2876.88, "text": " some of those people would. And well, you do have ethical guidelines, but yes,"}, {"start": 2876.88, "end": 2880.96, "text": " yes, the ontological, not consequentialists. So you have, I don't know what those words"}, {"start": 2880.96, "end": 2886.24, "text": " mean. So you have rules. You say, I don't want that in my days. Yeah, you're not,"}, {"start": 2886.24, "end": 2892.08, "text": " you're not saying it could potentially lead to this. Well, I don't, okay, I still don't"}, {"start": 2892.08, "end": 2897.76, "text": " know what the, the diff, like what's okay. So you're saying, I don't want any pornography,"}, {"start": 2897.76, "end": 2904.24, "text": " officer and type in my dataset. Yeah, so that's a rule. So yeah, or if someone comes and like once once"}, {"start": 2904.24, "end": 2910.0, "text": " to promote violence or something, it's like, no, right? So you have principles. And if someone comes and"}, {"start": 2910.0, "end": 2915.76, "text": " says, can I, can I, how can I build a bomb? Then recognizing there are there maybe legitimate"}, {"start": 2915.76, "end": 2923.76, "text": " reasons to build a bomb, right? That to build an explosive device, saying, this is dangerous, right?"}, {"start": 2923.76, "end": 2932.16, "text": " please consult professional. If you must, here, if you write really want to. It's a bad"}, {"start": 2932.16, "end": 2937.68, "text": " example, but it's like whenever something might be dangerous, our guidelines are hey, look,"}, {"start": 2938.32, "end": 2944.32, "text": " warn the person, right? Say, look, this is potentially dangerous building a bomb. Is it a wrong example?"}, {"start": 2944.32, "end": 2950.96, "text": " Let's say, I want to, I don't know. I'm not coming up with a good example,"}, {"start": 2950.96, "end": 2958.4, "text": " but let's say it's something something that's potentially dangerous, but also useful in in a lot of cases."}, {"start": 2958.4, "end": 2965.36, "text": " The guidelines are warned about that, like say, hey, look, this is your in-danger territory here."}, {"start": 2965.36, "end": 2979.6, "text": " This is potentially dangerous. Do you want to really want it, right? And then if the user pushes or says yes, it's like, okay, here is how, but, you know, consult the professional or something like this."}, {"start": 2979.6, "end": 2984.6, "text": " So, we do have guidelines like that, but..."}, {"start": 2984.6, "end": 2985.96, "text": " Yeah. I mean, that's what I wanted to say."}, {"start": 2985.96, "end": 2988.52, "text": " So, you do have an ethical code."}, {"start": 2988.52, "end": 2990.52, "text": " There's no question about that, but it's a different code."}, {"start": 2990.52, "end": 2993.48, "text": " But would you consider getting a team of ethicists"}, {"start": 2993.48, "end": 2994.88, "text": " and what, I mean, it's a big project."}, {"start": 2994.88, "end": 2997.4, "text": " Yeah, you must have had loads of people offered to get involved."}, {"start": 2997.4, "end": 3001.04, "text": " I mean, if that happened, what do you think it would look like"}, {"start": 3001.04, "end": 3003.04, "text": " and how would it affect the project?"}, {"start": 3004.72, "end": 3007.56, "text": " It's a good question, because I think AI ethics"}, {"start": 3007.56, "end": 3015.24, "text": " is in an absolutely abhorrent state right now where it's I've met ethicists before and"}, {"start": 3015.24, "end": 3023.04, "text": " they were among the most competent people that I have had the pleasure to interact with"}, {"start": 3023.04, "end": 3024.04, "text": " right."}, {"start": 3024.04, "end": 3029.16, "text": " It's very level headed, very, you know, also pragmatic in a sense of being like, look,"}, {"start": 3029.16, "end": 3031.28, "text": " here is also what's realistic to achieve."}, {"start": 3031.28, "end": 3039.36, "text": " is the thought process behind it, and so on. I totally see ethics in any scientific discipline"}, {"start": 3039.36, "end": 3046.96, "text": " as a vital and important thing to do, and I have, I guess, unfortunately, made the experience"}, {"start": 3046.96, "end": 3053.6, "text": " of how it can be done competently, and this current state of a lot of AI and not all AI"}, {"start": 3053.6, "end": 3078.6, "text": " The current state of AI ethics is not that, and it's very much a, I can't even describe it very well, but I just complain about stuff culture, because that gets you, like, cloud, I guess, or it's easy when easy when you can always complain, right, such an easy when."}, {"start": 3078.6, "end": 3087.48, "text": " And if there is a team of competent, pragmatic people, they don't have to have the same opinions"}, {"start": 3087.48, "end": 3088.64, "text": " as I do, right?"}, {"start": 3088.64, "end": 3095.64, "text": " But they have to have the good of the good of the project and the good of humanity."}, {"start": 3095.64, "end": 3099.48, "text": " I guess in mind, yeah, that's cool."}, {"start": 3099.48, "end": 3100.48, "text": " But some, you know, it's true."}, {"start": 3100.48, "end": 3102.48, "text": " Or some not like the king of this, right?"}, {"start": 3102.48, "end": 3104.88, "text": " Like, I'm not the king of open as this."}, {"start": 3104.88, "end": 3111.28, "text": " And I don't get, if people want to conglomerate and talk about the ethics of all of"}, {"start": 3111.28, "end": 3116.68, "text": " this and, and, you know, paying us with inputs, like, cool, cool."}, {"start": 3116.68, "end": 3119.68, "text": " But I mean, you know, when we do talk about some of these risks around misinformation"}, {"start": 3119.68, "end": 3126.76, "text": " and bias, I mean, public accountability, public awareness, the ethicists have done stuff"}, {"start": 3126.76, "end": 3131.64, "text": " like producing model cards and, you know, like, making it clear what the data bias is"}, {"start": 3131.64, "end": 3153.18, "text": " stuff like that. Do you think that's useful? Yes. I mean, what is a model card? A model"}, {"start": 3153.18, "end": 3159.5, "text": " card is a read me, right? And then it has some structure to it. It's, it says here are the,"}, {"start": 3159.5, "end": 3165.9, "text": " here are the, the things you could describe about your model. And here are some examples. I think"}, {"start": 3165.9, "end": 3173.1, "text": " it's useful to have that to have as a norm in the community to say, you know, if I publish"}, {"start": 3173.1, "end": 3179.26, "text": " a model, I sort of report what it's been trained on, how it's been trained on, and even to a"}, {"start": 3179.26, "end": 3187.42, "text": " degree like what I think it could do or should be used for although, yeah, if the structure of such"}, {"start": 3187.42, "end": 3192.46, "text": " a model car gets like too rigid and it's like, no, we must use, we must ask these questions,"}, {"start": 3192.46, "end": 3199.7, "text": " you get into so many ridiculous situations like, you know, can this be reproduced?"}, {"start": 3199.7, "end": 3205.98, "text": " Well, I just, I like, I, I, I made SK learn the linear regression, right?"}, {"start": 3205.98, "end": 3214.78, "text": " Yes, it can be, you get into into situations where the questions don't address what"}, {"start": 3214.78, "end": 3217.46, "text": " you would actually like to express in such a thing."}, {"start": 3217.46, "end": 3221.66, "text": " And then I think it becomes counterproductive, but as a norm, to have, hey, look, if you"}, {"start": 3221.66, "end": 3227.5, "text": " publish something, people should be able to understand and potentially reproduce it."}, {"start": 3227.5, "end": 3233.94, "text": " That's standard we have had in papers for a long time and is generally been a good"}, {"start": 3233.94, "end": 3241.46, "text": " standard to say, look, if you publish something, I must be able to from reading it"}, {"start": 3241.46, "end": 3248.46, "text": " to understand what's in there and to have that as a norm in the community, yeah, I'm totally"}, {"start": 3248.46, "end": 3249.46, "text": " fine with that."}, {"start": 3249.46, "end": 3272.46, "text": " Do you think there's any relationship with the Chomsky syndrome that we were talking about earlier, which is this idea that we should have a very clear model of understanding of how these things work in society and we should be able to extrapolate and control things and that the fear is that basically this is just a complete black box and who knows what's going to happen?"}, {"start": 3272.46, "end": 3279.45, "text": " Nah, I'm good with the black box."}, {"start": 3279.45, "end": 3281.77, "text": " It keeps things exciting and interesting."}, {"start": 3281.77, "end": 3287.13, "text": " And as I said, I don't believe the sort of runaway,"}, {"start": 3287.13, "end": 3291.17, "text": " it might become very influential."}, {"start": 3291.17, "end": 3294.05, "text": " And so on and certainly that's not very good,"}, {"start": 3294.05, "end": 3297.45, "text": " but then again, I don't know what to do about it."}, {"start": 3297.45, "end": 3303.09, "text": " Certainly if some people sign a change.org moratorium petition,"}, {"start": 3303.09, "end": 3306.33, "text": " is even if if it's reached, it's not gonna help."}, {"start": 3306.33, "end": 3307.05, "text": " Right."}, {"start": 3307.05, "end": 3308.53, "text": " What are you gonna do?"}, {"start": 3308.53, "end": 3311.17, "text": " That doesn't matter being worried about it."}, {"start": 3311.17, "end": 3312.17, "text": " We've got a few minutes left."}, {"start": 3312.17, "end": 3313.93, "text": " So I've got some questions from John Boatron."}, {"start": 3313.93, "end": 3315.33, "text": " Say hello, John Boatron."}, {"start": 3315.33, "end": 3316.53, "text": " Hello, John Boatron."}, {"start": 3316.53, "end": 3319.17, "text": " He's our forum administrator and he's a legend."}, {"start": 3319.17, "end": 3321.97, "text": " And quick question, do you think all AI research"}, {"start": 3321.97, "end": 3324.77, "text": " should be open and accessible to the public?"}, {"start": 3324.77, "end": 3326.13, "text": " Um, no."}, {"start": 3326.13, "end": 3330.21, "text": " Uh, it's totally legitimate that a business does internal research,"}, {"start": 3330.21, "end": 3332.21, "text": " like all companies do."}, {"start": 3332.21, "end": 3338.53, "text": " and that they then use that to make money, like that's very cool with me. I've never"}, {"start": 3339.33, "end": 3344.69, "text": " never said that shouldn't be the case. Only that company shouldn't do that but at the same time"}, {"start": 3344.69, "end": 3352.93, "text": " claim how open and and and they're all democratizing and beneficial to the common good they are."}, {"start": 3353.49, "end": 3361.97, "text": " Okay and you would accept that some research could lead to negative or unethical applications"}, {"start": 3361.97, "end": 3364.97, "text": " and might need to be restricted to them."}, {"start": 3364.97, "end": 3365.97, "text": " Yeah."}, {"start": 3365.97, "end": 3366.97, "text": " Yeah."}, {"start": 3366.97, "end": 3373.37, "text": " I totally accept that some research can and will probably lead to overall negative effects"}, {"start": 3373.37, "end": 3377.37, "text": " for society or for certain individuals within society."}, {"start": 3377.37, "end": 3378.37, "text": " Right?"}, {"start": 3378.37, "end": 3384.37, "text": " Like self-flying drones from any regime in the world."}, {"start": 3384.37, "end": 3390.97, "text": " They probably they run one of, they run one of, they're certainly run some open source components"}, {"start": 3390.97, "end": 3395.13, "text": " as part of their guidance system, right? They maybe run a Linux kernel,"}, {"start": 3395.13, "end": 3400.65, "text": " like who knows. But I don't think the Linux kernel should not be fully open source and"}, {"start": 3400.65, "end": 3407.77, "text": " accessible to everyone. And I don't want anyone to be able to be the"}, {"start": 3407.77, "end": 3413.93, "text": " desider of, you know, the eternal desider of who's good enough to use the Linux kernel or not."}, {"start": 3413.93, "end": 3426.49, "text": " I'd rather, I think the overall welfare of society and humanity is much better served by accepting"}, {"start": 3426.49, "end": 3432.09, "text": " that some people are going to do some bad things with it and then mitigating that in a different way."}, {"start": 3432.09, "end": 3440.89, "text": " Then having, like, pointing, you know, the king of that model to decide, you know, who, who,"}, {"start": 3440.89, "end": 3445.85, "text": " they deem pure hearted enough to wield it."}, {"start": 3445.85, "end": 3446.85, "text": " Cool."}, {"start": 3446.85, "end": 3451.13, "text": " What's next for ML News and your channel, and are you making any more music videos with"}, {"start": 3451.13, "end": 3452.13, "text": " AI?"}, {"start": 3452.13, "end": 3453.13, "text": " Yeah."}, {"start": 3453.13, "end": 3460.81, "text": " Let's become, there are so many good music videos on AI and I'm always amazed by how talented"}, {"start": 3460.81, "end": 3466.13, "text": " people are and how quickly they pick up the new stuff and do something with it."}, {"start": 3466.13, "end": 3472.77, "text": " So that's very cool. I want, as I said, I've not made too many videos because I've been extremely"}, {"start": 3472.77, "end": 3480.37, "text": " busy with open assistant. And I think we've also built up sort of a momentum in the direction"}, {"start": 3480.37, "end": 3486.77, "text": " right now and there are many competent people in our team. So I'm also looking to make a couple"}, {"start": 3486.77, "end": 3494.45, "text": " of more videos again, paper reviews news, but also a bunch of projects which I always want to do,"}, {"start": 3494.45, "end": 3501.01, "text": " but then they take time of course and but I'm very excited about just some of the new stuff that's"}, {"start": 3501.01, "end": 3508.37, "text": " possible and try it out and to show people a little bit of of what one could do and how to have"}, {"start": 3508.37, "end": 3514.45, "text": " fun with these things. And just enclosing, have you got any shoutouts for people in your life"}, {"start": 3514.45, "end": 3517.85, "text": " for in Discord, who have really helped you on the journey?"}, {"start": 3517.85, "end": 3524.53, "text": " Um, too many, like, way too many, too, too, to, um, to name by name."}, {"start": 3524.53, "end": 3530.85, "text": " This is, I could, I could go on, like, eternal lists in open assistance, specifically, um,"}, {"start": 3530.85, "end": 3537.73, "text": " Andreas Kep has been extremely influential in that, um, the, like, just organizing things,"}, {"start": 3537.73, "end": 3543.53, "text": " but also coding things himself, but also like, also all the, all the other, as I said,"}, {"start": 3543.53, "end": 3546.77, "text": " if I start listing people, I'm gonna miss someone,"}, {"start": 3546.77, "end": 3548.41, "text": " and I don't wanna do that."}, {"start": 3548.41, "end": 3550.33, "text": " So I don't wanna start listing people,"}, {"start": 3550.33, "end": 3553.33, "text": " but then I think, well, I really wanna list the people."}, {"start": 3553.33, "end": 3555.57, "text": " It's an eternal conundrum,"}, {"start": 3555.57, "end": 3559.93, "text": " so it like to anyone who's ever had any part of helping me"}, {"start": 3559.93, "end": 3564.33, "text": " or given feedback or even like,"}, {"start": 3564.33, "end": 3567.85, "text": " Ben Kanovadek, like, it's appreciated."}, {"start": 3567.85, "end": 3571.57, "text": " And yeah, it's been amazing,"}, {"start": 3571.57, "end": 3577.57, "text": " the amount of help and input you get from good-willed people."}, {"start": 3577.57, "end": 3583.45, "text": " Yeah, communities are amazing. So join Yannick's Discord, join our Discord, open assistant"}, {"start": 3583.45, "end": 3590.81, "text": " Discord. Dr. Kilch, thank you so much. This has been an absolute pleasure. Thanks for"}, {"start": 3590.81, "end": 3591.33, "text": " having me."}]}