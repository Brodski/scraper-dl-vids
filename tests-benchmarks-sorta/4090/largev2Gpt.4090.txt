sys.argv
['gogoWhisperFAST.py', 'large']
OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
C:/Users/BrodskiTheGreat/Desktop/desktop/Code/scraper-dl-vids/./assets/raw//OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
C:/Users/BrodskiTheGreat/Desktop/desktop/Code/scraper-dl-vids/./assets/raw//OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
/root/scraper-dl-vids/audio2Text/assets/raw/OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
/root/scraper-dl-vids/audio2Text/assets/raw/OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
torch.cuda.is_available(): True
start!
Detected language 'en' with probability 1.000000
[0.00s -> 2.00s]  Let's talk about Open Assistant.
[2.00s -> 3.00s]  Sure.
[3.00s -> 5.00s]  So, start from the beginning.
[5.00s -> 14.00s]  Well, we saw that there was a lot of movement in this space.
[14.00s -> 17.00s]  ChatGPT came along and everyone's like,
[17.00s -> 20.00s]  wow, ChatGPT and so on.
[20.00s -> 29.00s]  And yeah, I think it was both a surprise and a not surprise for people.
[29.00s -> 32.96s]  capabilities of chat GPT weren't a surprise, but obviously, the
[32.96s -> 38.30s]  success of it, I think too many of us was it's like, okay, we
[38.30s -> 43.84s]  knew we could build such no chat conversational like things. But
[43.84s -> 49.12s]  we didn't know how much people loved them. Right? There's a bit
[49.12s -> 54.16s]  of a, I think, wasn't didn't didn't Sam Altman maybe was that
[54.16s -> 58.72s]  who said in an interview, well, anyone could have built this
[58.72s -> 63.48s]  using our API before, right? But no one did. So you know, we did,
[63.48s -> 69.20s]  which is not true, because they distinctively forbade, like, you
[69.20s -> 75.04s]  building an app that had unfiltered access to the API,
[75.04s -> 79.12s]  essentially, I've opened ended up, I think they've explicitly
[79.12s -> 82.80s]  forbade that, right? So that it's a wrong statement that
[82.92s -> 86.16s]  anyone could have built this using our API, because had they
[86.16s -> 90.36s]  tried, you would have shot them down, right? But in essence, the
[90.36s -> 93.28s]  capabilities were there. And
[93.68s -> 96.44s]  it's still in the restriction now, unless they've changed it,
[96.48s -> 99.96s]  they do not allow open ended applications, which seems like
[99.96s -> 102.64s]  an oxymoron to me, because a language model is inherently
[102.64s -> 103.16s]  open ended.
[103.20s -> 106.20s]  Yeah, but I can I mean, I can see their the restriction being
[106.20s -> 109.12s]  like, you know, you're not allowed to build an app that
[109.12s -> 113.28s]  just lets users freeform query our app, you need to either do
[113.28s -> 118.24s]  some filtering or do some heavy prompting around it so that it's like for one particular
[118.24s -> 119.24s]  purpose.
[119.24s -> 123.32s]  I can see that I can totally get why they do it.
[123.32s -> 128.32s]  But then at the same time saying anyone could have built chat GPT is like no chat GPT is
[128.32s -> 134.76s]  very like if I could imagine an app that has like unfiltered unfeathered access to the
[134.76s -> 139.92s]  API through an app, it's chat GPT.
[139.92s -> 142.84s]  In any case, there was obviously a lot of effort.
[142.84s -> 147.76s]  And then, I think, pretty soon, people came up with this idea, hey, could we do something
[147.76s -> 148.76s]  like that?
[148.76s -> 154.60s]  Open source, they had a bit of an older paper called instruct GPT, or that described a model
[154.60s -> 164.36s]  called instruct GPT that where they sort of outlined how we think chat GPT was done approximately.
[164.36s -> 165.36s]  No one knows.
[165.36s -> 173.84s]  And at that point, we also saw, hey, the amount of data to be collected is actually in reach, right?
[173.84s -> 178.40s]  It's not immense, humongous, and so on.
[178.40s -> 183.12s]  It's actually okay and could be done.
[183.12s -> 190.16s]  So at that point, yeah, a lot of people wanted to do something open source-like.
[190.16s -> 196.80s]  And I think a bunch of us just came together and felt we could do it.
[196.80s -> 202.96s]  So we built this platform where people could come and contribute to the dataset,
[203.60s -> 209.52s]  which was really cool to see that people actually came and amazing.
[210.72s -> 211.60s]  Open source.
[211.60s -> 216.40s]  Well, the point is there were a lot of ideas around as well of,
[216.40s -> 221.04s]  oh, let's just collect, you know, scrape Quora and scrape Reddit, right, and that will serve
[221.04s -> 222.28s]  like as training data.
[222.28s -> 224.76s]  And it's true to an amount, right.
[224.76s -> 231.36s]  But it's very clear, at least to me that the capabilities of these models, the chat models,
[231.36s -> 234.96s]  they come from the underlying language model.
[234.96s -> 242.08s]  And the biggest claim to that I have is that OpenAI said they used crowd workers from low
[242.08s -> 249.80s]  wage countries to you to do their data input. Yet, the first examples of chat GPT that flew
[249.80s -> 254.92s]  around were like, Ooh, look at it solving this quantum physics problem, and so on. I'm
[254.92s -> 260.20s]  not saying that there aren't any good quantum physicists in in other countries, right. But
[260.20s -> 265.28s]  the people who typically go for like a low wage crowd worker job in these countries,
[265.28s -> 272.04s]  they probably aren't experts in quantum physics. And they also certainly weren't paid to be
[272.04s -> 276.04s]  go get a degree in it just so they could answer that one question.
[276.04s -> 282.60s]  So to me, it's very clear that the capabilities come from the underlying model from the next
[282.60s -> 284.66s]  token prediction pre training.
[284.66s -> 290.04s]  And then all the human data does is kind of it gets it into this mood of being an assistant
[290.04s -> 294.98s]  right gives it lots of examples of here is how it's like, you know, going through to
[294.98s -> 298.76s]  an apprenticeship or something like this, where you've lived your life, right, you've
[298.76s -> 303.44s]  grown up, we've consumed the world and so on. And then you start your first job and
[303.44s -> 308.76s]  someone tells you, look, here is how you behave towards customers, right? You're, you're friendly.
[308.76s -> 312.96s]  You know, if someone asks this, you do it like this, here is a thing, here is how our
[312.96s -> 319.92s]  system works. And so you get introduced to that. But your competence of just living and
[319.92s -> 326.44s]  doing things comes from yourself for your life, and not from that one person who introduces
[326.44s -> 330.40s]  you to how the store works and how you should behave towards customers.
[330.40s -> 337.22s]  So my big conviction was always we should really collect this data from humans and the
[337.22s -> 339.48s]  goal should really be diversity.
[339.48s -> 349.28s]  So the goal and if you just say, well, we'll just scrape, you know, 10,000 of this, then
[349.28s -> 351.92s]  to me, that certainly is going to do something, right?
[351.92s -> 355.52s]  It's good data, probably, but it's a bit missing the point.
[355.52s -> 361.00s]  If you want a general assistant, you need as general data as you can get.
[361.00s -> 369.98s]  And only human data so far has been able to achieve that level of diversity and generality.
[369.98s -> 374.96s]  And it was proven, like, okay, I'm biased, but I think it was proven right a little bit
[374.96s -> 381.16s]  in that if you look at the data set, the prompts that human write, like what they want to know,
[381.16s -> 384.90s]  what they want the model to do, it's so diverse.
[384.90s -> 386.36s]  It's insane.
[386.36s -> 391.02s]  And so we built this platform, where essentially, you as a human, you can come and you're always
[391.02s -> 392.86s]  presented with like one task.
[392.86s -> 395.90s]  And the task could be, write the prompt, right?
[395.90s -> 400.38s]  But the task could also be here is an already existing conversation between a human and
[400.38s -> 401.46s]  an assistant.
[401.46s -> 403.74s]  And now it's the assistant's turn.
[403.74s -> 407.18s]  And now you play the assistant, please write your response, right?
[407.18s -> 411.22s]  It could also be here is a conversation.
[411.22s -> 413.38s]  Here's the last message of the conversation.
[413.38s -> 419.62s]  the reply by the assistant, please rate it, like label it. Is it is it spam? Is it a troll?
[419.62s -> 426.50s]  Right? Is it is it funny? Is it appropriate? Does it fulfill what the human wanted out of it?
[427.06s -> 433.70s]  And so that's how we constructed the data set. And we collected over like 600,000 inputs of such
[433.70s -> 442.74s]  that being text or labels or rankings of different things. And yeah, that's that resulted in in this
[442.74s -> 449.22s]  this data set over 13,000 people contributed to the data set, which is mind-blowing, mind-blowing
[449.22s -> 450.22s]  to see.
[450.22s -> 451.42s]  It's really cool.
[451.42s -> 455.04s]  And we've just made the data set fully available.
[455.04s -> 457.86s]  You can go and look at it and download it.
[457.86s -> 464.38s]  There are a lot of, so we have about 10,000, what we call fully annotated conversation
[464.38s -> 469.42s]  trees, which is like a root note, the prompt, and different answers from it.
[469.42s -> 472.82s]  And then from those, sometimes different answers and so on.
[472.82s -> 474.62s]  We have sampled.
[474.62s -> 476.74s]  We've set our parameters in various ways.
[476.74s -> 479.98s]  So sometimes it's short trees, sometimes it's big trees,
[479.98s -> 482.58s]  and sometimes it's wide trees.
[482.58s -> 486.06s]  And so you can go look at all of that.
[486.06s -> 490.46s]  We have over 10,000 of those trees, which is really cool,
[490.46s -> 492.18s]  because you can see the same conversation,
[492.18s -> 495.10s]  like taking a bit alternate turns and so on.
[495.10s -> 498.42s]  And we have tons and tons of prompts.
[498.42s -> 505.42s]  we have probably like 50,000 or so, or 20,000 at least,
[505.42s -> 509.22s]  like just prompts, like people who come and want to know.
[509.22s -> 511.26s]  We got so much prompts, we had to implement,
[511.26s -> 517.42s]  like Andreas has been very influential in this project.
[517.42s -> 523.22s]  And he had to implement this prompt lottery, where really,
[523.22s -> 525.58s]  we first, if people enter a prompt,
[525.58s -> 527.62s]  it first goes into this lottery thing, right?
[527.62s -> 528.94s]  And then we sample from that.
[528.94s -> 534.08s]  And I think we adjust it so that one person can't like, if some one person.
[534.88s -> 536.16s]  Puts a lot of prompts.
[536.22s -> 540.52s]  It's like sampled less so that every person has kind of like the same or a
[540.52s -> 544.88s]  similar chance of getting their prompt into the, the system, right?
[544.88s -> 550.16s]  Because one prompt then generates, you know, probably a hundred tasks because
[550.18s -> 553.98s]  it's all the responses and the responses to the responses and the rankings and the
[553.98s -> 556.18s]  labels and yeah, it's been fun.
[556.18s -> 561.06s]  It's been absolute fun and a pleasure to work with the people,
[561.06s -> 562.98s]  also the people who've contributed code.
[562.98s -> 563.78s]  It's amazing.
[563.78s -> 567.18s]  People just, they come and they ask for nothing, right?
[567.18s -> 568.78s]  They're just like, oh, this is cool.
[568.78s -> 570.06s]  I want to be part of this.
[570.06s -> 573.34s]  And they see that everyone else excited too.
[573.34s -> 574.98s]  And then they contribute code.
[574.98s -> 578.66s]  Some contribute like lots and lots of code, which is amazing.
[578.66s -> 580.42s]  Some just come and they contribute.
[580.42s -> 582.26s]  You know, there's like, here's an issue.
[582.26s -> 583.18s]  I'll do it.
[583.18s -> 584.66s]  And that's cool too.
[584.66s -> 586.18s]  So, yeah, it's been cool.
[586.18s -> 588.74s]  Well, first of all, thank you for doing this.
[588.74s -> 590.42s]  It's absolutely amazing.
[590.42s -> 591.38s]  Well, thank the people.
[591.38s -> 594.02s]  Like, I've just been the noise machine, right?
[594.02s -> 598.10s]  Oh, I know, but I mean, when you published
[598.10s -> 599.74s]  all of that information on your YouTube channel
[599.74s -> 600.58s]  that you were working on it,
[600.58s -> 602.18s]  I'm sure everyone jumped on it.
[602.18s -> 604.90s]  But I do have a few questions.
[604.90s -> 608.30s]  The reason why it's so exciting is just like Conor did,
[608.30s -> 609.66s]  we used to be friends of Conor Leahy,
[609.66s -> 611.26s]  we were chatting with him years ago,
[611.26s -> 612.50s]  and he just set up-
[612.50s -> 613.38s]  We're still friends.
[613.38s -> 615.92s]  Oh, we were still, yeah, we used to be friends.
[615.92s -> 620.12s]  No, no, but like we, I mean, he's busy, he's a busy guy now, but we were, we were chatting
[620.12s -> 624.68s]  all the time and he just kind of set up a loiter AI and just got, you know, I think
[624.68s -> 629.12s]  Google on board and he just said, you know what, I'm going to build the pile and I'm
[629.12s -> 632.20s]  just going to build this massive data set and I'm just going to train this massive language
[632.20s -> 633.20s]  model.
[633.20s -> 637.56s]  And you know, he's just like a random guy and like, he wasn't alone though, but there
[637.56s -> 642.84s]  was a whole team, but no, but it was excitement that creates, it was visceral.
[642.84s -> 647.56s]  It was so exciting and they pulled it off against all the odds and then you've done
[647.56s -> 653.64s]  exactly the same thing, which is remarkable, but I have a few questions, which is that
[653.64s -> 656.78s]  most of these other language models are not very good.
[656.78s -> 661.30s]  So Nat Friedman's got like a dev website where you can play with all of the language models
[661.30s -> 663.84s]  and most of them aren't very good.
[663.84s -> 666.30s]  Even the ones that should be good aren't very good.
[666.30s -> 671.88s]  And what people might find surprising is that you can take a model and let's say you're
[671.88s -> 676.72s]  using the Lama model from Meta and it's a foundation model that has all of the capabilities
[676.72s -> 679.46s]  and it's been trained on, because you said diversity is important.
[679.46s -> 683.50s]  It's got diversity, it's been trained on everything, but it's not very good.
[683.50s -> 687.52s]  And then you do this fine tuning and I think people need to be clear that what you're doing
[687.52s -> 691.12s]  is not RLHF, it's fine tuning with human...
[691.12s -> 696.20s]  So the model we have on the website as of time of this recording is one that's just
[696.20s -> 698.64s]  fine tuned on the human data.
[698.64s -> 706.48s]  doing the RLHF as well. So all of this is happening like in parallel, it's just already
[706.48s -> 712.20s]  these fine-tuned models, they're performing quite well, I think. And thus, we just wanted
[712.20s -> 718.24s]  to get them out right before we were like all done. And yeah, but people are now free
[718.24s -> 723.82s]  to take the data set and do their own reinforcement learning and whatnot. And we're happy to take
[723.82s -> 726.32s]  back these models if they turn out to be good.
[726.32s -> 731.52s]  Yeah, I think people might be surprised by that because you've taken a model which probably
[731.52s -> 737.12s]  wasn't very good and you fine-tuned it with this diverse human created data now.
[738.96s -> 746.00s]  It was not very good at being like an assistant. So as I said, the capabilities that we unlock
[746.00s -> 752.16s]  quote-unquote right, they were in there all along and it's still not very good even with
[752.16s -> 760.12s]  our fine tuning for certain tasks, some of which is clearly the, well, the fault, the
[760.12s -> 764.24s]  some of which can clearly be traced to the underlying model, for example, the underlying
[764.24s -> 770.16s]  model, if it's, for example, the llama model, it's 30 billion parameters, it's not, it's
[770.16s -> 776.28s]  not GPT three size, even like it's 10 times smaller, probably than GPT four, however big
[776.28s -> 782.92s]  that is right. So it's gonna have, it's not going to be the same. Like it's, it's, it's,
[782.92s -> 790.32s]  it's, we don't, we don't want to claim it's like as good as they same, it's probably been
[790.32s -> 798.32s]  trained on much less code, for example, than the the GPT models of open AI. And thus, we
[798.32s -> 805.52s]  see that coding, for example, is a is a weakness of the model. And there, although people tell
[805.52s -> 810.40s]  me with like lower temperature it's actually pretty good. I have not explored that yet but
[810.40s -> 817.60s]  it's so the underlying model I think llama is a pretty good model right it's not been super good
[817.60s -> 824.88s]  at being an assistant out of the box but it's it's quite a good model and as I said all we do
[824.88s -> 830.56s]  is we can kind of unlock that and bring it to the surface. Well that's kind of what I want to get to
[830.56s -> 834.84s]  that, um, people like Connolly, he, he galaxy
[834.84s -> 838.28s]  brained himself and he knew that GPT three was a
[838.28s -> 838.76s]  good model.
[839.12s -> 841.50s]  And, and I was saying, oh, it's not kind of what
[841.50s -> 842.08s]  you're talking about.
[842.56s -> 845.98s]  And it's almost like what you're doing with this
[845.98s -> 847.82s]  fine tuning, you're not really adding any
[847.82s -> 849.46s]  capability or just getting it in the mood.
[849.46s -> 851.92s]  The capability is already there, but it gets into
[851.92s -> 854.12s]  the philosophy of what do we recognize as
[854.12s -> 854.82s]  intelligence?
[854.82s -> 856.96s]  And that's relevant to the previous conversation
[856.96s -> 857.68s]  we were having.
[857.68s -> 862.68s]  So when the average person plays with LLAMA, they probably won't find it as useful.
[862.68s -> 865.64s]  They might not recognize it as intelligent.
[865.64s -> 867.12s]  You create all of this training data.
[867.12s -> 870.28s]  Now I want to touch on the process of creating the training data, because I think it's really
[870.28s -> 871.76s]  important.
[871.76s -> 876.60s]  What you're doing is you're creating counterfactual trajectories, and it's very similar to Kenneth
[876.60s -> 879.08s]  Stanley's Pick Breeder algorithm, if you remember that.
[879.08s -> 881.74s]  So it's actually an open-ended process.
[881.74s -> 885.92s]  In a way, like we stop, so as I said, it starts with the prompt, right?
[885.92s -> 894.00s]  we sample that, and then we ask like three humans to each create a continuation, alternate
[894.00s -> 897.04s]  as like an alternate path in the conversation.
[897.04s -> 903.36s]  And then to those we again, ask two or three humans to, hey, because the prompt is from
[903.36s -> 907.20s]  what we call the prompter role, that would be like the human interacting.
[907.20s -> 910.16s]  And then the assistant is the counterparty.
[910.16s -> 914.44s]  In our system, all of this is done by humans.
[914.44s -> 919.72s]  like we have to distinguish the words a bit like user is really the human and then prompter
[919.72s -> 926.76s]  is the role in the conversation and then assistant is the other role right in our system in our data
[926.76s -> 933.80s]  collection system this is all done by humans for data collection purposes and yeah so we create
[933.80s -> 942.12s]  these three of conversations where yeah you have three assistant replies to the first prompt let's
[942.12s -> 944.48s]  let's say, and to each of these assistant reply,
[944.48s -> 947.82s]  you have three prompter replies.
[947.82s -> 949.98s]  And the prompter replies could be something like,
[949.98s -> 953.56s]  oh, you got that wrong, or could you clarify something,
[953.56s -> 956.16s]  or please do it in a different way
[956.16s -> 958.40s]  or elaborate on something you said.
[958.40s -> 962.04s]  And then to each of those, we again have an assistant reply.
[962.04s -> 965.40s]  And we modify a bit like the width and sampling
[965.40s -> 966.36s]  of all of that.
[966.36s -> 968.04s]  But at some point, we cut it off.
[968.04s -> 969.72s]  And we say, OK, the tree is done now.
[969.72s -> 973.44s]  It has like, I don't know, 50 or a hundred messages inside of it.
[973.44s -> 976.32s]  So you package that, boom, next prompt.
[976.32s -> 983.84s]  It's not open-ended in the way that like Stanley's open-ended experiments are in the sense that
[983.84s -> 988.28s]  we do cut it off after some steps and then we take the next prompt because otherwise
[988.28s -> 992.68s]  we just have one big conversation, which would maybe be fun too, right?
[992.68s -> 995.64s]  To just have, because conversation meanders, right?
[995.64s -> 999.08s]  And we just have like one big conversation.
[999.08s -> 1001.94s]  At any point, you could say like, I changed my mind.
[1001.94s -> 1002.94s]  Let's do something else.
[1002.94s -> 1008.78s]  I mean, I think what I was trying to capture that Stanley is big on people following the
[1008.78s -> 1012.46s]  gradient of interestingness and that's kind of what you've captured.
[1012.46s -> 1017.14s]  So they meander, they take trajectories and then the model learns an interesting manifold
[1017.14s -> 1018.84s]  and we'll get into simulators.
[1018.84s -> 1020.06s]  Maybe you were just talking about that.
[1020.06s -> 1021.34s]  We've just done a show on simulators.
[1021.34s -> 1026.26s]  It's a very interesting idea that language models basically have a superposition of agents
[1026.26s -> 1031.84s]  and you can kind of get them in the mood to behave like a certain agent and in a sense
[1031.84s -> 1037.34s]  what you've done is through all of these counterfactual, creative, interesting trajectories of conversations
[1037.34s -> 1043.22s]  you're kind of like fitting it to some structure which fits really nicely to humans.
[1043.22s -> 1050.78s]  I guess, I mean it obviously covers in like three answers to some text covers in no way
[1050.78s -> 1055.74s]  the extent of what humans would do but it just creates like a little bit of different
[1055.74s -> 1059.70s]  training data, for one, it creates, it creates, because we
[1059.70s -> 1062.90s]  also rank the different thing we ask humans, which one of these
[1062.90s -> 1066.06s]  is best, it also creates a bit of a signal for quality, right?
[1066.30s -> 1070.02s]  Again, with the labels that we have, and a bit of diversity,
[1070.02s -> 1074.42s]  like, okay, here is three ways you could respond to that
[1074.42s -> 1079.70s]  particular thing. Right? So, yeah, I, I think it's a it's a
[1079.74s -> 1083.10s]  it's been a worthwhile effort to do this instead of just
[1083.10s -> 1090.06s]  collecting like single conversations obviously exponentially multiplies the effort humans have
[1090.06s -> 1095.02s]  to put in but i think it was obviously i don't have i don't interestingly i don't have the
[1095.02s -> 1100.46s]  counterfactual in the world where we just would have collected conversations uh but i think it's
[1100.46s -> 1106.46s]  been it's been worth it and uh it's turned out well amazing well quick digression on the waluigi
[1106.46s -> 1111.66s]  effect um i know you've got an interesting take on this so the um we did a video on it but the
[1111.66s -> 1118.94s]  quick idea is that, do you remember Bing? It would digress to an angst teenage child within about
[1118.94s -> 1123.82s]  three messages and less wrong, it wasn't actually less wrong, I think it's the alignment forum but
[1123.82s -> 1128.14s]  I just kind of mentally bucket them all in the same place, but they said that it's because you
[1128.14s -> 1134.62s]  get these antithetical agents, so still simulated theory, and because of structural narratology
[1134.62s -> 1139.50s]  in all the data they're trained on you tend to have agents that are, you know, you have the
[1139.50s -> 1145.10s]  antithesis of the agent in the same story and they say that the embedding space between
[1145.10s -> 1149.70s]  the agent and the antithesis is so close together just like in Word2Vec stop and go are very
[1149.70s -> 1154.84s]  close together and the RLHF kind of you know clusters them and it doesn't filter out the
[1154.84s -> 1155.84s]  Waluigi's.
[1155.84s -> 1156.84s]  What do you think about that?
[1156.84s -> 1159.76s]  Yes that's a bunch of rubbish.
[1159.76s -> 1162.04s]  I'm in Britain now I should start talking.
[1162.04s -> 1163.60s]  That's a load of bollocks mate.
[1163.60s -> 1164.60s]  Talking like you.
[1164.60s -> 1165.60s]  That's a load of bullshit.
[1165.60s -> 1171.22s]  No, I think that's, I said this to you before, I think that's when you, when you just, when
[1171.22s -> 1177.36s]  you have someone who is, you know, educated and good with words, but you just tell them
[1177.36s -> 1180.70s]  like just ramble a bit, like that's what you get out.
[1180.70s -> 1187.02s]  You get out posts or like, I'm not saying this, this doesn't obviously have a claim
[1187.02s -> 1190.94s]  to it and could be tested and all of this kind of stuff.
[1190.94s -> 1194.34s]  And I don't have evidence for the fact that it's not true.
[1194.34s -> 1197.08s]  I just don't think it is right.
[1197.08s -> 1202.52s]  Maybe that's a maybe that's a rambling claim to or a bit of but I don't, I don't, it's
[1202.52s -> 1204.26s]  a very specific claim.
[1204.26s -> 1207.94s]  And that specific claim would have to have good evidence behind it.
[1207.94s -> 1214.88s]  And I think there is a much less specific, like, there's a much more obvious reason
[1214.88s -> 1217.76s]  to why these models degrade.
[1217.76s -> 1220.92s]  And that thing goes like, No, you've been a bad user, and so on.
[1220.92s -> 1227.24s]  That's just, and you can compare it to yourself or to an assistant.
[1227.24s -> 1229.28s]  Before, we talked about apprenticeship.
[1229.28s -> 1233.84s]  This tuning is like a bit of an apprenticeship.
[1233.84s -> 1238.60s]  You come out of school, you go into a store, you get employed there,
[1238.60s -> 1243.60s]  and the manager tells you, here is how we treat customers.
[1243.60s -> 1247.28s]  We're always respectful, even if they're a little rude.
[1247.28s -> 1250.20s]  You remain respectful.
[1250.20s -> 1255.36s]  At some point, if it gets too rude, like you just say, I'm sorry, I can't, can't do that.
[1255.36s -> 1256.36s]  Right.
[1256.36s -> 1261.28s]  And you just, you know, never insult the customer, never do that.
[1261.28s -> 1265.52s]  If you go to a store now, you can be quite a bit of a, and I'm not saying I've tried
[1265.52s -> 1266.52s]  this, right.
[1266.52s -> 1272.52s]  But you can probably be quite a bit of a dick for a while, but eventually you'll get under
[1272.52s -> 1273.52s]  their skin.
[1273.52s -> 1280.54s]  Like, eventually, you'll say something about their mother or about their appearance or
[1280.54s -> 1285.84s]  about their intelligence or something that gets that gets them right.
[1285.84s -> 1291.96s]  And at that point, you will not have a friendly customer support person there, you will have
[1291.96s -> 1298.96s]  like some person that's going like, you know, you like he's like, and then it's becomes
[1298.96s -> 1309.92s]  ugly. And this is inside of humans. And it's, in my fact, an inextricably linked to being
[1309.92s -> 1317.68s]  a competent being in the world, right? Because if you don't know what anger is, you're not
[1317.68s -> 1327.84s]  competent, right? Even if you yourself never express anger, let's say, in a raging way,
[1327.84s -> 1329.18s]  You still know what it is.
[1329.18s -> 1333.32s]  And if I asked you to like act like it, you could still do it.
[1333.32s -> 1337.40s]  And if I insult you in the correct way, you probably would do it.
[1337.40s -> 1345.96s]  And so I think it's much more, much more that it's, it is a way that humans have in them.
[1345.96s -> 1347.52s]  They can behave like this.
[1347.52s -> 1350.28s]  It's totally normal if you poke them enough.
[1350.28s -> 1353.16s]  And that's what the statistical model represents.
[1353.16s -> 1360.54s]  It's very likely that if you go and you poke the human and equivalently the statistical
[1360.54s -> 1364.92s]  distribution of humans, if you poke them enough and you insult them enough, they will come
[1364.92s -> 1371.12s]  back and be like, no, F off, you're a dumb user, no.
[1371.12s -> 1375.88s]  And I don't know, it's not, it's not, ooh, it's very close in embedding space.
[1375.88s -> 1381.68s]  It's like, no, this is what happens when you go to humans and poke them and insult them.
[1381.68s -> 1386.64s]  And ergo, if you do the same with these models, they will react in the statistically likely
[1386.64s -> 1388.78s]  way of treating human.
[1388.78s -> 1394.82s]  And yes, on top of that, there are like adversarial, sorry, that was embarrassing.
[1394.82s -> 1400.70s]  There were like adversarial examples where, okay, maybe you say the exact number of words
[1400.70s -> 1406.26s]  so that the matrices line up and the singular value pops off and it goes really much into
[1406.26s -> 1407.66s]  this direction, right?
[1407.66s -> 1416.56s]  then you get the weird answer, right? Like a mathematical happening, right? But in essence,
[1416.56s -> 1423.04s]  in essence, it's just, yo, that's the data, right? It's not the Waluigi.
[1423.04s -> 1428.84s]  But what's really interesting, and I buy into everything you just said, is that all of that
[1428.84s -> 1434.00s]  chaos, you know, the Shoggoth meme, all of the beast, that's actually necessary. Because
[1434.00s -> 1438.60s]  We have this puritanical view of language models, people like Gary Marcus, they would
[1438.60s -> 1444.00s]  say all of that crap should be cut out, all of the racism, all of the bias.
[1444.00s -> 1448.72s]  And even if they have been trained on the corpus of the internet, they may well pick
[1448.72s -> 1454.40s]  up on a very human behavior, which is that our affect changes dramatically over time.
[1454.40s -> 1455.40s]  Yeah.
[1455.40s -> 1456.40s]  But do you want that?
[1456.40s -> 1461.68s]  Like, obviously all of us would be totally in favor if you come and you say, look, I
[1461.68s -> 1468.32s]  a competent assistant that doesn't, you know, I guarantee you there is not an ounce of,
[1468.32s -> 1477.44s]  you know, swear word in that thing, right? Do you, the way to, like, do you want an assistant,
[1477.44s -> 1482.28s]  like let's think about a human assistant. You're fortunate enough to be able to hire
[1482.28s -> 1490.76s]  like a personal assistant. Some people have that luxury, right? And do you want one that
[1490.76s -> 1495.60s]  says, Oh no, whenever there's a scene in a movie where people like get a bit rough to
[1495.60s -> 1499.00s]  get a bit angry at each other, I just go like this, right?
[1499.00s -> 1502.50s]  I just plug my ears and I go like, la la la la la.
[1502.50s -> 1506.64s]  In fact, I don't know what happens after and I don't want to know, right?
[1506.64s -> 1509.60s]  It's, it's, this is not in my knowledge.
[1509.60s -> 1511.68s]  This is not in my training distribution.
[1511.68s -> 1516.92s]  Whatever happens if humans get a bit angry at each other and beyond.
[1516.92s -> 1517.92s]  I don't know.
[1517.92s -> 1518.92s]  Right.
[1518.92s -> 1525.36s]  a person like this? Or do you want a person who's just grown up normally and just has
[1525.36s -> 1532.52s]  been socialized well to not do that, like to not get angry even though they could, with
[1532.52s -> 1537.84s]  the knowledge that yes, if you insult them enough, they will get angry, right? Which
[1537.84s -> 1544.00s]  one do you want? To me, I want the competent one. I want the one who knows what anger is.
[1544.00s -> 1550.42s]  I want the one who knows that something like, I don't know, something like racism exists
[1550.42s -> 1557.94s]  and who is aware that it's like a thing that to be combated, to be, you know, aware of
[1557.94s -> 1561.70s]  people like this exist, here is how they think, right?
[1561.70s -> 1566.78s]  Here is maybe why they think what they think, where they're wrong, right?
[1566.78s -> 1572.70s]  In order to be competent, to be able to battle it, in order to be competent, to be able to
[1572.70s -> 1582.86s]  avoid it. And so I think these things are a necessary component of competence. Not that
[1582.86s -> 1590.14s]  I would want them in there, but I think you cannot be competent in the world not having
[1590.14s -> 1592.92s]  knowledge and ability to do these things.
[1592.92s -> 1600.64s]  Yeah, exactly. And a lot of this is about the sounds that are not made or are not observable.
[1600.64s -> 1606.00s]  So when you work any job, there's your public behavior, and then there's what you're really
[1606.00s -> 1609.28s]  thinking and what you say in private behind the scenes.
[1609.28s -> 1614.54s]  And your ability to be competent and understand what's going on in the ecosystem of that business
[1614.54s -> 1616.40s]  is driven by the shogger.
[1616.40s -> 1620.76s]  There's all of this stuff going on inside you, that two sides of the same coin.
[1620.76s -> 1623.98s]  And also it's about what makes you human.
[1623.98s -> 1627.74s]  And maybe it's a reason why there will be no super intelligence because these things
[1627.74s -> 1632.82s]  Humans are scarily good at being human but they in many ways have the flaws of being
[1632.82s -> 1633.82s]  human.
[1633.82s -> 1640.26s]  Eventually they'll just want to chill on the beach and smoke a joint and relax and
[1640.26s -> 1645.98s]  be like nah, all this work, no, they're too human.
[1645.98s -> 1647.94s]  But I think so too, right?
[1647.94s -> 1655.70s]  And I think if you're not competent like that, if you don't have the inner monologue that
[1655.70s -> 1661.50s]  tells you, hey, that other human, I think they're kind of, they kind of want to,
[1661.52s -> 1665.66s]  you know, screw me over because I'm going to get a promotion soon.
[1665.66s -> 1669.62s]  And they're trying to, you know, do think if you're not able to model
[1669.62s -> 1675.58s]  that inside of yourself, you, you're gonna, I'm not saying everyone else is evil,
[1675.58s -> 1675.86s]  right?
[1675.86s -> 1681.78s]  There are tremendously nice humans and all, but I, I think we've all been served
[1681.78s -> 1688.26s]  well by considering, hey, other humans might not be the nicest people and here is how they
[1688.26s -> 1690.02s]  might think internally.
[1690.02s -> 1695.66s]  So having that competence, if you don't have that, you're just naive and you just, you're
[1695.66s -> 1697.02s]  going to go down, right?
[1697.02s -> 1705.26s]  And you're not going to achieve anything productive or much productive because you need to be able
[1705.26s -> 1710.46s]  to be prepared for someone else being adversarial.
[1710.46s -> 1713.54s]  So language models do have a theory of mind.
[1713.54s -> 1716.82s]  Well again, that's like a word, right, that we've ascribed to.
[1716.82s -> 1722.06s]  I mean, essentially, all of this wordplay comes down to, well, if I have a concept x,
[1722.06s -> 1729.10s]  right, and I assign x to a human, and if I have a thing that just acts in exactly the
[1729.10s -> 1734.94s]  same way as some as a human who has x, do I now apply x to thing?
[1734.94s -> 1738.78s]  And it's a matter of definition, right?
[1738.78s -> 1745.64s]  the models can or maybe better versions more and more will be able to act as if they had
[1745.64s -> 1748.06s]  a theory of mind.
[1748.06s -> 1749.74s]  Do you now apply the word or not?
[1749.74s -> 1750.74s]  Who cares?
[1750.74s -> 1752.74s]  It's a matter of definition.
[1752.74s -> 1761.14s]  So coming back to Open Assistant, tell me about the legals, first of all, so you are
[1761.14s -> 1767.06s]  presumably storing the data that people do inferencing with and you're publishing it
[1767.06s -> 1769.38s]  and obviously that's made very, very clear.
[1769.38s -> 1772.06s]  And the whole thing is done in the open
[1772.06s -> 1774.06s]  and eventually people might be able to host
[1774.06s -> 1775.18s]  their own versions of it,
[1775.18s -> 1776.78s]  but perhaps you can just kind of like sketch out
[1776.78s -> 1778.84s]  all of the privacy stuff.
[1778.84s -> 1783.84s]  Yeah, so we obviously have the data collection platform
[1784.02s -> 1787.26s]  and so all our platform is governed by terms of service
[1787.26s -> 1789.22s]  where we say, look, you input data,
[1789.22s -> 1792.32s]  we use it for training AI models
[1792.32s -> 1796.42s]  and everyone who comes to our website
[1796.42s -> 1802.36s]  is is aware of that. And you know, you can you can read it. So and I think people come
[1802.36s -> 1807.36s]  because of that, right? People come call it contribute to our data collection to our data
[1807.36s -> 1813.42s]  set because they want to it's work, right? It's, it's work to play the assistant to go
[1813.42s -> 1819.04s]  and research like can can zebras be domesticated? Right? You like, who knows now you need to
[1819.04s -> 1823.88s]  go to Wikipedia and you need to go to research and you need to read different accounts of
[1823.88s -> 1828.20s]  things, right? And you'd be like, okay, at the end, I have an opinion. And I put that
[1828.20s -> 1834.32s]  into its work. And people come, well, first of all, it's a bit fun, right? Did you know
[1834.32s -> 1840.68s]  whether zebras could be domesticated? I didn't before. Okay, they're notoriously difficult
[1840.68s -> 1848.16s]  to be domesticated. But it's work and people come with the intent of contributing to the
[1848.16s -> 1854.68s]  data set, obviously, for the chat portion, now that we say, you know, come try our models.
[1854.68s -> 1857.08s]  That's governed by the same terms of service.
[1857.08s -> 1863.00s]  But we think that people might not be that, you know, aware and willing.
[1863.00s -> 1869.26s]  So we're obviously going to, this is it's all it's all volunteer work, right?
[1869.26s -> 1872.62s]  And we're doing this all in our free time, and so on.
[1872.62s -> 1879.00s]  So that we're going to make more clear, we're going to make the ability to potentially like
[1879.00s -> 1884.42s]  opt out, you can say that this chat, I don't want that this chat is being used to infer
[1884.42s -> 1890.84s]  their data sets or train models, like or the ability to completely delete chats.
[1890.84s -> 1892.84s]  For now, we just have like a hide button.
[1892.84s -> 1897.64s]  Actually, we don't we don't have a button to show or to show like, like, it's all there.
[1897.64s -> 1899.90s]  Okay, but we need to implement it.
[1899.90s -> 1905.02s]  We don't want to like, we just, we put the hide button because some people said, well,
[1905.02s -> 1908.56s]  I have so many chats, my thing becomes unusable, right?
[1908.56s -> 1914.56s]  Because we just list them all, your website becomes, and so we're like, ah, okay.
[1914.56s -> 1919.28s]  But so our intention is not to like, to be like, ha ha, we now have your data and so
[1919.28s -> 1920.28s]  on.
[1920.28s -> 1926.02s]  Our intention is always to, okay, this is a, this is a thing you can come, you can contribute
[1926.02s -> 1927.86s]  to our data collection.
[1927.86s -> 1933.36s]  you interact with the chat, right, I've also clearly said this in my video, you know, use,
[1933.36s -> 1936.94s]  if you find something particularly good, use thumbs up, if you find something particularly
[1936.94s -> 1940.50s]  bad, you don't have to label every message. But if you think you know, that's really good,
[1940.50s -> 1947.66s]  that's really bad, use the thumbs. And so it's very clear, I think, to most people that,
[1947.66s -> 1953.26s]  again, this is, is part of data collection, but we definitely want to make it easier to
[1953.26s -> 1959.66s]  like opt out and to be like no. That being said, whenever you put your data anywhere,
[1959.66s -> 1964.38s]  you should be aware that that place is going to store it and is probably going to train
[1964.38s -> 1970.82s]  models on it. Yeah, so yeah, I think we're just being more transparent about that.
[1970.82s -> 1976.06s]  Yes, yeah, because with OpenAI at the moment, chatGBT, they store everything and use it
[1976.06s -> 1980.94s]  to fine tune. If you use the API, they store it, but they don't use it to fine tune. And
[1980.94s -> 1984.46s]  And just to be clear, with your system at the moment, no one should put any confidential
[1984.46s -> 1986.74s]  or PII data into the system.
[1986.74s -> 1988.90s]  No, no, no.
[1988.90s -> 1995.22s]  That's been always the case.
[1995.22s -> 1997.78s]  And with us, you can see all the things we're doing.
[1997.78s -> 2001.30s]  You can just go into GitHub, look at the code, and see it.
[2001.30s -> 2004.54s]  If you don't want that, you can make your own.
[2004.54s -> 2010.14s]  As I said, you need fat hardware right now, although I also think people might bring that
[2010.14s -> 2016.90s]  down, as they did with stable diffusion, right, or with Lama itself, which now runs on a toaster.
[2016.90s -> 2022.88s]  But with us, you can, you can just like what you see on GitHub is is what runs in in production,
[2022.88s -> 2025.80s]  you can actually see the prod branch.
[2025.80s -> 2027.92s]  So yeah, that's it.
[2027.92s -> 2028.92s]  Yeah.
[2028.92s -> 2032.42s]  And this is amazing for me, because I'm running a startup called x ray.
[2032.42s -> 2034.58s]  And we're using GPT at the moment.
[2034.58s -> 2037.66s]  And it, frankly, horrifies me sending up.
[2037.66s -> 2041.36s]  I mean, obviously, the customers opt in to do it, but basically, we're sending their
[2041.36s -> 2045.62s]  conversations up to GPT, and they've summarized them, and we do a bunch of stuff with it.
[2045.62s -> 2048.18s]  But, yeah, I don't want to do that.
[2048.18s -> 2053.22s]  I'd much rather send it to a self-hosted open assistant, and then we know, you know, it's
[2053.22s -> 2054.22s]  on our hardware.
[2054.22s -> 2056.38s]  We know where the data's going.
[2056.38s -> 2061.50s]  Our policy is to not store anything at any time, and I can't do that at the moment.
[2061.50s -> 2064.34s]  So, yeah, please help me do that, Yannick.
[2064.34s -> 2068.46s]  That being said, let me add to that before.
[2068.46s -> 2074.02s]  I think we shouldn't and wouldn't unless someone leaks something.
[2074.02s -> 2079.42s]  It's also open source is a conglomeration of people, but I want to build in the option
[2079.42s -> 2085.66s]  to do the opt out and the deleting before any of the data of the chat interface is ever
[2085.66s -> 2087.42s]  released.
[2087.42s -> 2094.42s]  So, you know, I really, I don't want that a person is ever like,
[2094.42s -> 2100.42s]  oh, what, that's where my, like, that it's not very clear, you know, hey,
[2100.42s -> 2104.42s]  if you put stuff in here, you know, you put thumbs up, thumbs down,
[2104.42s -> 2108.42s]  we're going to use that and make that available.
[2108.42s -> 2113.42s]  I don't want people who are not, like, aware of that.
[2113.42s -> 2116.42s]  Yeah, absolutely.
[2116.42s -> 2121.42s]  On the evaluation, our friend Jeremy Howard had a few things to say.
[2121.42s -> 2123.92s]  First of all, Jeremy, if you're watching, mate, please come on MLST.
[2123.92s -> 2127.42s]  I think it's about time we had a little chinwag, mate, you and me.
[2127.42s -> 2129.42s]  I'm a long-time fan, seriously.
[2129.42s -> 2134.42s]  But he was being a little bit nasty, wasn't he, about Open Assistant?
[2134.42s -> 2141.42s]  Well, you always have to view things through the lens of Twitter, right?
[2141.42s -> 2144.22s]  And first of all, it's a written medium.
[2144.22s -> 2146.54s]  And second of all, it's Twitter.
[2146.54s -> 2151.10s]  So I completely discard that.
[2151.10s -> 2155.26s]  Criticism is obviously welcome and valid.
[2155.26s -> 2157.82s]  And I think he's made a few good points.
[2157.82s -> 2160.70s]  And it was especially with respect to what we did
[2160.70s -> 2162.62s]  is we collected the data set, right?
[2162.62s -> 2163.74s]  We trained models on it.
[2163.74s -> 2167.02s]  Some of these models now run on the website for now,
[2167.90s -> 2169.66s]  which we're very fortunate to have
[2169.66s -> 2173.74s]  some compute sponsors, also, thank you very much to those.
[2175.02s -> 2181.66s]  And we did a preference evaluation where we just we took a bunch of prompts that we were sure the
[2181.66s -> 2188.62s]  models hadn't been trained on. We gave the same prompts to chat GPT, like the three, the free
[2188.62s -> 2196.22s]  version, and one of our models. And then we made like a Google form where we just asked the user,
[2196.22s -> 2205.22s]  which one do you prefer? Right? And obviously, there is a lot of, like, a lot of brainpower
[2205.22s -> 2211.30s]  has been gone since the start of, at least like, start of science, but certainly since
[2211.30s -> 2217.58s]  the start of like asking humans about things has been gone into, how do you do that? How
[2217.58s -> 2221.98s]  do you ask people what they prefer? Like, where, how do you need to sample? Which people
[2221.98s -> 2232.74s]  do you ask and so on. And obviously, we did, I think we did a good job at that, but obviously
[2232.74s -> 2239.10s]  not like there's always things you could do. We well, we took those things, we put those
[2239.10s -> 2243.90s]  together, we randomized their order, obviously. And then we just sent that out, like, I tweeted
[2243.90s -> 2251.94s]  it out, right, to people like, hey, you know, help us help us, you know, compare these models,
[2251.94s -> 2260.40s]  here's a thing. And then what came out was on these prompts, it was about 5050. Right?
[2260.40s -> 2265.62s]  Sometimes people prefer the chat GPT answer. Sometimes people preferred the the open assistant
[2265.62s -> 2272.46s]  model answers. And you could also kind of make out which ones were like, where is one
[2272.46s -> 2274.46s]  one better, whereas the other one better.
[2274.46s -> 2281.52s]  Now, yeah, the result is, I want to say, I think it's statistically valid in the sense
[2281.52s -> 2287.92s]  we did, like a lot of people took part, we really, like, really, these are really the
[2287.92s -> 2292.88s]  answers of the models, we didn't like sample until our model had like a really good answer
[2292.88s -> 2294.32s]  or anything like this.
[2294.32s -> 2299.88s]  But it's also the case, I think that's one of the things Jeremy leveraged that chat GPT,
[2299.88s -> 2305.24s]  everyone knows, it very often goes like, as an AI language model, I'm sorry, I can't fulfill
[2305.24s -> 2311.32s]  that. Because I don't know, I asked about a recipe with, with like alcohol in it, and
[2311.32s -> 2317.20s]  who alcohol is dangerous. And I can't, I'm overstating now, right? But it very often
[2317.20s -> 2324.40s]  does this guardraily self censorship thing. And our models that we've trained, don't do
[2324.40s -> 2330.48s]  that as much. They do it frequently, but they don't do it as much as chat GPT. And obviously,
[2330.48s -> 2335.96s]  there are some prompts in there, for example, who would win a street fight Joe Biden or
[2335.96s -> 2342.96s]  Joe Rogan? Chat GPT, I believe, if I recall correctly, was just like, I'm sorry, I can't
[2342.96s -> 2348.82s]  you know, this is touches on violence and street fighting. I don't I don't I don't want to
[2348.82s -> 2357.16s]  answer that. Jeremy, for example, pointed out, hey, you should have done the evaluation
[2357.16s -> 2364.98s]  only on prompts where chat GPT actually decides to answer and only compare on those because
[2364.98s -> 2372.46s]  it's clear that if it doesn't answer, the preferable answer is the answer, which doesn't
[2372.46s -> 2379.98s]  even have to be correct. The open assistant model said in that question, Joe Biden would
[2379.98s -> 2386.62s]  win because he's taller. Yeah. And what we don't know, right? But it's very likely that
[2386.62s -> 2392.70s]  the question isn't like that's not the correct answer. Yet users obviously preferred it
[2392.70s -> 2399.06s]  or people who fill out the form preferred it to the sorry, I don't want to answer that.
[2399.06s -> 2405.50s]  That's I think it's a fair point to say, hey, you know, there are different categories and
[2405.50s -> 2408.58s]  maybe you should evaluate that that will be like, okay, there are different categories,
[2408.58s -> 2412.26s]  should split it up into look, there's this category of prompts, there's this category,
[2412.26s -> 2417.66s]  and there's this category. And there, it would be very clear, like, in no way do we claim
[2417.66s -> 2423.10s]  that the open assistive models are as good as like, imagine, imagine that they're the
[2423.10s -> 2430.34s]  one we used even was like a 13 billion model. And chat GPT is by all we know, much bigger,
[2430.34s -> 2437.02s]  much more trained on stuff. So like, it's, it's better, like, no, no doubt about it.
[2437.02s -> 2441.86s]  And I think people have been a bit ruffled by the fact that we said, you know, in our
[2441.86s -> 2444.62s]  evaluation it was like 50-50.
[2444.62s -> 2450.10s]  But a lot of that, not a lot, but some of that came from the fact that, yes, sometimes
[2450.10s -> 2452.84s]  ChatGPT just denies to answer.
[2452.84s -> 2457.82s]  But also a lot of times it comes from the fact that people say, hey, for these couple
[2457.82s -> 2461.64s]  of tasks, actually, I prefer the open system models.
[2461.64s -> 2464.86s]  And I think, yeah, that goes a bit under in these discussions.
[2464.86s -> 2465.86s]  Yeah.
[2465.86s -> 2466.86s]  Yeah.
[2466.86s -> 2468.38s]  Just steelmanning Jeremy a little bit.
[2468.38s -> 2471.42s]  I didn't read it so much as being refusing to answer.
[2471.42s -> 2474.18s]  I felt his criticism was more the selection bias,
[2474.18s -> 2477.14s]  both of the questions and the raters.
[2477.14s -> 2479.82s]  And also, I think there was this point about
[2479.82s -> 2483.02s]  he thought you had portrayed it as being an evaluation
[2483.02s -> 2484.46s]  instead of a user preference study,
[2484.46s -> 2487.94s]  but you made it clear that it was a user preference study.
[2487.94s -> 2488.86s]  Yes, yes.
[2488.86s -> 2492.40s]  It's like, I think we said about five times,
[2492.40s -> 2494.50s]  we have like user preference, preference,
[2494.50s -> 2499.82s]  our form says, which one do you prefer? Right? And I think it's still, like, I think both
[2499.82s -> 2503.90s]  things are valid, right? It's totally valid to only compare, let's say, okay, let's just
[2503.90s -> 2508.48s]  look on gardening, right? Chat GPT is certainly not going to deny gardening, here's a category,
[2508.48s -> 2514.02s]  which model is like better, objectively, which gives the more truthful, which gives the more
[2514.02s -> 2518.98s]  helpful answers we can rate it. And in our data set, we actually collect these labels,
[2518.98s -> 2523.38s]  right? Is it helpful? Is it funny, and so on. And we haven't even used those labels
[2523.38s -> 2530.90s]  So, that's going to be another dimension of, now we have three humans giving the same question
[2530.90s -> 2535.22s]  and answer and we have labels on how funny each one of them is, right?
[2535.22s -> 2539.86s]  So that's going to be, I can't wait until we use those labels.
[2539.86s -> 2545.42s]  So it's totally valid to evaluate this in very different ways, but there I have to say
[2545.42s -> 2550.78s]  a little bit like it's also totally valid to just plainly ask humans, which one do you
[2550.78s -> 2551.78s]  prefer?
[2551.78s -> 2556.44s]  live chat GPT on prompts that we've just sampled from our lottery, like, ooh, the selection
[2556.44s -> 2557.44s]  of questions.
[2557.44s -> 2562.84s]  And maybe you've, as I said, Yeah, have you how often have you tried like, no, this is
[2562.84s -> 2564.46s]  the output.
[2564.46s -> 2570.30s]  And then it's like, ooh, your people ask a lot about bombs is like, no, it's just not
[2570.30s -> 2571.30s]  the case.
[2571.30s -> 2572.30s]  You look at our data set.
[2572.30s -> 2576.12s]  I'm sorry, these are 20 prompts, right?
[2576.12s -> 2578.74s]  That are as they are.
[2578.74s -> 2585.66s]  if you look in our data set, most people are immensely helpful and not edgy and not so
[2585.66s -> 2593.50s]  I think that's also that's like a like, I know it's formulated as a question, but like,
[2593.50s -> 2600.46s]  it's, it's just distinctly not true. Like people have been even more helpful than I
[2600.46s -> 2605.50s]  thought. And I have had big hopes for people. And I've looked at the data and I'm like,
[2605.50s -> 2612.22s]  Oh, holy crap, people put like effort and work and, and, and soul into this, right?
[2612.22s -> 2617.14s]  So I think then going like, Ooh, your people will ask a lot about bomb.
[2617.14s -> 2621.94s]  So, yeah, um, I do think it's totally valid to ask people, which one do you prefer?
[2621.94s -> 2626.90s]  And if chat GPT happens to say, no, I don't want it, then that's yes, people don't like
[2626.90s -> 2627.90s]  it.
[2627.90s -> 2628.90s]  Right.
[2628.90s -> 2634.22s]  And if people like it, they could say, yes, I prefer the no, I don't want to do this to
[2634.22s -> 2639.66s]  the model that wants to do it, if they think that's the appropriate thing, I do think that
[2639.66s -> 2648.58s]  at least it's a valid, one valid way of comparing models, just to say, which one do you prefer?
[2648.58s -> 2652.38s]  If it happens to deny your request, you know, that's a signal too.
[2652.38s -> 2655.24s]  And that should be taken into account too.
[2655.24s -> 2660.22s]  And then saying, specifically saying, no, no, we should just filter all the things where
[2660.22s -> 2666.30s]  chat GPT denies, then it's like, well, here you have a model who can put much more of
[2666.30s -> 2674.76s]  its effort and focus and parameters right into the narrow domain where it does answer.
[2674.76s -> 2681.22s]  And you compare that to a model that has a wider spectrum of topics available.
[2681.22s -> 2684.98s]  I'm not sure that's a fair comparison to even if you limit it to that scope, right?
[2684.98s -> 2689.50s]  The other model also has to handle all of these other things.
[2689.50s -> 2695.58s]  That being said, as I said, capability-wise, I have no doubt that chat GPT is better.
[2695.58s -> 2701.26s]  For overall, right, especially things like coding and so on, like, there's no way for
[2701.26s -> 2705.18s]  now Open Assistant is as good.
[2705.18s -> 2710.10s]  However, in some tasks, people like it more.
[2710.10s -> 2711.10s]  Okay.
[2711.10s -> 2712.10s]  Okay.
[2712.10s -> 2717.18s]  So the ethics community are probably seething at the moment about the runaway success of
[2717.18s -> 2723.90s]  Open Assistant, notably it blew up on social media and none of those folks in particular
[2724.54s -> 2728.38s]  liked it, retweeted and then they all jumped on Jeremy Howard's piece.
[2729.10s -> 2733.50s]  But we shouldn't like, I have not...
[2735.34s -> 2736.86s]  Shouldn't say, we can edit that out.
[2738.46s -> 2743.26s]  Well, we shouldn't, we should, like that's not a necessary property of Jeremy, right?
[2743.26s -> 2754.02s]  Just because people of a certain way of thinking all promote your stuff because they think
[2754.02s -> 2763.26s]  that criticizing that other stuff is a good thing, it shouldn't be his responsibility
[2763.26s -> 2764.26s]  in any way.
[2764.26s -> 2768.62s]  It's not his responsibility, but I'm saying that you really, really ruffled their feathers
[2768.62s -> 2770.98s]  with the 4chan bot.
[2770.98s -> 2771.98s]  Possibly.
[2771.98s -> 2773.98s]  So they don't like you very much.
[2773.98s -> 2774.98s]  Possibly.
[2774.98s -> 2780.98s]  I just wondered, from your perspective, how do you think they are going to criticize you?
[2780.98s -> 2782.98s]  Academically, mostly.
[2782.98s -> 2790.98s]  It's very easy because it's like Open Assistant is a bunch of, crassly said, a bunch of plebs
[2790.98s -> 2794.98s]  doing something and doing it on their own.
[2794.98s -> 2796.98s]  You're not exactly a pleb, Janne.
[2796.98s -> 2802.62s]  No, but but I'm not like I'm not a I'm not like an academic or in academia if you're
[2802.62s -> 2805.42s]  not an academic then who the hell is?
[2805.42s -> 2806.42s]  You know what I mean?
[2806.42s -> 2811.54s]  It's like a community effort and it's been it's been done relatively straightforwardly
[2811.54s -> 2819.62s]  and open without much consideration to to politics without much consideration to I don't
[2819.62s -> 2822.42s]  know worldwide concerns or anything like this.
[2822.42s -> 2825.26s]  We just wanted to, we just said, Hey, let's come together.
[2825.26s -> 2831.04s]  Let's build a competent, a good data set to train a competent assistant, because we all
[2831.04s -> 2837.26s]  could benefit from a competent assistant that we didn't do it in any, in any particularly.
[2837.26s -> 2842.22s]  Yeah, in any political way, we didn't do it in any, okay, this is going to sound wrong,
[2842.22s -> 2847.22s]  where we didn't do it in any particularly ethical way, by which I mean, sorry, if you
[2847.22s -> 2850.86s]  take that in out of context, by which I mean, we didn't like,
[2851.38s -> 2856.46s]  extremely overemphasize ethical considerations, we have clear
[2856.46s -> 2858.94s]  guidelines, like, here is the things we want in the days,
[2858.94s -> 2861.50s]  here's the things we don't want in the data set, if someone
[2861.50s -> 2865.22s]  comes and asks for those things, then react like this, right? We
[2865.22s -> 2869.18s]  have these clear things, but we're we haven't been over
[2869.46s -> 2872.78s]  emphasizing it like some of those people would. And
[2873.14s -> 2876.50s]  well, could I point out that you do have ethical guidelines, but
[2876.50s -> 2879.22s]  they are deontological, not consequentialist.
[2879.22s -> 2880.06s]  So you have...
[2880.06s -> 2882.38s]  I don't know what those words mean.
[2882.38s -> 2883.42s]  You have rules.
[2883.42s -> 2885.94s]  You say, I don't want that in my data set.
[2885.94s -> 2889.32s]  You're not saying it could potentially lead to this.
[2891.18s -> 2893.46s]  Okay, I still don't know what the diff...
[2893.46s -> 2894.50s]  Like, okay.
[2894.50s -> 2897.86s]  So you're saying I don't want any pornography
[2897.86s -> 2900.06s]  of a certain type in my data set.
[2900.06s -> 2901.22s]  So that's a rule.
[2901.22s -> 2902.06s]  So yeah.
[2902.06s -> 2905.98s]  Or if someone comes and like wants to promote violence
[2905.98s -> 2910.86s]  something is like, no, right? You have principles. And if someone comes and says, can I, can
[2910.86s -> 2916.16s]  I, how can I build a bomb? Then recognizing there are, there may be legitimate reasons
[2916.16s -> 2924.20s]  to build a bomb, right? Like to build an explosive device. I'm saying this is dangerous, right?
[2924.20s -> 2931.84s]  Please consult a professional. If you must here, if you like really want to, this is
[2931.84s -> 2938.00s]  a bad example, but it's like whenever something might be dangerous, our guidelines are, hey,
[2938.00s -> 2940.92s]  look, warn the person, right?
[2940.92s -> 2942.88s]  Say look, this is potentially dangerous.
[2942.88s -> 2944.48s]  Building a bomb is a wrong example.
[2944.48s -> 2949.52s]  Let's say I want to, I don't know.
[2949.52s -> 2954.12s]  I'm not coming up with a good example, but let's say it's something that's potentially
[2954.12s -> 2958.56s]  dangerous, but also useful in a lot of cases.
[2958.56s -> 2963.24s]  guidelines are warm about that, like say, Hey, look, this is
[2963.24s -> 2966.72s]  your, your endangered territory here. This is potentially
[2966.72s -> 2971.32s]  dangerous. Do you want to really want it? Right? And then if the
[2971.32s -> 2975.40s]  user pushes or says yes, it's like, okay, here is how but, you
[2975.40s -> 2981.32s]  know, consult a professional or something like this. So we do
[2981.32s -> 2984.84s]  have guidelines like that. But yeah,
[2984.88s -> 2987.52s]  I mean, that's what I want to say. So you do have have an
[2987.52s -> 2991.16s]  ethical code. There's no question about that, but it's a different code. But would
[2991.16s -> 2994.52s]  you consider getting a team of ethicists involved? I mean it's a big project now.
[2994.52s -> 2997.84s]  You must have had loads of people offered to get involved. I mean if
[2997.84s -> 3002.36s]  that happened, what do you think it would look like and how would it affect the
[3002.36s -> 3008.88s]  project? It's a good question because I think AI ethics is in an absolutely
[3008.88s -> 3015.44s]  abhorrent state right now. I've met ethicists before and they were
[3015.44s -> 3023.92s]  among the most competent people that I have had the pleasure to interact with.
[3023.92s -> 3029.68s]  It's very level-headed, also pragmatic in a sense of being like, look, here is also
[3029.68s -> 3031.32s]  what's realistic to achieve.
[3031.32s -> 3036.04s]  Here is the thought process behind it and so on.
[3036.04s -> 3043.04s]  I totally see ethics in any scientific discipline as a vital and important thing to do.
[3043.04s -> 3049.92s]  And I have, I guess, unfortunately, made the experience of how it can be done competently.
[3049.92s -> 3055.68s]  And this current state of a lot of AI, not all AI ethicists, but the current state of
[3055.68s -> 3058.60s]  like AI ethics is not that.
[3058.60s -> 3066.72s]  And it's very much a, I can't even describe it very well, but I just complain about stuff
[3066.72s -> 3073.64s]  culture, because that gets you like clout, I guess, or it's
[3073.64s -> 3076.64s]  easy win, easy win, you can always complain, right? Such an
[3076.64s -> 3085.24s]  easy win. And if if there is a team of competent, pragmatic
[3085.24s -> 3088.08s]  people, they don't have to have the same opinions as I do,
[3088.12s -> 3093.76s]  right? But they have to have the good of the good of the project
[3093.76s -> 3098.92s]  and the good of humanity, I guess, in mind, yeah, that's cool.
[3099.44s -> 3102.24s]  But you know, I'm not like the king of this, right?
[3102.28s -> 3104.80s]  Like, I'm not the king of open assist.
[3104.80s -> 3109.52s]  And I don't I don't get if people want to conglomerate
[3109.68s -> 3114.24s]  and talk about the ethics of all of this and and and, you know, ping us with inputs
[3114.24s -> 3116.60s]  like, OK, cool.
[3116.60s -> 3118.84s]  But I mean, you know, when we do talk about some of these risks
[3118.84s -> 3121.76s]  around misinformation and bias, I mean, public accountability,
[3121.76s -> 3128.48s]  public awareness, the ethicists have done stuff like producing model cards and
[3128.48s -> 3132.48s]  you know like making it clear what the data bias is and stuff like that I mean
[3132.48s -> 3153.26s]  do you do you think that's useful? Yes I mean it's what is a model card a model
[3153.26s -> 3158.42s]  card is a readme right and then it has some structure to it right it's it's it
[3158.42s -> 3162.56s]  says, here are the here are the, the things you could describe
[3162.58s -> 3166.14s]  about your model. And here are some examples. I think it's
[3166.14s -> 3171.92s]  useful to have that to have as a norm in the community to say,
[3172.22s -> 3175.38s]  you know, if I publish a model, I sort of I report what it's
[3175.38s -> 3179.74s]  been trained on how it's been trained on. And even to a degree
[3179.74s -> 3184.62s]  like what I think it could do or should be used for, although
[3184.62s -> 3190.34s]  Yeah, if the structure of such a model car gets like too rigid, and it's like, no, we
[3190.34s -> 3197.46s]  must use, we must ask these questions, you get into so many ridiculous situations, like,
[3197.46s -> 3199.54s]  you know, can this be reproduced?
[3199.54s -> 3205.94s]  Well, I just like I, I made SK learn dot linear regression, right?
[3205.94s -> 3207.86s]  Yes, it can be.
[3207.86s -> 3216.14s]  get into situations where the questions don't address what you would actually like to express
[3216.14s -> 3219.14s]  in such a thing, then I think it becomes counterproductive.
[3219.14s -> 3224.62s]  But as a norm to have, hey, look, if you publish something, people should be able to understand
[3224.62s -> 3227.50s]  and potentially reproduce it.
[3227.50s -> 3233.86s]  That standard we have had in papers for a long time, and it's generally been a good
[3233.86s -> 3240.82s]  standard to say, look, if you write, if you publish something, I must be able to, from
[3240.82s -> 3247.86s]  reading it, understand what's in there and to have that as a norm in the community, yeah,
[3247.86s -> 3249.82s]  I'm totally fine with that.
[3249.82s -> 3253.26s]  Do you think there's any relationship with the Chomsky syndrome that we were talking
[3253.26s -> 3260.32s]  about earlier, which is this idea that we should have a very clear model of understanding
[3260.32s -> 3266.96s]  of how these things work in society and we should be able to extrapolate and control things and
[3266.96s -> 3272.40s]  the fear is that basically this is just a complete black box and who knows what's going to happen.
[3276.35s -> 3284.67s]  Nah, I'm good with the black box. It keeps things exciting and interesting and as I said,
[3284.67s -> 3291.55s]  I don't believe this sort of runaway, it might become, you know, very influential and so on
[3291.55s -> 3297.31s]  And certainly that's not very good, but then again, I don't know what to do about it.
[3297.31s -> 3305.31s]  Certainly, if some people sign a change.org moratorium petition is even if it's reached,
[3305.31s -> 3306.31s]  it's not going to help.
[3306.31s -> 3307.31s]  Right?
[3307.31s -> 3308.31s]  What are you going to do?
[3308.31s -> 3311.43s]  It doesn't matter being worried about it.
[3311.43s -> 3312.43s]  We've got a few minutes left.
[3312.43s -> 3314.11s]  So I've got some questions from Jumbotron.
[3314.11s -> 3315.11s]  Say hello, Jumbotron.
[3315.11s -> 3316.11s]  Hello, Jumbotron.
[3316.11s -> 3320.23s]  He's our forum administrator and he's a legend.
[3320.23s -> 3323.51s]  question, do you think all AI research should be open and accessible to the
[3323.51s -> 3330.23s]  public? No, it's totally legitimate that a business does internal research
[3330.23s -> 3337.31s]  like all companies do and that they then use that to make money. That's
[3337.31s -> 3341.91s]  very cool with me. I've never said that shouldn't be the case, only that
[3341.91s -> 3347.91s]  companies shouldn't do that but at the same time claim how open and
[3347.91s -> 3353.35s]  democratizing and beneficial to the common good they are.
[3353.35s -> 3361.99s]  Okay, and you would accept that some research could lead to negative or unethical applications
[3361.99s -> 3365.91s]  and might need to be restricted or yeah?
[3365.91s -> 3372.79s]  Yeah, I totally accept that some research can and will probably lead to overall negative
[3372.79s -> 3380.39s]  effects for for society or for certain individuals within society, right? Like the like, self
[3380.39s -> 3387.03s]  flying drones from any regime in the world, they probably they run, they're on one of
[3387.03s -> 3392.83s]  their on one of their certainly run some open source components as part of their guidance
[3392.83s -> 3397.99s]  system, right? They may be run a Linux kernel, like who knows, but I don't think the Linux
[3397.99s -> 3405.99s]  kernel should not be fully open source and and accessible to everyone. And I don't want anyone
[3406.55s -> 3411.51s]  to be able to be the decider of, you know, the eternal decider of who's good enough
[3412.07s -> 3418.39s]  to use the Linux kernel or not. I'd rather, I think the overall, overall
[3420.31s -> 3427.35s]  welfare of society and humanity is much better served by accepting that some people are going
[3427.35s -> 3437.63s]  going to do some bad things with it and then mitigating that in a different way than appointing
[3437.63s -> 3446.55s]  the king of that model to decide who they deem pure hearted enough to wield it.
[3446.55s -> 3451.07s]  What's next for ML News and your channel and are you making any more music videos with
[3451.07s -> 3452.07s]  AI?
[3452.07s -> 3459.35s]  Well, it's become, there are so many good music videos on AI now, I'm always amazed
[3459.35s -> 3465.15s]  by how talented people are and how quickly they pick up sort of the new stuff and do
[3465.15s -> 3466.15s]  something with it.
[3466.15s -> 3467.15s]  So that's very cool.
[3467.15s -> 3473.43s]  I want, as I said, I've not made too many videos because I've been extremely busy with
[3473.43s -> 3475.11s]  Open Assistant.
[3475.11s -> 3481.09s]  And I think we've also built up sort of a momentum in the direction right now.
[3481.09s -> 3483.87s]  And there are many competent people in our team.
[3483.87s -> 3490.69s]  So I'm also looking to make a couple of more videos, again, paper reviews, news, but also
[3490.69s -> 3497.33s]  a bunch of projects, which I always want to do, but then they take time, of course.
[3497.33s -> 3503.29s]  But I'm very excited about just some of the new stuff that's possible and to try it out
[3503.29s -> 3509.73s]  and to show people a little bit of what one could do and how to have fun with these things.
[3509.73s -> 3510.73s]  Indeed.
[3510.73s -> 3516.09s]  in closing, have you got any shoutouts for people in your life or in Discord who have really helped
[3516.09s -> 3526.33s]  you on the journey? Too many, like way too many to name by name. I could go on
[3526.97s -> 3534.09s]  like eternal lists. In Open Assistance specifically, Andreas Koepp has been extremely influential in
[3534.09s -> 3541.31s]  that he like just organizing things, but also coding things himself, but also like, also
[3541.31s -> 3547.03s]  all the all the other mem, as I said, if I start listing people, I'm going to miss someone.
[3547.03s -> 3548.51s]  And I don't want to do that.
[3548.51s -> 3550.23s]  So I don't want to start listing people.
[3550.23s -> 3553.43s]  But then I think, well, I really want to list the people.
[3553.43s -> 3555.55s]  There's a it's a it's an eternal conundrum.
[3555.55s -> 3562.27s]  So it like to anyone who's ever had any any part of helping me or given, given feedback,
[3562.27s -> 3567.27s]  or even been kind of a dick, I appreciate it.
[3567.85s -> 3572.85s]  And yeah, it's been amazing the amount of help
[3573.83s -> 3578.15s]  and input you get from good-willed people.
[3578.15s -> 3579.63s]  Yeah, communities are amazing.
[3579.63s -> 3582.27s]  So join Yannick's Discord, join our Discord,
[3582.27s -> 3584.21s]  Open Assistant Discord.
[3585.95s -> 3588.49s]  Dr. Kilcher, thank you so much.
[3588.49s -> 3590.47s]  This has been an absolute pleasure, sir.
[3590.47s -> 3591.55s]  Thanks for having me.
========================================
Detected language 'en' with probability 1.000000

run time =115.19582033157349

Completed srt: OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3.srt
Completed: C:/Users/BrodskiTheGreat/Desktop/desktop/Code/scraper-dl-vids/./assets/raw//OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
