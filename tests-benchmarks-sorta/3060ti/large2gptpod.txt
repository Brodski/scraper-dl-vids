OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
C:/Users/BrodskiTheGreat/Desktop/desktop/Code/scraper-dl-vids/./assets/raw//OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
C:/Users/BrodskiTheGreat/Desktop/desktop/Code/scraper-dl-vids/./assets/raw//OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
/root/scraper-dl-vids/audio2Text/assets/raw/OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
/root/scraper-dl-vids/audio2Text/assets/raw/OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
torch.cuda.is_available(): True
start!
Detected language 'en' with probability 1.000000
[0.00s -> 2.00s]  Let's talk about Open Assistant.
[2.00s -> 3.00s]  Sure.
[3.00s -> 5.00s]  So, start from the beginning.
[5.00s -> 14.00s]  Well, we saw that there was a lot of movement in this space.
[14.00s -> 17.00s]  ChatGPT came along and everyone's like,
[17.00s -> 20.00s]  wow, ChatGPT and so on.
[20.00s -> 29.00s]  And yeah, I think it was both a surprise and a not surprise for people.
[29.00s -> 32.96s]  capabilities of chat GPT weren't a surprise, but obviously, the
[32.96s -> 38.30s]  success of it, I think, too many of us was it's like, okay, we
[38.30s -> 43.84s]  knew we could build such no chat conversational like things. But
[43.84s -> 49.12s]  we didn't know how much people loved them. Right. There's a bit
[49.12s -> 53.92s]  of a, I think, wasn't didn't didn't Sam Altman, maybe was
[53.92s -> 58.44s]  that who said in an interview, well, anyone could have built
[58.44s -> 64.84s]  this using our API before, right, but no one did. So you know, we did, which is not true,
[64.84s -> 74.26s]  because they distinctively forbade, like, you building an app that had unfiltered access
[74.26s -> 80.62s]  to the API, essentially, open ended up, I think they've explicitly forbade that, right.
[80.62s -> 85.74s]  So that it's a wrong statement that anyone could have built this using our API, because
[85.74s -> 89.82s]  had they tried, you would have shot them down, right? But in
[89.82s -> 94.54s]  essence, the capabilities were there. And it's still in the
[94.58s -> 97.66s]  restriction now, unless they've changed it, they do not allow
[97.70s -> 100.86s]  open ended applications, which seems like an oxymoron to me,
[100.86s -> 103.18s]  because a language model is inherently open ended.
[103.22s -> 106.22s]  Yeah, but I can I mean, I can see their the restriction being
[106.22s -> 109.14s]  like, you know, you're not allowed to build an app that
[109.14s -> 113.30s]  just lets users freeform query our app, you need to either do
[113.30s -> 117.14s]  some filtering or do some heavy prompting around it so that it's
[117.14s -> 122.58s]  like for one particular purpose. I can see that I can totally get
[122.58s -> 125.58s]  why they do it. But then at the same time saying anyone could
[125.58s -> 129.78s]  have built chat GPT is like no chat GPT is very like if I could
[130.22s -> 134.66s]  imagine an app that has like unfiltered unfeathered access to
[134.66s -> 141.20s]  the API through an app, it's chat GPT. In any case, there was
[141.20s -> 145.40s]  obviously a lot of effort. And then, I think, pretty soon,
[145.40s -> 148.00s]  people came up with this idea, hey, could we do something like
[148.00s -> 151.96s]  that? open source, they had a bit of an older paper called
[151.96s -> 155.84s]  instruct GPT, or that described a model called instruct GPT,
[155.84s -> 162.44s]  that were they sort of outlined how we think chat GPT was done
[162.48s -> 168.36s]  approximately. No one knows but and at that point, we also saw
[168.36s -> 173.52s]  hey, the amount of data to be collected is actually in reach,
[173.64s -> 178.32s]  right? It's not, it's not immense, humongous, and so on.
[178.48s -> 184.24s]  It's actually okay. And and could be done. So at that point,
[184.64s -> 190.12s]  yeah, a lot of people wanted to do something open source like,
[190.20s -> 196.28s]  and I think a bunch of us just came together and, and felt we
[196.28s -> 202.64s]  could do it. So we built this platform where people could come and contribute to the data
[202.64s -> 213.08s]  set, which was really cool to see that people actually came and amazing open source. Well,
[213.08s -> 217.04s]  the point is, the point is there were a lot of ideas around as well of, oh, let's just
[217.04s -> 221.40s]  collect, you know, scrape Quora and scrape Reddit, right? And that will serve like as
[221.40s -> 227.76s]  training data. And it's true to an amount, right? But it's very clear, at least to me,
[227.76s -> 232.56s]  that the capabilities of these models, the chat models, they come from the underlying
[232.56s -> 240.80s]  language model. And the biggest claim to that I have is that OpenAI said they used crowd
[240.80s -> 249.20s]  workers from low wage countries to do their data input. Yet, the first examples of chat
[249.20s -> 253.28s]  GPT that flew around were like, Ooh, look at it solving this quantum physics problem
[253.28s -> 254.28s]  and so on.
[254.28s -> 259.76s]  I'm not saying that there aren't any good quantum physicists in other countries, right?
[259.76s -> 265.28s]  But the people who typically go for like a low wage crowd worker job in these countries,
[265.28s -> 269.36s]  they probably aren't experts in quantum physics.
[269.36s -> 274.76s]  And they also certainly weren't paid to go get a degree in it just so they could answer
[274.76s -> 276.02s]  that one question.
[276.02s -> 282.58s]  So to me, it's very clear that the capabilities come from the underlying model from the next
[282.58s -> 284.66s]  token prediction pre training.
[284.66s -> 290.02s]  And then all the human data does is kind of it gets it into this mood of being an assistant
[290.02s -> 294.90s]  right gives it lots of examples of who here is how it's like, you know, going through
[294.90s -> 298.74s]  an apprenticeship or something like this, where you've lived your life, right, you've
[298.74s -> 301.56s]  grown up, you've consumed the world and so on.
[301.56s -> 305.96s]  And then you start your first job and someone tells you, look, here is how you behave towards
[305.96s -> 310.56s]  customers, right? You're, you're friendly, you know, if someone asks this, you do it
[310.56s -> 315.44s]  like this, here is a thing, here is how our system works. And so you get introduced to
[315.44s -> 322.84s]  that. But your competence of just living and doing things comes from yourself for your
[322.84s -> 328.54s]  life, and not from that one person who introduces you to how the store works and how you should
[328.54s -> 335.94s]  behave towards customers. So my big conviction was always we should really collect this
[335.94s -> 339.46s]  data from humans and the goal should really be diversity.
[339.46s -> 349.38s]  So the goal, and if you just say, well, we'll just scrape, you know, 10,000 of this, then
[349.38s -> 351.90s]  to me that certainly is going to do something, right?
[351.90s -> 355.50s]  It's good data, probably, but it's a bit missing the point.
[355.50s -> 362.62s]  If you want a general assistant, you need as general data as you can get and only human
[362.62s -> 369.98s]  data so far, has been able to achieve that level of diversity and generality.
[369.98s -> 374.96s]  And it was proven, like, okay, I'm biased, but I think it was proven right a little bit
[374.96s -> 380.94s]  in that if you look at the data set, the prompts that human right, like what they want to know
[380.94s -> 386.36s]  what they want the model to do, it's so diverse, it's insane.
[386.36s -> 391.02s]  And so we built this platform, where essentially you as a human, you can come and you're always
[391.02s -> 395.90s]  presented with like one task, and the task could be, write the prompt, right?
[395.90s -> 400.38s]  But the task could also be here is an already existing conversation between a human and
[400.38s -> 401.46s]  an assistant.
[401.46s -> 403.72s]  And now it's the assistant's turn.
[403.72s -> 407.16s]  And now you play the assistant, please write your response, right?
[407.16s -> 411.22s]  It could also be here is a conversation.
[411.22s -> 413.38s]  Here's the last message of the conversation.
[413.38s -> 417.64s]  It's the reply by the assistant, please rate it, like label it.
[417.64s -> 418.92s]  Is it is it spam?
[418.92s -> 419.92s]  Is it a troll?
[419.92s -> 420.92s]  Right?
[420.92s -> 426.52s]  Is it funny? Is it appropriate? Does it fulfill what the human wanted out of it?
[426.52s -> 432.60s]  Right. And so that's how we constructed the data set. And we collected over like 600,000
[432.60s -> 440.20s]  inputs of such that being text or labels or rankings of different things. And yeah, that's
[440.84s -> 446.60s]  that resulted in in this data set over 13,000 people contributed to the data set, which is
[446.60s -> 451.04s]  which is mind-blowing to see and it's really cool.
[451.04s -> 454.92s]  And we've just made the dataset fully available.
[454.92s -> 457.72s]  You can go and look at it and download it.
[457.72s -> 460.64s]  There are a lot of, so we have about 10,000
[460.64s -> 465.28s]  what we call fully annotated conversation trees, which
[465.28s -> 469.36s]  is like a root node, the prompt, and different answers from it.
[469.36s -> 472.80s]  And then from those, sometimes different answers and so on.
[472.80s -> 474.60s]  We have sampled.
[474.60s -> 476.64s]  We've set our parameters in various ways.
[476.64s -> 479.96s]  So sometimes it's short trees, sometimes it's big trees,
[479.96s -> 482.56s]  and sometimes it's wide trees.
[482.56s -> 486.04s]  And so you can go look at all of that.
[486.04s -> 490.46s]  We have over 10,000 of those trees, which is really cool,
[490.46s -> 492.16s]  because you can see the same conversation,
[492.16s -> 495.08s]  like taking a bit alternate turns and so on.
[495.08s -> 498.44s]  And we have tons and tons of prompts.
[498.44s -> 505.44s]  we have probably like 50,000 or so, or 20,000 at least,
[505.44s -> 509.24s]  like just prompts, like people who come and want to know.
[509.24s -> 511.28s]  We got so much prompts we had to implement,
[511.28s -> 517.44s]  like Andreas has been very influential in this project.
[517.44s -> 523.24s]  And he had to implement this prompt lottery, where really,
[523.24s -> 525.60s]  we first, if people enter a prompt,
[525.60s -> 527.64s]  it first goes into this lottery thing, right?
[527.64s -> 529.00s]  And then we sample from that.
[529.00s -> 532.72s]  And I think we adjust it so that one person can't,
[532.72s -> 536.28s]  like if one person puts a lot of prompts,
[536.28s -> 539.56s]  it's like sampled less so that every person has kind of like
[539.56s -> 542.80s]  the same or similar chance of getting their prompt
[542.80s -> 544.94s]  into the system, right?
[544.94s -> 547.96s]  Because one prompt then generates, you know,
[547.96s -> 551.44s]  probably a hundred tasks because it's all the responses
[551.44s -> 553.84s]  and the responses to the responses and the rankings
[553.84s -> 554.96s]  and the labels.
[554.96s -> 556.28s]  And yeah, it's been fun.
[556.28s -> 561.16s]  It's been absolute fun and a pleasure to work with the people,
[561.16s -> 563.08s]  also the people who've contributed code.
[563.08s -> 563.92s]  It's amazing.
[563.92s -> 567.24s]  People just, they come and they ask for nothing, right?
[567.24s -> 568.88s]  They're just like, oh, this is cool.
[568.88s -> 570.12s]  I wanna be part of this.
[570.12s -> 573.36s]  And they see that everyone else excited too.
[573.36s -> 575.08s]  And then they contribute code.
[575.08s -> 577.56s]  Some contribute like lots and lots of code,
[577.56s -> 578.70s]  which is amazing.
[578.70s -> 580.44s]  Some just come and they contribute.
[580.44s -> 583.32s]  You know, there's like, here's an issue, I'll do it.
[583.32s -> 584.68s]  And that's cool too.
[584.68s -> 586.24s]  So yeah, it's been cool.
[586.24s -> 588.80s]  Well, first of all, thank you for doing this.
[588.80s -> 590.44s]  It's absolutely amazing.
[590.44s -> 591.40s]  Well, thank the people.
[591.40s -> 594.08s]  Like, I've just been the noise machine, right?
[594.08s -> 595.28s]  Oh, I know.
[595.28s -> 599.00s]  I know, but I mean, when you published all of that information
[599.00s -> 600.60s]  on your YouTube channel that you were working on it,
[600.60s -> 602.20s]  I'm sure everyone jumped on it.
[602.20s -> 604.16s]  But I do have a few questions.
[605.10s -> 608.28s]  The reason why it's so exciting is just like Conor did,
[608.28s -> 609.68s]  we used to be friends of Conor Leahy.
[609.68s -> 612.48s]  We were chatting with him years ago, and he just set up...
[612.48s -> 614.64s]  We're still friends. Oh, we were still, yeah.
[614.64s -> 616.04s]  We used to be friends.
[616.04s -> 622.68s]  I mean, he's a busy guy now, but we were chatting all the time and he just kind of set up Aloytha
[622.68s -> 626.44s]  AI and just got, I think Google on board.
[626.44s -> 629.64s]  And he just said, you know what, I'm going to build the pile and I'm just going to build
[629.64s -> 632.52s]  this massive data set and I'm just going to train this massive language model.
[632.52s -> 634.84s]  And he's just like a random guy.
[634.84s -> 635.84s]  And like-
[635.84s -> 636.84s]  He wasn't him alone though.
[636.84s -> 637.84s]  He wasn't alone.
[637.84s -> 638.84s]  There was a whole team.
[638.84s -> 639.84s]  Yeah, yeah.
[639.84s -> 640.84s]  But no, but it was-
[640.84s -> 641.84s]  The excitement it creates.
[641.84s -> 642.84s]  It was visceral.
[642.84s -> 647.56s]  It was so exciting and they pulled it off against all the odds and then you've done
[647.56s -> 653.64s]  exactly the same thing, which is remarkable, but I have a few questions, which is that
[653.64s -> 656.78s]  most of these other language models are not very good.
[656.78s -> 661.30s]  So Nat Friedman's got like a dev website where you can play with all of the language models
[661.30s -> 663.84s]  and most of them aren't very good.
[663.84s -> 666.30s]  Even the ones that should be good aren't very good.
[666.30s -> 671.88s]  And what people might find surprising is that you can take a model and let's say you're
[671.88s -> 676.72s]  using the Lama model from Meta and it's a foundation model that has all of the capabilities
[676.72s -> 679.46s]  and it's been trained on, because you said diversity is important.
[679.46s -> 683.50s]  It's got diversity, it's been trained on everything, but it's not very good.
[683.50s -> 687.52s]  And then you do this fine tuning and I think people need to be clear that what you're doing
[687.52s -> 691.12s]  is not RLHF, it's fine tuning with human...
[691.12s -> 696.22s]  So the model we have on the website as of time of this recording is one that's just
[696.22s -> 698.64s]  fine tuned on the human data.
[698.64s -> 706.48s]  doing the RLHF as well. So all of this is happening like in parallel. It's just already
[706.48s -> 712.20s]  these fine-tuned models, they're performing quite well, I think. And thus, we just wanted
[712.20s -> 718.24s]  to get them out right before we were like all done. And yeah, but people are now free
[718.24s -> 723.68s]  to take the dataset and do their own reinforcement learning and whatnot. And we're happy to
[723.68s -> 726.32s]  take back these models if they turn out to be good.
[726.32s -> 731.52s]  Yeah, I think people might be surprised by that because you've taken a model, which probably
[731.52s -> 737.12s]  wasn't very good. And you fine tuned it with this diverse human created data. Now,
[738.96s -> 746.00s]  it was not very good at being like an assistant. So as I said, the capabilities that we unlock,
[746.00s -> 750.88s]  quote unquote, right, they were in there all along. And it's still not very good.
[750.88s -> 755.00s]  even with our fine tuning for certain tasks,
[756.32s -> 760.00s]  some of which is clearly the, well, the fault,
[760.00s -> 761.66s]  some of which can clearly be traced
[761.66s -> 762.76s]  to the underlying model.
[762.76s -> 764.96s]  For example, the underlying model,
[764.96s -> 766.80s]  if it's, for example, the Lama model,
[766.80s -> 768.96s]  it's 30 billion parameters,
[768.96s -> 771.96s]  it's not GPT-3 size,
[771.96s -> 775.80s]  even like it's 10 times smaller probably than GPT-4,
[775.80s -> 777.52s]  however big that is, right?
[777.52s -> 781.40s]  So it's gonna have, it's not gonna be the same.
[781.40s -> 786.40s]  Like it's, we don't wanna claim it's like as good as they.
[788.76s -> 792.48s]  Same, it's probably been trained on much less code,
[792.48s -> 797.00s]  for example, than the GPT models of open AI.
[797.00s -> 801.32s]  And thus we see that coding, for example,
[801.32s -> 803.36s]  is a weakness of the model.
[803.36s -> 805.86s]  And there, although people tell me
[805.86s -> 808.26s]  with like lower temperature, it's actually pretty good.
[808.26s -> 811.54s]  I have not explored that yet, but it's,
[811.54s -> 812.86s]  so the underlying model,
[812.86s -> 816.00s]  I think LLAMA is a pretty good model, right?
[816.00s -> 819.54s]  It's not been super good at being an assistant
[819.54s -> 822.54s]  out of the box, but it's quite a good model.
[822.54s -> 827.54s]  And as I said, all we do is we kind of unlock that
[827.54s -> 829.14s]  and bring it to the surface.
[829.14s -> 830.70s]  Well, that's kind of what I want to get to,
[830.70s -> 834.06s]  that people like Conor Leahy,
[834.06s -> 837.42s]  He galaxy brained himself and he knew that GPT
[837.50s -> 838.76s]  three was a good model.
[839.12s -> 841.50s]  And, and I was saying, no, it's not kind of what
[841.50s -> 842.06s]  you're talking about.
[842.54s -> 845.98s]  And it's almost like what you're doing with this
[845.98s -> 847.82s]  fine tuning, you're not really adding any
[847.82s -> 849.46s]  capability or just getting it in the mood.
[849.46s -> 851.90s]  The capability is already there, but it gets into
[851.90s -> 854.10s]  the philosophy of what do we recognize as
[854.10s -> 856.26s]  intelligence and that's relevant to the previous
[856.30s -> 857.66s]  conversation we were having.
[858.14s -> 860.76s]  So when the average person plays with Lama, they
[860.76s -> 862.54s]  probably won't find it as useful.
[862.54s -> 865.50s]  they might not recognize it as intelligent.
[865.50s -> 867.06s]  You create all of this training data.
[867.06s -> 868.42s]  Now I want to touch on the process
[868.42s -> 869.54s]  of creating the training data,
[869.54s -> 871.62s]  because I think it's really important.
[871.62s -> 873.50s]  What you're doing is you're creating
[873.50s -> 875.46s]  counterfactual trajectories.
[875.46s -> 877.26s]  It's very similar to Kenneth Stanley's
[877.26s -> 878.98s]  pick breeder algorithm, if you remember that.
[878.98s -> 881.62s]  So it's actually an open-ended process.
[881.62s -> 884.22s]  In a way, like we stop, so as I said,
[884.22s -> 885.86s]  it starts with the prompt, right?
[885.86s -> 889.02s]  We sample that, and then we ask like
[889.02s -> 896.38s]  three humans to each create a continuation alternate as like an alternate path in the
[896.38s -> 897.38s]  conversation.
[897.38s -> 903.38s]  And then to those we again, ask two or three humans to, hey, because the prompt is from
[903.38s -> 907.18s]  what we call the prompter role, that would be like the human interacting.
[907.18s -> 911.70s]  And then the assistant is the counterparty in our system.
[911.70s -> 916.98s]  All of this is play is done by humans, like we we have to distinguish the words a bit
[916.98s -> 922.04s]  Like user is really the human and then prompter is the role in the conversation.
[922.04s -> 924.60s]  And then assistant is the other role, right?
[924.60s -> 930.82s]  In our system, in our data collection system, this is all done by humans for data collection
[930.82s -> 932.14s]  purposes.
[932.14s -> 940.96s]  And yeah, so we create these three of conversations where, yeah, you have three assistant replies
[940.96s -> 943.22s]  to the first prompt, let's say.
[943.22s -> 947.90s]  And to each of these assistant reply, you have three prompter replies.
[947.90s -> 952.14s]  And the prompter replies could be something like, oh, you got that wrong, or could you
[952.14s -> 957.78s]  clarify something or, you know, please do it in a different way or elaborate on something
[957.78s -> 958.78s]  you said.
[958.78s -> 963.58s]  And then to each of those, we again have an assistant reply and we modify a bit like the
[963.58s -> 966.38s]  width and sampling of all of that.
[966.38s -> 969.82s]  But at some point, we cut it off and we say, okay, the tree is done now.
[969.82s -> 973.44s]  It has like, I don't know, 50 or 100 messages inside of it.
[973.44s -> 976.34s]  So you package that, boom, next prompt.
[976.34s -> 983.84s]  It's not open-ended in the way that like Stanley's open-ended experiments are in the sense that
[983.84s -> 988.28s]  we do cut it off after some steps and then we take the next prompt because otherwise
[988.28s -> 992.68s]  we just have one big conversation, which would maybe be fun too, right?
[992.68s -> 995.64s]  To just have, because conversation meanders, right?
[995.64s -> 999.08s]  And we just have like one big conversation.
[999.08s -> 1001.94s]  At any point, you could say like, I changed my mind.
[1001.94s -> 1002.94s]  Let's do something else.
[1002.94s -> 1008.78s]  I mean, I think what I was trying to capture that Stanley is big on people following the
[1008.78s -> 1012.46s]  gradient of interestingness and that's kind of what you've captured.
[1012.46s -> 1017.14s]  So they meander, they take trajectories and then the model learns an interesting manifold
[1017.14s -> 1018.84s]  and we'll get into simulators.
[1018.84s -> 1020.06s]  Maybe you were just talking about that.
[1020.06s -> 1021.34s]  We've just done a show on simulators.
[1021.34s -> 1025.74s]  It's a very interesting idea that language models basically have a superposition of
[1025.74s -> 1031.34s]  agents and you can kind of get them in the mood to behave like a certain agent.
[1031.34s -> 1035.80s]  And in a sense what you've done is through all of these counterfactual, creative, interesting
[1035.80s -> 1039.98s]  trajectories of conversations, you're kind of like fitting it to some structure which
[1039.98s -> 1043.18s]  fits really nicely to humans.
[1043.18s -> 1044.18s]  I guess.
[1044.18s -> 1051.82s]  I mean it obviously covers in like three answers to some text covers in no way the extent of
[1051.82s -> 1057.34s]  what humans would do, but it just creates like a little bit of different training data for one.
[1057.34s -> 1063.34s]  It creates because we also rank the different thing. We ask humans which one of these is best.
[1063.34s -> 1068.30s]  It also creates a bit of a signal for quality, right? Again, with the labels that we have and
[1068.30s -> 1075.42s]  a bit of diversity like, okay, here is three ways you could respond to that particular thing.
[1075.42s -> 1083.66s]  Right. So, yeah, I think it's been a worthwhile effort to do this instead of just collecting
[1083.66s -> 1090.74s]  like single conversations. Obviously, exponentially multiplies the effort humans have to put in,
[1090.74s -> 1096.12s]  but I think it was, obviously, I don't have, interestingly, I don't have the counterfactual
[1096.12s -> 1101.58s]  in the world where we just would have collected conversations, but I think it's been worth
[1101.58s -> 1104.10s]  it and it's turned out well.
[1104.10s -> 1109.22s]  Amazing. Well, quick digression on the Waluigi effect. I know you've got an interesting take
[1109.22s -> 1114.98s]  on this, so we did a video on it, but the quick idea is that, do you remember Bing?
[1114.98s -> 1121.62s]  It would digress to an angst teenage child within about three messages, and less wrong,
[1121.62s -> 1124.74s]  it wasn't actually less wrong, I think it's the alignment forum, but I just kind of mentally
[1124.74s -> 1129.70s]  bucket them all in the same place, but they said that it's because you get these antithetical
[1129.70s -> 1135.34s]  agents, so still simulated theory and because of structural narratology, in all the data
[1135.34s -> 1140.18s]  they're trained on, you tend to have agents that are, you know, you have the antithesis
[1140.18s -> 1145.64s]  of the agent in the same story and they say that the embedding space between the agent
[1145.64s -> 1149.92s]  and the antithesis is so close together, just like in Word2Vec, stop and go are very close
[1149.92s -> 1155.56s]  together and the RLHF kind of, you know, clusters them and it doesn't filter out the Waluigis.
[1155.56s -> 1156.56s]  What do you think about that?
[1156.56s -> 1159.76s]  That's a bunch of rubbish.
[1159.76s -> 1160.76s]  I'm in Britain now.
[1160.76s -> 1161.76s]  I should start talking like you.
[1161.76s -> 1165.76s]  That's a load of bollocks, mate.
[1165.76s -> 1166.76s]  No, I think that's...
[1166.76s -> 1169.00s]  I said this to you before.
[1169.00s -> 1177.00s]  I think that's when you have someone who is educated and good with words, but you just
[1177.00s -> 1179.74s]  tell them, like, just ramble a bit.
[1179.74s -> 1180.74s]  That's what you get out.
[1180.74s -> 1184.08s]  You get out posts or...
[1184.08s -> 1189.76s]  I'm not saying this doesn't obviously have a claim to it and could be tested and all
[1189.76s -> 1190.94s]  of this kind of stuff.
[1190.94s -> 1194.36s]  And I don't have evidence for the fact that it's not true.
[1194.36s -> 1197.08s]  I just don't think it is, right?
[1197.08s -> 1204.08s]  Maybe that's a rambling claim too or a bit of, but I don't, it's a very specific claim
[1204.08s -> 1207.94s]  and that specific claim would have to have good evidence behind it.
[1207.94s -> 1214.96s]  And I think there is a much less specific, like, there's a much more obvious reason to
[1214.96s -> 1217.76s]  why these models degrade.
[1217.76s -> 1220.92s]  And that thing goes like, no, you've been a bad user, and so on.
[1220.92s -> 1227.52s]  And that's, that's just, I, and you can compare it to yourself or to an assistant before,
[1227.52s -> 1233.88s]  like we talked about apprenticeship, you, it's the this tuning is like a bit of an apprenticeship,
[1233.88s -> 1240.72s]  You come out of school, you go into a store, you get employed there and the manager tells
[1240.72s -> 1243.76s]  you here is how we treat customers.
[1243.76s -> 1248.20s]  We were always respectful even if they're a little rude.
[1248.20s -> 1250.28s]  You remain respectful.
[1250.28s -> 1257.80s]  At some point if it gets too rude, you just say, I'm sorry, I can't do that.
[1257.80s -> 1261.28s]  Never insult the customer, never do that.
[1261.28s -> 1265.64s]  If you go to a store now, you can be quite a bit of a, and I'm not saying I've tried
[1265.64s -> 1271.20s]  this right, but you can probably be quite a bit of a dick for a while.
[1271.20s -> 1273.72s]  But eventually you'll get under their skin.
[1273.72s -> 1281.00s]  Eventually, you'll say something about their mother or about their appearance or about
[1281.00s -> 1285.84s]  their intelligence or something that gets that gets them right.
[1285.84s -> 1291.64s]  And at that point, you will not have a friendly customer, a support person there, you will
[1291.64s -> 1298.96s]  have like some person that's going like, you know, you like he's like, and then it's becomes
[1298.96s -> 1300.48s]  ugly.
[1300.48s -> 1304.40s]  And this is this is inside of, of humans.
[1304.40s -> 1312.24s]  And it's, in my fact, an inextricably inextricably linked to being a competent being in in the
[1312.24s -> 1313.24s]  world.
[1313.24s -> 1314.24s]  Right?
[1314.24s -> 1319.24s]  If you don't know what anger is, you're not competent, right?
[1319.24s -> 1327.24s]  Even if you yourself never express anger, let's say, in a raging way,
[1327.24s -> 1329.24s]  you still know what it is.
[1329.24s -> 1333.24s]  And if I asked you to act like it, you could still do it.
[1333.24s -> 1337.24s]  And if I insult you in the correct way, you probably would do it.
[1337.24s -> 1341.24s]  And so I think it's much more that.
[1341.24s -> 1348.18s]  that it's, it is a way that humans have in them, they can behave like this, it's totally
[1348.18s -> 1350.26s]  normal if you poke them enough.
[1350.26s -> 1353.12s]  And that's what the statistical model represents.
[1353.12s -> 1359.78s]  It's just, it's very likely that if you go and you poke the human and equivalently the
[1359.78s -> 1362.26s]  statistical distribution of humans, right?
[1362.26s -> 1366.74s]  If you poke them enough and you insult them enough, they will come back and be like, no,
[1366.74s -> 1368.38s]  F off, right?
[1368.38s -> 1371.10s]  You're a dumb user, no.
[1371.10s -> 1375.86s]  And I don't know, it's not, it's not, ooh, it's very close in embedding space.
[1375.86s -> 1381.66s]  It's like, no, this is what happens when you go to humans and poke them and insult them.
[1381.66s -> 1386.62s]  And ergo, if you do the same with these models, they will react in the statistically likely
[1386.62s -> 1388.74s]  way of treating human.
[1388.74s -> 1394.78s]  And yes, on top of that, there are like adversarial, sorry, that was embarrassing.
[1394.78s -> 1400.66s]  There were like adversarial examples where, okay, maybe you say the exact number of words
[1400.66s -> 1404.86s]  so that the matrices line up and the singular value pops off
[1404.86s -> 1407.54s]  and it goes really much into this direction, right?
[1407.54s -> 1410.22s]  And then you get the weird answer, right?
[1410.22s -> 1414.98s]  Like a mathematical happening, right?
[1414.98s -> 1418.98s]  But in essence, in essence, it's just,
[1418.98s -> 1420.78s]  yo, that's the data, right?
[1420.78s -> 1422.90s]  It's not the Waluigi.
[1422.90s -> 1424.74s]  But what's really interesting,
[1424.74s -> 1427.30s]  and I buy into everything you just said,
[1427.30s -> 1430.54s]  is that all of that chaos, you know, the Shoggoth meme,
[1430.54s -> 1433.74s]  all of the beast, that's actually necessary,
[1433.74s -> 1436.78s]  because we have this puritanical view of language models.
[1436.78s -> 1439.46s]  People like Gary Marcus, they would say,
[1439.46s -> 1441.26s]  all of that crap should be cut out,
[1441.26s -> 1443.70s]  all of the racism, all of the bias.
[1443.70s -> 1446.14s]  And even if they have been trained
[1446.14s -> 1447.74s]  on the corpus of the internet,
[1447.74s -> 1451.34s]  they may well pick up on a very human behavior,
[1451.34s -> 1454.50s]  which is that our affect changes dramatically over time.
[1454.50s -> 1455.54s]  Yeah, but do you want that?
[1455.54s -> 1459.74s]  Like, obviously all of us would be
[1459.74s -> 1465.18s]  totally in favor if you come and you say, look, I have a competent assistant that doesn't,
[1465.18s -> 1470.70s]  you know, I guarantee you there is not an ounce of, you know, swear word in that thing,
[1470.70s -> 1478.42s]  right? Do you want the way to like, do you want an assistant? Like, let's think about
[1478.42s -> 1483.02s]  a human assistant. Like you're, you're fortunate enough to be able to hire like a personal
[1483.02s -> 1491.98s]  assistant. Some people have that luxury, right? And do you want one that says, oh no, whenever
[1491.98s -> 1496.78s]  there is a scene in a movie where people like get a bit rough to get a bit angry at each
[1496.78s -> 1502.50s]  other, I just go like this, right? I just plug my ears and I go like, la, la, la, la.
[1502.50s -> 1508.02s]  In fact, I don't know what happens after. And I don't want to know, right? This is not
[1508.02s -> 1513.26s]  in my knowledge, this is not in my training distribution, whatever happens, if humans
[1513.26s -> 1518.22s]  get a bit angry at each other and beyond, I don't know, right?
[1518.22s -> 1524.36s]  If you want a person like this, or do you want a person who's just grown up normally
[1524.36s -> 1531.14s]  and just has been socialized well to not do that, like to not get angry, even though they
[1531.14s -> 1537.86s]  could with the knowledge that yes, if you insult them enough, they will get angry, right?
[1537.86s -> 1541.86s]  Which one do you want? To me, I want the competent one.
[1541.86s -> 1544.02s]  I want the one who knows what anger is.
[1544.02s -> 1550.46s]  I want the one who knows that something like racism exists
[1550.46s -> 1556.02s]  and who is aware that it's a thing to be combated,
[1556.02s -> 1559.70s]  to be aware of people like this exist.
[1559.70s -> 1561.74s]  Here is how they think, right?
[1561.74s -> 1566.82s]  Here is maybe why they think what they think, where they're wrong, right?
[1566.82s -> 1572.70s]  in order to be competent, to be able to battle it, in order to be competent to be able to
[1572.70s -> 1574.22s]  avoid it.
[1574.22s -> 1583.22s]  And so I think these things are a necessary component of competence, not that I would
[1583.22s -> 1584.62s]  want them in there.
[1584.62s -> 1591.90s]  But I think you cannot be competent in the world not having knowledge and ability to
[1591.90s -> 1592.90s]  do these things.
[1592.90s -> 1593.90s]  Yeah, exactly.
[1593.90s -> 1600.68s]  And a lot of this is about the sounds that are not made or are not observable.
[1600.68s -> 1606.02s]  So when you work any job, there's your public behavior, and then there's what you're really
[1606.02s -> 1609.30s]  thinking and what you say in private behind the scenes.
[1609.30s -> 1614.58s]  And your ability to be competent and understand what's going on in the ecosystem of that business
[1614.58s -> 1616.42s]  is driven by the shogger.
[1616.42s -> 1620.78s]  There's all of this stuff going on inside you, that two sides of the same coin.
[1620.78s -> 1624.00s]  And also it's about what makes you human.
[1624.00s -> 1627.76s]  And maybe it's a reason why there will be no superintelligence because these things
[1627.76s -> 1633.98s]  are scarily good at being human but they in many ways have the flaws of being human.
[1633.98s -> 1640.46s]  Eventually they'll just want to chill on the beach and smoke a joint and relax and be like
[1640.46s -> 1646.00s]  nah all this work no they're too human.
[1646.00s -> 1647.96s]  But I think so too, right?
[1647.96s -> 1655.74s]  And I think if you're not competent like that, if you don't have the inner monologue that
[1655.74s -> 1663.88s]  tells you, hey, that other human, I think they kind of want to screw me over because
[1663.88s -> 1668.32s]  I'm going to get a promotion soon and they're trying to do things.
[1668.32s -> 1673.56s]  If you're not able to model that inside of yourself, you're going to...
[1673.56s -> 1675.98s]  I'm not saying everyone else is evil, right?
[1675.98s -> 1683.26s]  are tremendously nice humans and all but I think we've all been served well by considering
[1683.26s -> 1690.02s]  hey, other humans might not be the nicest people and here is how they might think internally.
[1690.02s -> 1695.66s]  So having that competence, if you don't have that you're just naive and you just you're
[1695.66s -> 1702.42s]  going to go down right and you're not going to achieve anything productive or much productive
[1702.42s -> 1710.42s]  because you need to be able to be prepared for someone else being adversarial.
[1710.42s -> 1713.50s]  So language models do have a theory of mind?
[1713.50s -> 1718.38s]  Well again that's like a word, right, that we've ascribed to, I mean essentially all
[1718.38s -> 1723.70s]  of this wordplay comes down to, well if I have a concept X, right, and I assign X to
[1723.70s -> 1731.22s]  a human and if I have a thing that just acts in exactly the same way as a human who has
[1731.22s -> 1736.98s]  as x, do I now apply x to thing? It's a matter of definition,
[1737.14s -> 1742.38s]  right? Certainly the the models can or maybe better versions
[1742.38s -> 1746.30s]  more and more will be able to act as if they had a theory of
[1746.30s -> 1750.90s]  mind. Do you now apply the word or not? Who cares? It's a matter
[1750.90s -> 1751.58s]  of definition.
[1752.54s -> 1755.62s]  So coming back to open assistant, tell me about the
[1755.62s -> 1765.78s]  the legals first of all. So you are presumably storing the data that people do inferencing with
[1765.78s -> 1771.22s]  and you're publishing it and obviously that's made very very clear and the whole thing is done
[1771.22s -> 1775.46s]  in the open and eventually people might be able to host their own versions of it but perhaps you
[1775.46s -> 1781.86s]  can just kind of like sketch out all of the privacy stuff. Yeah so we obviously have the
[1781.86s -> 1787.86s]  data collection platform. And so all our platform is governed by terms of service, where we say,
[1787.86s -> 1795.50s]  look, you input data, we use it for training AI models. And and everyone who comes to our
[1795.50s -> 1802.14s]  website is is aware of that. And you know, you can you can read it. So and I think people
[1802.14s -> 1806.82s]  come because of that, right? People come call it correct contribute to our data collection
[1806.82s -> 1812.82s]  to our data set, because they want to it's work, right? It's, it's work to play the assistant
[1812.82s -> 1818.62s]  to go and research like, can can zebras be domesticated? Right? You're like, who knows?
[1818.62s -> 1823.30s]  Now you need to go to Wikipedia, and you need to go to research, and you need to read different
[1823.30s -> 1827.94s]  accounts of things, right? And you'd be like, okay, at the end, I have an opinion. And I
[1827.94s -> 1834.18s]  put that into its work. And people come, well, first of all, it's a bit fun, right? Did
[1834.18s -> 1839.92s]  know whether zebras could be domesticated? I didn't before. Okay, they're notoriously
[1839.92s -> 1847.84s]  difficult to be domesticated. But it's work and people come with the intent of contributing
[1847.84s -> 1854.68s]  to the data set, obviously, for the chat portion, now that we say, you know, come try our models,
[1854.68s -> 1861.40s]  that's governed by the same terms of service. But we think that people might not be that,
[1861.40s -> 1868.58s]  know, aware and willing. So we're obviously gonna, this is, it's all it's all volunteer
[1868.58s -> 1874.70s]  work, right. And we're doing this all in our free time, and so on. So that we were going
[1874.70s -> 1879.68s]  to make more clear, we're going to make the ability to potentially like opt out, you can
[1879.68s -> 1885.34s]  say that this chat, I don't want that this chat is being used to infer their data sets
[1885.34s -> 1891.32s]  or to train models, like, or the ability to completely delete chats. For now, we just
[1891.32s -> 1895.84s]  have like a hide button. Actually, we don't we don't have a button to show or to show
[1895.84s -> 1900.44s]  all like, like, it's all there. Okay, but we need to implement it. We don't want to
[1900.44s -> 1906.00s]  like, we just we put the hide button because some people said, Well, I have so many chats,
[1906.00s -> 1911.72s]  my thing becomes unusable. Right? Because we just list them all. Website becomes and
[1911.72s -> 1913.60s]  So we're like, ah, okay.
[1915.08s -> 1917.52s]  So our intention is not to be like,
[1917.52s -> 1919.48s]  ha ha, we now have your data and so on.
[1919.48s -> 1924.48s]  Our intention is always to, okay, this is a thing.
[1924.72s -> 1927.76s]  You can come, you can contribute to our data collection.
[1927.76s -> 1929.56s]  When you interact with the chat, right?
[1929.56s -> 1931.84s]  I've also clearly said this in my video,
[1933.28s -> 1935.66s]  if you find something particularly good, use thumbs up.
[1935.66s -> 1937.28s]  If you find something particularly bad,
[1937.28s -> 1938.60s]  you don't have to label every message,
[1938.60s -> 1941.16s]  but if you think that's really good, that's really bad,
[1941.16s -> 1947.04s]  use the thumbs. And so it's very clear, I think, to most people
[1947.04s -> 1950.48s]  that, again, this is, is part of data collection, but we
[1950.48s -> 1955.80s]  definitely want to make it easier to like opt out and to
[1955.80s -> 1959.00s]  be like, no, that being said, whenever you put your data
[1959.00s -> 1962.44s]  anywhere, you should be aware that that place is going to
[1962.44s -> 1967.12s]  store it and is probably going to train models on it. Yeah, so
[1967.12s -> 1970.96s]  I think we're just being more transparent about that.
[1970.96s -> 1973.12s]  Yes, because with OpenAI at the moment,
[1973.12s -> 1976.72s]  ChatGBT, they store everything and use it to fine-tune.
[1976.72s -> 1980.32s]  If you use the API, they store it, but they don't use it to fine-tune.
[1980.32s -> 1982.56s]  And just to be clear, with your system at the moment,
[1982.56s -> 1987.12s]  no one should put any confidential or PII data into the system.
[1987.12s -> 1991.52s]  No, no, no. That's been always the case.
[1991.52s -> 1994.96s]  Yeah, so...
[1994.96s -> 1997.88s]  And with us, you can see all the things we're doing, right?
[1997.88s -> 2001.16s]  You can just go into GitHub, look at the code and see it.
[2001.16s -> 2004.40s]  And if you don't want that, you can make your own.
[2004.40s -> 2007.64s]  As I said, you need like fat hardware right now,
[2007.64s -> 2011.24s]  although I also think people might bring that down
[2011.24s -> 2013.16s]  as they did with stable diffusion, right?
[2013.16s -> 2016.72s]  Or with Llama itself, which now runs on a toaster.
[2016.72s -> 2020.20s]  But with us, you can just, like what you see on GitHub
[2020.20s -> 2022.80s]  is what runs in production.
[2022.80s -> 2025.72s]  and you can actually see the prod branch.
[2025.72s -> 2028.08s]  So that's it, yeah.
[2028.08s -> 2029.44s]  And this is amazing for me
[2029.44s -> 2032.12s]  because I'm running a startup called X-Ray
[2032.12s -> 2034.32s]  and we're using GPT at the moment.
[2034.32s -> 2038.32s]  And it frankly horrifies me sending up,
[2038.32s -> 2040.04s]  I mean, obviously the customers opt in to do it,
[2040.04s -> 2042.68s]  but basically we're sending their conversations up to GPT
[2042.68s -> 2043.76s]  and they've summarized them
[2043.76s -> 2045.48s]  and we do a bunch of stuff with it.
[2046.36s -> 2048.40s]  But yeah, I don't wanna do that.
[2048.40s -> 2052.00s]  I'd much rather send it to a self-hosted open assistant.
[2052.00s -> 2054.52s]  then we know, you know, it's on our hardware, we know where the
[2054.52s -> 2059.08s]  data is going. Our policy is to not store anything at any time.
[2059.12s -> 2063.12s]  Yeah. And I can't do that at the moment. Yeah, please help me do
[2063.12s -> 2063.48s]  that. Yeah.
[2064.16s -> 2068.60s]  That being said, like, let me add to that before I like, I
[2068.60s -> 2073.48s]  think we shouldn't and wouldn't unless someone someone leaks
[2073.48s -> 2077.28s]  something. It's also open source is a conglomeration of people,
[2077.28s -> 2080.44s]  but I want to like build in the the option to like do the opt
[2080.44s -> 2087.40s]  out and the deleting before any of the data of the chat interface is ever released.
[2087.40s -> 2095.62s]  So yeah, you know, I really, I really, I don't want that a person is ever like, Oh, what?
[2095.62s -> 2102.52s]  That's where my like, that it's not very clear, you know, hey, if you put stuff in here, you
[2102.52s -> 2108.36s]  know, you put thumbs up, thumbs down, we were going to use that and make that available.
[2108.36s -> 2114.36s]  I don't want people who are not like aware of that and yeah.
[2114.36s -> 2116.36s]  Yeah, absolutely.
[2116.36s -> 2121.36s]  On the evaluation, our friend Jeremy Howard had a few things to say.
[2121.36s -> 2123.36s]  And first of all, Jeremy, if you're watching, mate, please come on MLST.
[2123.36s -> 2127.36s]  I think it's about time we had a little chinwag, mate, you and me.
[2127.36s -> 2129.36s]  I'm a long-time fan, seriously.
[2129.36s -> 2134.36s]  But he was being a little bit nasty, wasn't he, about Open Assistant?
[2134.36s -> 2141.44s]  Well, you always have to view things through the lens of Twitter, right?
[2141.44s -> 2146.56s]  And first of all, it's a written medium, and second of all, it's Twitter.
[2146.56s -> 2151.12s]  So I completely discard that.
[2151.12s -> 2155.32s]  Criticism is obviously welcome and valid.
[2155.32s -> 2157.92s]  And I think he's made a few good points.
[2157.92s -> 2162.72s]  And it was especially with respect to what we did is we collected the data set, right?
[2162.72s -> 2163.88s]  We trained models on it.
[2163.88s -> 2168.00s]  Some of these models now run on the website for now,
[2168.00s -> 2170.96s]  which we're very fortunate to have some compute sponsors.
[2170.96s -> 2175.12s]  Also, thank you very much to those.
[2175.12s -> 2178.12s]  And we did a preference evaluation
[2178.12s -> 2181.64s]  where we took a bunch of prompts that we were sure
[2181.64s -> 2184.20s]  the models hadn't been trained on.
[2184.20s -> 2190.16s]  We gave the same prompts to chat GPT, like the free version.
[2190.16s -> 2192.92s]  and one of our models.
[2192.92s -> 2195.18s]  And then we made like a Google form
[2195.18s -> 2198.32s]  where we just asked the user, which one do you prefer?
[2198.32s -> 2203.32s]  And obviously there is a lot of brain power
[2205.32s -> 2209.44s]  has been gone since the start of at least like
[2209.44s -> 2211.84s]  start of science, but certainly since the start
[2211.84s -> 2214.48s]  of like asking humans about things
[2214.48s -> 2217.52s]  has been gone into how do you do that?
[2217.52s -> 2219.92s]  How do you ask people what they prefer?
[2219.92s -> 2222.72s]  like, how do you need to sample, which people do you ask,
[2222.72s -> 2223.52s]  and so on.
[2223.52s -> 2231.84s]  And obviously, I think we did a good job at that,
[2231.84s -> 2233.92s]  but obviously not.
[2233.92s -> 2235.40s]  There's always things you could do.
[2235.40s -> 2236.64s]  What did you do?
[2236.64s -> 2238.24s]  Well, we took those things.
[2238.24s -> 2239.40s]  We put those together.
[2239.40s -> 2240.94s]  We randomized their order, obviously.
[2240.94s -> 2243.40s]  And then we just sent that out.
[2243.40s -> 2248.44s]  I tweeted it out to people, like, hey,
[2248.44s -> 2255.52s]  help us, you know, compare these models, here's a thing. And then
[2255.54s -> 2259.76s]  what came out was on these prompts, it was about 5050.
[2259.80s -> 2262.62s]  Right? Sometimes people prefer the chat GPT answer. Sometimes
[2262.62s -> 2268.84s]  people preferred the the open assistant model answers. And you
[2268.84s -> 2272.28s]  could also kind of make out which ones were like, where is
[2272.28s -> 2276.00s]  one better? Where's the other one better? Now? Yeah, the
[2276.00s -> 2282.72s]  The result is, I want to say, I think it's statistically valid in the sense we did, like
[2282.72s -> 2289.12s]  a lot of people took part, we really, like, really, these are really the answers of the
[2289.12s -> 2293.40s]  models, we didn't like sample until our model had like a really good answer or anything
[2293.40s -> 2294.40s]  like this.
[2294.40s -> 2299.88s]  But it's also the case, I think that's one of the things Jeremy leveraged that chat GPT,
[2299.88s -> 2304.84s]  as everyone knows, it very often goes like as an AI language model, I'm sorry, I can't
[2304.84s -> 2311.12s]  fulfill that. Because I don't know, I asked about a recipe with with like alcohol in it.
[2311.12s -> 2317.10s]  And who alcohol is dangerous. And I can't I'm overstating now, right? But it very often
[2317.10s -> 2324.40s]  does this guardraily self censorship thing. And our models that we've trained don't do
[2324.40s -> 2330.48s]  that as much they do it frequently, but they don't do it as much as chat GPT. And obviously,
[2330.48s -> 2335.96s]  There are some prompts in there, for example, who would win a street fight, Joe Biden or
[2335.96s -> 2338.68s]  Joe Rogan?
[2338.68s -> 2343.40s]  Chat GPT, I believe, if I recall correctly, was just like, I'm sorry, I can't, you know,
[2343.40s -> 2346.66s]  this is touches on violence and street fighting.
[2346.66s -> 2349.92s]  I don't, I don't, I don't want to answer that.
[2349.92s -> 2358.80s]  Jeremy, for example, pointed out, hey, you should have done the evaluation only on prompts
[2358.80s -> 2365.54s]  That's where GPT actually decides to answer and only compare on those because it's clear
[2365.54s -> 2372.76s]  that if it doesn't answer, the preferable answer is the answer, which doesn't even have
[2372.76s -> 2375.16s]  to be correct.
[2375.16s -> 2382.60s]  The open assistant model said in that question, Joe Biden would win because he's taller.
[2382.60s -> 2384.96s]  And we don't know, right?
[2384.96s -> 2389.92s]  but it's very likely that the question isn't, like, that's not the correct answer.
[2389.92s -> 2396.08s]  Yet users obviously preferred it, or people who filled out the form preferred it to the
[2396.08s -> 2399.12s]  sorry, I don't want to answer that.
[2399.12s -> 2405.56s]  I think it's a fair point to say, hey, you know, there are different categories, and
[2405.56s -> 2408.56s]  maybe you should evaluate that that will be like, okay, there are different categories,
[2408.56s -> 2412.24s]  maybe you should split it up into look, there's this category of prompts, there's this category,
[2412.24s -> 2418.24s]  in this category. And there it would be very clear, like, in no way do we claim that the
[2418.24s -> 2423.74s]  open assistant models are as good as like, imagine, imagine that they're the one we used
[2423.74s -> 2430.78s]  even was like a 13 billion model. And chat GPT is by all we know, much bigger, much more
[2430.78s -> 2438.00s]  trained on stuff. So like, it's, it's better, like, no, no doubt about it. And I think people
[2438.00s -> 2442.64s]  have been a bit ruffled by the fact that we said, you know, in our evaluation, it was
[2442.64s -> 2450.12s]  like 50 50. But a lot of that, not a lot, but some of that came from the fact that yes,
[2450.12s -> 2455.76s]  sometimes chatgivity just denies to answer. But also, a lot of times, it comes from the
[2455.76s -> 2460.52s]  fact that people say, hey, for these couple of tasks, actually, I prefer the open assist
[2460.52s -> 2464.92s]  the models. And I think, yeah, that goes a bit under in these discussions.
[2464.92s -> 2468.36s]  Yeah, yeah, I mean just steel manning Jeremy a little bit
[2468.36s -> 2471.32s]  I didn't read it so much as being refusing to answer
[2471.32s -> 2476.68s]  I felt his criticism was more the selection bias both of the questions and the raters and
[2477.36s -> 2484.32s]  Also, I think there was this point about he thought you had portrayed it as being an evaluation instead of a user preference study
[2484.32s -> 2488.60s]  But you made it clear that it was a user preference study. Yes. Yes
[2488.60s -> 2492.16s]  It's it's like I think we said about five times
[2492.16s -> 2497.28s]  we have like user preference, preference, our form says, which one do you prefer? Right?
[2497.28s -> 2501.88s]  And I think it's still like, I think both things are valid, right? It's totally valid
[2501.88s -> 2506.28s]  to only compare, let's say, okay, let's just look on gardening, right? Chat GPT is certainly
[2506.28s -> 2510.60s]  not going to deny gardening, here's a category, which model is like better, objectively, which
[2510.60s -> 2516.80s]  gives the more truthful, which gives the more helpful answers we can rate it. And in our
[2516.80s -> 2521.28s]  data set, we actually collect these labels, right? Is it helpful? Is it funny, and so
[2521.28s -> 2523.52s]  and we haven't even used those labels yet.
[2523.52s -> 2526.76s]  So that's going to be another dimension of,
[2526.76s -> 2531.60s]  now we have three humans giving the same question and answer,
[2531.60s -> 2534.80s]  and we have labels on how funny each one of them is.
[2534.80s -> 2539.28s]  So that's going to be, I can't wait until we use those labels.
[2539.28s -> 2543.16s]  So it's totally valid to evaluate this in very different ways,
[2543.16s -> 2545.96s]  but there I have to say a little bit,
[2545.96s -> 2549.80s]  it's also totally valid to just plainly ask humans,
[2549.80s -> 2551.64s]  which one do you prefer?
[2551.64s -> 2554.40s]  And if chat GPT on prompts that we've just sampled
[2554.40s -> 2557.80s]  from our lottery, like, ooh, the selection of questions,
[2557.80s -> 2559.72s]  maybe you've, as I said, yeah, have you,
[2559.72s -> 2561.44s]  how often have you tried?
[2561.44s -> 2564.04s]  Like, no, this is the output.
[2564.04s -> 2567.88s]  And then it's like, ooh, your people ask a lot about bombs.
[2567.88s -> 2572.48s]  It's like, no, it's just not, you look at our data set.
[2572.48s -> 2575.96s]  I'm sorry, these are 20 prompts, right?
[2575.96s -> 2580.22s]  that are as they are, but if you look in our data set,
[2580.22s -> 2585.22s]  most people are immensely helpful and not edgy and not,
[2585.58s -> 2588.18s]  so I think that's also, that's like a,
[2590.24s -> 2591.66s]  I know it's formulated as a question,
[2591.66s -> 2596.62s]  but like, it's just distinctly not true.
[2596.62s -> 2601.02s]  Like people have been even more helpful than I thought,
[2601.02s -> 2603.02s]  and I have had big hopes for people,
[2603.02s -> 2605.38s]  and I've looked at the data and I'm like,
[2605.38s -> 2612.22s]  Oh, holy crap, people put like effort and work and soul into this, right?
[2612.22s -> 2617.10s]  So I think then going like, oh, your people will ask a lot about bomb.
[2617.10s -> 2621.94s]  So yeah, I do think it's totally valid to ask people, which one do you prefer?
[2621.94s -> 2626.66s]  And if chat GPT happens to say, no, I don't want it, then that's yes, people don't like
[2626.66s -> 2627.66s]  it.
[2627.66s -> 2628.66s]  Right.
[2628.66s -> 2634.22s]  And if people like it, they could say, yes, I prefer the no, I don't want to do this to
[2634.22s -> 2639.66s]  the model that wants to do it, if they think that's the appropriate thing, I do think that
[2639.66s -> 2648.58s]  at least it's a valid, one valid way of comparing models just to say, which one do you prefer?
[2648.58s -> 2652.38s]  If it happens to deny your request, you know, that's a signal too.
[2652.38s -> 2655.24s]  And that should be taken into account too.
[2655.24s -> 2660.22s]  And then saying, specifically saying, no, no, we should just filter all the things where
[2660.22s -> 2666.30s]  chat GPT denies, then it's like, well, here you have a model who can put much more of
[2666.30s -> 2674.76s]  its effort and focus and parameters right into the narrow domain where it does answer.
[2674.76s -> 2681.22s]  And you compare that to a model that has a wider spectrum of topics available.
[2681.22s -> 2684.98s]  I'm not sure that's a fair comparison to even if you limit it to that scope, right?
[2684.98s -> 2689.50s]  The other model also has to handle all of these other things.
[2689.50s -> 2695.58s]  That being said, as I said, capability-wise, I have no doubt that chat GPT is better.
[2695.58s -> 2701.26s]  For overall, right, especially things like coding and so on, like, there's no way for
[2701.26s -> 2705.18s]  now Open Assistant is as good.
[2705.18s -> 2710.10s]  However, in some tasks, people like it more.
[2710.10s -> 2711.10s]  Okay.
[2711.10s -> 2712.10s]  Okay.
[2712.10s -> 2717.18s]  So the ethics community are probably seething at the moment about the runaway success of
[2717.18s -> 2723.90s]  Open Assistant, notably it blew up on social media and none of those folks in particular
[2724.54s -> 2728.38s]  liked it, retweeted and then they all jumped on Jeremy Howard's piece.
[2729.10s -> 2733.50s]  But we shouldn't like, I have not...
[2735.34s -> 2736.86s]  Shouldn't say, we can edit that out.
[2738.46s -> 2743.26s]  Well, we shouldn't, we should, like that's not a necessary property of Jeremy, right?
[2743.26s -> 2754.02s]  Just because people of a certain way of thinking all promote your stuff because they think
[2754.02s -> 2763.26s]  that criticizing that other stuff is a good thing, it shouldn't be his responsibility
[2763.26s -> 2764.26s]  in any way.
[2764.26s -> 2768.62s]  It's not his responsibility, but I'm saying that you really, really ruffled their feathers
[2768.62s -> 2770.98s]  with the 4chan bot.
[2770.98s -> 2771.98s]  Possibly.
[2771.98s -> 2773.98s]  So they don't like you very much.
[2773.98s -> 2774.98s]  Possibly.
[2774.98s -> 2780.98s]  I just wondered, from your perspective, how do you think they are going to criticize you?
[2780.98s -> 2782.98s]  Academically, mostly.
[2782.98s -> 2790.98s]  It's very easy because it's like Open Assistant is a bunch of, crassly said, a bunch of plebs
[2790.98s -> 2794.98s]  doing something and doing it on their own.
[2794.98s -> 2796.98s]  You're not exactly a pleb, Janne.
[2796.98s -> 2802.34s]  No, but but I'm not like, I'm not a I'm not like, an academic or in academia.
[2802.34s -> 2803.94s]  If you're not an academic, then who the hell?
[2803.94s -> 2808.90s]  Yeah. But you know what I mean? It's like a community effort. And it's been it's been
[2808.90s -> 2815.70s]  done relatively straightforwardly and open without much consideration to, to politics
[2815.70s -> 2822.26s]  without much consideration to, I don't know, worldwide concerns or anything like this.
[2822.26s -> 2825.26s]  Because we just wanted to we just said, Hey, let's come together.
[2825.26s -> 2831.04s]  Let's build a competent, a good data set to train a competent assistant, because we all
[2831.04s -> 2837.22s]  could benefit from a competent assistant that we didn't do it in any, in any particularly.
[2837.22s -> 2842.38s]  Yeah, in any political way, we didn't do it in any, okay, this is gonna sound wrong, but
[2842.38s -> 2847.88s]  we didn't do it in any particularly ethical way, by which I mean, sorry, if you take that
[2847.88s -> 2852.48s]  in out of context, by which I mean, we didn't like, extremely
[2852.48s -> 2856.48s]  overemphasize ethical considerations, we have clear
[2856.48s -> 2858.96s]  guidelines, like, here is the things we want in the days,
[2858.96s -> 2861.52s]  here's the things we don't want in the data set, if someone
[2861.52s -> 2865.20s]  comes and asks for those things, then react like this, right? We
[2865.20s -> 2869.16s]  have these clear things, but we're we haven't been over
[2869.44s -> 2872.80s]  emphasizing it like some of those people would. And
[2873.12s -> 2876.52s]  well, could I point out that you do have ethical guidelines, but
[2876.52s -> 2879.20s]  they are deontological, not consequentialist.
[2879.20s -> 2880.04s]  So you have...
[2880.04s -> 2882.40s]  I don't know what those words mean.
[2882.40s -> 2884.16s]  You have rules, you say,
[2884.16s -> 2885.96s]  I don't want that in my data set.
[2885.96s -> 2889.34s]  You're not saying it could potentially lead to this.
[2891.20s -> 2893.48s]  Okay, I still don't know what the diff...
[2893.48s -> 2894.52s]  Like, okay.
[2894.52s -> 2897.88s]  So you're saying I don't want any pornography
[2897.88s -> 2900.08s]  of a certain type in my data set.
[2900.08s -> 2901.22s]  So that's a rule.
[2901.22s -> 2903.16s]  So yeah, or if someone comes and like,
[2903.16s -> 2907.80s]  wants to promote violence or something, it's like, no, right?
[2907.80s -> 2908.76s]  So you have principles.
[2908.76s -> 2912.28s]  And if someone comes and says, how can I build a bomb?
[2912.28s -> 2916.24s]  Then recognizing there are, there may be legitimate reasons
[2916.24s -> 2918.12s]  to build a bomb, right?
[2918.12s -> 2923.84s]  Like to build an explosive device, saying, this is dangerous, right?
[2923.84s -> 2926.68s]  Please consult a professional.
[2926.68s -> 2932.72s]  If you must, here, if you really want to, this is a bad example.
[2932.72s -> 2936.36s]  But it's like whenever something might be dangerous,
[2936.36s -> 2940.76s]  our guidelines are, hey, look, warn the person, right?
[2940.76s -> 2942.64s]  Say, look, this is potentially dangerous.
[2942.64s -> 2944.36s]  Building a bomb is a wrong example.
[2944.36s -> 2947.56s]  Let's say, I want to, I don't know.
[2949.32s -> 2951.20s]  I'm not coming up with a good example,
[2951.20s -> 2955.36s]  but let's say it's something that's potentially dangerous,
[2955.36s -> 2958.52s]  but also useful in a lot of cases.
[2958.52s -> 2961.62s]  The guidelines are, warn about that,
[2961.62s -> 2964.98s]  like say, Hey, look, this is your your endangered territory
[2964.98s -> 2968.70s]  here. This is potentially dangerous. Do you want to really
[2968.70s -> 2973.10s]  want it? Right? And then if the user pushes or says yes, it's
[2973.10s -> 2978.22s]  like, okay, here is how but, you know, consult a professional or
[2978.42s -> 2984.54s]  something like this. So we do have guidelines like that. But
[2984.56s -> 2987.54s]  yeah, I mean, that's what I want to say. So you do have have an
[2987.78s -> 2989.86s]  ethical code. There's no question about that. But it's a
[2989.86s -> 2993.78s]  different code but would you consider getting a team of ethicists involved? I mean it's
[2993.78s -> 2997.74s]  a big project now, you must have had loads of people offered to get involved. I mean
[2997.74s -> 3004.84s]  if that happened what do you think it would look like and how would it affect the project?
[3004.84s -> 3010.06s]  It's a good question because I think AI ethics is in an absolutely abhorrent state right
[3010.06s -> 3017.78s]  now where it's, I've met ethicists before and they were among the most you know competent
[3017.78s -> 3023.46s]  people that I have had the pleasure to interact with, right?
[3023.46s -> 3028.82s]  It's very level-headed, very, you know, also pragmatic in a sense of being like,
[3028.82s -> 3031.14s]  look, here is also what's realistic to achieve,
[3031.14s -> 3034.42s]  here is the thought process behind it and so on.
[3034.42s -> 3039.38s]  Like, I totally see ethics in any scientific discipline
[3039.38s -> 3042.82s]  as a vital and important thing to do.
[3042.82s -> 3046.90s]  And I have, I guess, unfortunately, made the experience
[3046.90s -> 3054.54s]  of how it can be done competently. And this current state of a lot of AI, not all AI ethicists,
[3054.54s -> 3062.14s]  but the current state of like AI ethics is not that and it's it's very much a, a, I can't
[3062.14s -> 3069.58s]  even describe it very well. But I just complain about stuff culture, because that gets you
[3069.58s -> 3076.42s]  like clout, I guess, or it's easy win, easy win, you can always complain, right? Such
[3076.42s -> 3086.62s]  an easy win. And if there is a team of competent, pragmatic people, they don't have to have
[3086.62s -> 3093.90s]  the same opinions as I do, right? But they have to have the good of the project and
[3093.90s -> 3101.70s]  the good of humanity, I guess, in mind. Yeah, that's cool. Also, I'm not like the king of
[3101.70s -> 3109.70s]  this, right? Like, I'm not the king of open assist and I don't get if people want to conglomerate
[3109.70s -> 3115.70s]  and talk about the ethics of all of this and ping us with inputs, like, cool, cool.
[3115.70s -> 3119.70s]  But I mean, you know, when we do talk about some of these risks around misinformation
[3119.70s -> 3126.42s]  and bias, I mean, public accountability, public awareness, the ethicists have done
[3126.42s -> 3131.58s]  stuff like producing model cards and you know, like making it clear what the data biases
[3131.58s -> 3132.58s]  and stuff like that.
[3132.58s -> 3144.80s]  I mean, do you do you think that's useful?
[3144.80s -> 3146.80s]  Yes.
[3146.80s -> 3155.12s]  Um, I mean, it's what is a model card, a model card is a readme, right?
[3155.12s -> 3157.84s]  And then it has some structure to it, right?
[3157.84s -> 3163.64s]  It's it's it says, here are the here are the, the things you could describe about your model.
[3163.64s -> 3171.48s]  And here are some examples, I think it's useful to have that to have as a norm in the community
[3171.48s -> 3175.96s]  to say, you know, if I publish a model, I sort of I report what it's been trained on
[3175.96s -> 3178.40s]  how it's been trained on.
[3178.40s -> 3186.00s]  And even to a degree, like what I think it could do or should be used for, although, yeah,
[3186.00s -> 3190.60s]  if the structure of such a model car gets like too rigid, and it's like, No, we must
[3190.60s -> 3198.28s]  you must ask these questions, you get into so many ridiculous situations like, you know,
[3198.28s -> 3205.52s]  can this be reproduced? Well, I just like I, I made SK learn dot linear regression,
[3205.52s -> 3214.52s]  right? Yes, it can be. You get into, into situations where the questions don't address
[3214.52s -> 3218.20s]  what you would actually like to express in such a thing. And then I think it becomes
[3218.20s -> 3223.32s]  counterproductive, but as a norm to have a look, if you publish something, people should be able to
[3223.32s -> 3232.20s]  understand and potentially reproduce it. That standard we have had in papers for a long time,
[3232.20s -> 3239.32s]  and it's generally been a good standard to say, look, if you write, if you publish something,
[3239.32s -> 3245.16s]  I must be able to from reading it, understand what's in there. And to have that as a norm
[3245.16s -> 3249.00s]  in the community. Yeah, I'm totally fine with that.
[3249.56s -> 3253.40s]  Do you think there's any relationship with the Chomsky syndrome that we were talking about
[3253.40s -> 3260.76s]  earlier, which is this idea that we should have a very clear model of understanding of
[3260.76s -> 3266.36s]  how these things work in society and we should be able to extrapolate and control things?
[3267.32s -> 3272.36s]  The fear is that basically this is just a complete black box and who knows what's going to happen?
[3272.36s -> 3281.59s]  Nah, I'm good with the black box. It keeps things exciting and interesting.
[3281.59s -> 3291.59s]  And as I said, I don't believe this sort of runaway, it might become, you know, very influential and so on.
[3291.59s -> 3297.59s]  And certainly that's not very good, but then again, I don't know what to do about it.
[3297.59s -> 3305.35s]  Certainly if some people sign a change.org moratorium petition, even if it's reached,
[3305.35s -> 3307.55s]  it's not going to help, right?
[3307.55s -> 3308.55s]  What are you going to do?
[3308.55s -> 3311.43s]  It doesn't matter being worried about it.
[3311.43s -> 3314.11s]  We've got a few minutes left, so I've got some questions from Jumbotron.
[3314.11s -> 3315.11s]  Say hello, Jumbotron.
[3315.11s -> 3316.11s]  Hello, Jumbotron.
[3316.11s -> 3320.19s]  He's our forum administrator and he's a legend.
[3320.19s -> 3321.19s]  Quick question.
[3321.19s -> 3324.67s]  Do you think all AI research should be open and accessible to the public?
[3324.67s -> 3325.67s]  No.
[3325.67s -> 3331.67s]  So, it's totally legitimate that a business does internal research like all companies
[3331.67s -> 3335.43s]  do and that they then use that to make money.
[3335.43s -> 3338.35s]  Like, that's very cool with me.
[3338.35s -> 3343.95s]  I've never said that shouldn't be the case, only that companies shouldn't do that but
[3343.95s -> 3353.19s]  at the same time claim how open and all democratizing and beneficial to the common good they are.
[3353.19s -> 3362.03s]  Okay, and you would accept that some research could lead to negative or unethical applications
[3362.03s -> 3364.95s]  and might need to be restricted?
[3364.95s -> 3372.83s]  Yeah, I totally accept that some research can and will probably lead to overall negative
[3372.83s -> 3378.71s]  effects for society or for certain individuals within society, right?
[3378.71s -> 3385.99s]  And self flying drones from any regime in the world, they probably they run, they're
[3385.99s -> 3391.83s]  on one of, they run one of their certainly run some open source components as part of
[3391.83s -> 3393.29s]  their guidance system, right?
[3393.29s -> 3398.61s]  They may be run a Linux kernel, like who knows, but I don't think the Linux kernel should
[3398.61s -> 3403.71s]  not be fully open source and accessible to everyone.
[3403.71s -> 3410.45s]  And I don't want anyone to be able to be the decider of, you know, the eternal decider
[3410.45s -> 3414.03s]  of who's good enough to use the Linux kernel or not.
[3414.03s -> 3425.11s]  I'd rather, I think the overall, overall welfare of society and humanity is much better served
[3425.11s -> 3430.21s]  by accepting that some people are going to do some bad things with it, and then mitigating
[3430.21s -> 3442.53s]  that in a different way than appointing the king of that model to decide who they deem
[3442.53s -> 3446.57s]  pure hearted enough to wield it.
[3446.57s -> 3451.09s]  What's next for ML News and your channel and are you making any more music videos with
[3451.09s -> 3452.09s]  AI?
[3452.09s -> 3459.37s]  Well, it's become, there are so many good music videos on AI now, I'm always amazed
[3459.37s -> 3465.15s]  by how talented people are and how quickly they pick up sort of the new stuff and do
[3465.15s -> 3466.15s]  something with it.
[3466.15s -> 3467.15s]  So that's very cool.
[3467.15s -> 3473.43s]  I want, as I said, I've not made too many videos because I've been extremely busy with
[3473.43s -> 3475.11s]  Open Assistant.
[3475.11s -> 3481.19s]  And I think we've also built up sort of a momentum in the direction right now, and there
[3481.19s -> 3483.87s]  There are many competent people in our team.
[3483.87s -> 3490.71s]  So I'm also looking to make a couple of more videos, again, paper reviews, news, but also
[3490.71s -> 3497.35s]  a bunch of projects, which I always want to do, but then they take time, of course.
[3497.35s -> 3503.29s]  But I'm very excited about just some of the new stuff that's possible and to try it out
[3503.29s -> 3509.71s]  and to show people a little bit of what one could do and how to have fun with these things.
[3509.71s -> 3510.71s]  Indeed.
[3510.71s -> 3515.63s]  Just in closing, have you got any shoutouts for people in your life or in Discord who
[3515.63s -> 3519.09s]  have really helped you on the journey?
[3519.09s -> 3524.59s]  Too many, like way too many to name by name.
[3524.59s -> 3528.43s]  This is, I could go on like eternal lists.
[3528.43s -> 3535.99s]  In Open Assistance specifically, Andreas Koepp has been extremely influential in that, like
[3535.99s -> 3542.31s]  just organizing things, but also coding things himself, but also like, also all the, all
[3542.31s -> 3543.31s]  the other mems.
[3543.31s -> 3547.63s]  As I said, if I start listing people, I'm going to miss someone and I don't want to
[3547.63s -> 3548.63s]  do that.
[3548.63s -> 3551.83s]  So I don't want to start listing people, but then I think, well, I really want to list
[3551.83s -> 3552.83s]  the people.
[3552.83s -> 3555.59s]  There is a, it's a, it's an eternal conundrum.
[3555.59s -> 3561.87s]  So it like to anyone who's ever had any, any part of helping me or, or given, given feedback
[3561.87s -> 3571.55s]  or even like been kind of a dick, like it's, I appreciate it and I, yeah, it's been amazing
[3571.55s -> 3577.35s]  the amount of help and input you get from good-willed people.
[3577.35s -> 3578.35s]  Yeah.
[3578.35s -> 3579.35s]  Yeah.
[3579.35s -> 3580.35s]  Communities are amazing.
[3580.35s -> 3584.43s]  So join Yannick's Discord, join our Discord, Open Assistant Discord.
[3584.43s -> 3588.55s]  Dr. Kilcher, thank you so much.
[3588.55s -> 3590.67s]  This has been an absolute pleasure, sir.
[3590.67s -> 3591.67s]  Thanks for having me.
========================================
Detected language 'en' with probability 1.000000

run time =193.10444164276123

Completed srt: OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3.srt
Completed: C:/Users/BrodskiTheGreat/Desktop/desktop/Code/scraper-dl-vids/./assets/raw//OPENASSISTANT+TAKES+ON+CHATGPT-TFa539R09EQ.mp3
